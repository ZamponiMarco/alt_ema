"V.M., Mostofi, Vahid Mirzaebrahim; E., Krul, Evan; D., Krishnamurthy, Diwakar; M.F., Arlitt, Martin F.","Trace-Driven Scaling of Microservice Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151504621&partnerID=40&md5=e295a30d5c7dd3460a549591e0a77bc1","The containerized microservices architecture is being increasingly used to build complex applications. To minimize operating costs, service providers typically rely on an auto-scaler to 'right size' their infrastructure amid fluctuating workloads. The agile nature of microservice development and deployment requires an auto-scaler that does not require significant effort to derive resource allocation decisions. In this paper, we investigate reducing auto-scaler development effort along a number of dimensions. First, we focus on a technique that does not require an expert to develop a model, e.g., a queuing model or machine learning model, of the system and tweak the model as the underlying microservice application changes. Second, we explore ways to limit the number of workload patterns that need to be considered. Third, we study techniques to reduce the number of resource allocation scenarios that one has to explore before deploying the auto-scaler. To address these goals, we first analyze the workload of 24,000 real microservice applications and find that a small number of workload patterns dominate for any given application. These results suggest that auto-scaler design can be driven by this small subset of popular workload patterns thereby limiting effort. To limit the number of resource allocation scenarios explored, we develop a novel heuristic optimization technique called MOAT, which outperforms Bayesian Optimization often used for such exercises. We combine insights obtained from real microservice workloads and MOAT to realize an auto-scaler called TRIM that requires no system modeling. For each popular workload pattern identified for an application, TRIM uses MOAT to pre-compute a near minimal resource allocation that satisfies end user response time targets. These resource allocations are then used at runtime when appropriate. We validate our approach using a variety of analytical, on-premise, and public cloud systems. From our results, TRIM in consort with MOAT significantly improves the performance of the industry-standard HPA auto-scaler by achieving up to 92% fewer response time violations and up to 34% lower costs compared to using HPA in isolation. © 2023 Elsevier B.V., All rights reserved.",Autoscaling approach application. Conducts a study on performance models and recurring load shapes but does not consider automatic load test generation.
"C., Mascia, Cristian; A., Guerriero, Antonio; L., Giamattei, Luca; R., Pietrantuono, Roberto; S., Russo, Stefano","Microservices Performance Testing with Causality-enhanced Large Language Models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011360178&partnerID=40&md5=4653a8f311da5e7a0f460a361962bc04","Efficient performance testing of microservices is essential for engineers to ensure that deviations of performance/resource usage metrics from expectations are promptly identified within their rapid release cycle. To this aim, engineers would need to explore the space of possible workload configurations and focus only on the critical ones, e.g., low-load configurations that unexpectedly cause performance issues. This requires a great effort, and can be infeasible in short release cycles.We present CALLMIT, a framework using Large Language Models (LLM) enhanced by causal reasoning to automatically generate critical workloads for microservices performance testing. Engineers query CALLMIT to generate workload configurations expected to expose deviations from performance requirements, so as to actually run only tests that trigger critical configurations. We present the experimental evaluation on three subjects, with comparison to a conventional Retrieval-Augmented Generation technique. The results show that causal models improve the correct identification by LLM of performance-critical workload configurations. © 2025 Elsevier B.V., All rights reserved.",Close approach. Performs load test generation for microservices through LLMs and causal reasoning. Does not involve autoscalers. Generates a static load test. Does not consider dynamics.
"M., Straesser, Martin; S., Eismann, Simon; J.V., von Kistowski, Jóakim V.; A., Bauer, Andre; S., Kounev, Samuel","Autoscaler Evaluation and Configuration: A Practitioner's Guideline","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158100722&partnerID=40&md5=3c05f8795b39662c6886f95fdcb1ce98","Autoscalers are indispensable parts of modern cloud deployments and determine the service quality and cost of a cloud application in dynamic workloads. The configuration of an autoscaler strongly influences its performance and is also one of the biggest challenges and showstoppers for the practical applicability of many research autoscalers. Many proposed cloud experiment methodologies can only be partially applied in practice, and many autoscaling papers use custom evaluation methods and metrics. This paper presents a practical guideline for obtaining meaningful and interpretable results on autoscaler performance with reasonable overhead. We provide step-by-step instructions for defining realistic usage behaviors and traffic patterns. We divide the analysis of autoscaler performance into a qualitative antipattern-based analysis and a quantitative analysis. To demonstrate the applicability of our guideline, we conduct several experiments with a microservice of our industry partner in a realistic test environment. © 2023 Elsevier B.V., All rights reserved.","Close work. Focuses on load test design. Qualitative tests propose load shapes, but leaves their configuration manual. Quantitative analysis focuses on statistically extracted workloads."
"M., Camilli, Matteo; A., Guerriero, Antonio; A.A., Janes, Andrea A.; B., Russo, Barbara; S., Russo, Stefano","Microservices Integrated Performance and Reliability Testing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133466454&partnerID=40&md5=2c7dbfa0b5b71841735e1706f9c2da05","Continuous quality assurance for extra-functional properties of modern software systems is today a big challenge as their complexity is constantly increasing to satisfy market demands. This is the case of microservice systems. They provide high control on the scale of operation by means of fine-grained service decomposition, but this demands careful consideration of the relations between performance of individual microservices and service failures. In this work, we propose MlPaRT, a novel methodology, and platform to automatically test microservice operations for performance and reliability in combination. The proposed platform can be integrated into a DevOps cycle to support continuous testing and monitoring by the automatic (1) generation and execution of performance-reliability ex-vivo testing sessions, (2) collection of monitoring data, (3) computation of performance and reliability metrics, and (4) integrated visualization of the results. We apply our approach by operating the platform on an open source benchmark. Results show that our integrated approach can provide additional insights into the performance and reliability behaviour of microservices as well as their mutual relationships. CCS CONCEPTS • Software and its engineering →Software performance; Software reliability; Software verification and validation. © 2022 Elsevier B.V., All rights reserved.",Employes statistical performance test generation from execution traces.  Statically defined performance traces.
"W., Meijer, Willem; C., Trubiani, Catia; A., Aleti, Aldeida","Experimental evaluation of architectural software performance design patterns in microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202513844&partnerID=40&md5=c637e9e89577e34f37b75c74bf616c53","Microservice architectures and design patterns enhance the development of large-scale applications by promoting flexibility. Industrial practitioners perceive the importance of applying architectural patterns but they struggle to quantify their impact on system quality requirements. Our research aims to quantify the effect of design patterns on system performance metrics, e.g., service latency and resource utilization, even more so when the patterns operate in real-world environments subject to heterogeneous workloads. We built a cloud infrastructure to host a well-established benchmark system that represents our test bed, complemented by the implementation of three design patterns: Gateway Aggregation, Gateway Offloading, Pipe and Filters. Real performance measurements are collected and compared with model-based predictions that we derived as part of our previous research, thus further consolidating the actual impact of these patterns. Our results demonstrate that, despite the difficulty to parameterize our benchmark system, model-based predictions are in line with real experimentation, since the performance behaviors of patterns, e.g., bottleneck switches, are mostly preserved. In summary, this is the first work that experimentally demonstrates the performance behavior of microservices-based architectural patterns. Results highlight the complexity of evaluating the performance of design patterns and emphasize the need for complementing theoretical models with empirical data. © 2024 Elsevier B.V., All rights reserved.",Evaluation of microservice performance given architecture and design patterns
"M., Ezzeddine, Mazen; F., Baudé, Fran¸Coise; F., Huet, Fabrice; F., Laaziz, Fatima","Latency Aware and Resource-Efficient Bin Pack Autoscaling for Distributed Event Queues: Parameters Impact and Setting","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218680805&partnerID=40&md5=7455eaf020c9d25d6329d95eba2a872e","The event driven architectural style is becoming increasingly popular for designing large scale real time cloud applications. In this architectural style, an application is composed of a set of services that communicate by exchanging events over a distributed event queue rather than over remote procedure calls RPC. To achieve real time processing, services (event consumers) in the application must be able to consume and process events in less than a desired latency. Meeting such desired latency must be accomplished at low cost in terms of resources used. This paper presents a new approach to dynamic event consumer provisioning in distributed event queues, focusing on latency-awareness and resource efficiency. Our proposed solution models the dynamic event consumer replica provisioner as a two-dimensional bin packing problem. Furthermore, we discuss how dynamic event consumers provisioning in distributed event queues necessitates a blocking synchronization protocol that is at conflict with meeting a desired latency for high percentile of events. To address this issue, we propose an extension to the bin pack autoscaler logic to reduce the tail latency caused by the events accumulated during the blocking synchronization protocol. As part of the experimental work, we provide insights and recommendations on the configuration of the proposed bin pack model when the events processing time and the workload feature large variance. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"J., Cai, Jianjun; H., Yu, Hao; H., Zhang, Hong; D., Li, Dongyu","Design and Implementation of an Automotive Remanufactured Parts Management Platform Based on Microservice Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011938881&partnerID=40&md5=f55012690f4bb65101402cb93a4f8680","Automotive remanufacturing is a crucial approach to realizing resource recycling and environmental protection. This paper proposes and implements an automotive remanufactured parts management platform based on a microservice architecture. By adopting modular design and service-oriented splitting, the platform enhances the flexibility, scalability, and maintainability of the system. This paper elaborates on the platform's design concepts, architectural design, key technology implementation, and performance evaluation, and validates its effectiveness through experiments. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"Q., Zhang, Qinhong","Design and implementation analysis of microservices architecture for enterprise management software based on DevOps","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010653390&partnerID=40&md5=3b82c04b0bacbe1bb8b8e88bb74d0fea","This study explores the design and implementation of DevOps-based microservice architecture for smart city enterprise management software. In the context of smart city construction, the digital transformation of enterprises puts forward higher requirements for the flexibility and scalability of system architecture, and the traditional monolithic architecture can no longer meet the business needs of modern enterprises. The research adopts Spring Cloud framework to develop the system and realizes a three-tier microservice architecture design for smart city enterprise services and divides the business tier into 12 core microservices through domain-driven design methodology. The system adopts MySQL parent-child cluster for data management and is deployed on Kubernetes platform. Jenkins is used to achieve continuous integration and deployment, and the environment consistency is ensured by Docker containerization technology, which realizes the DevOps practice of full process automation. Functional testing verifies that the functional integrity of the system reaches 99.5%, and performance testing shows that the system supports 10,000 concurrent users with an average response time of 200ms, and system availability reaches 99.95%. With the read-write separation strategy, database query performance was improved by 62% and order processing time was reduced from 15 minutes to three minutes. The system provides replicable technical solutions and practical experience for the digital transformation of smart city enterprises, and has important theoretical and practical value for promoting the construction of smart cities. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"C.F.H.F.H., Villa, César Felipe Henao Felipe Henao; C.A., Echeverri-Gutiérrez, Camilo Andrés; L.C.A., Agudelo, Leidy Catalina Acosta; D.A.G., Arango, David Alberto García; L.M.G., Cano, Lisbet Maria Garzon; E.S., Arboleda, Euris Santa; M.E.H., Garcia, Marlo Eliecer Hoyos","Optimizing Microservices Performance and Scalability Through Automated Monitoring with Kubernetes and Prometheus","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000843699&partnerID=40&md5=5e040ac37eb48857460424852f46bd1b","Performance monitoring is essential to ensure the scalability and efficiency of microservices-based applications. This paper presents the design, development, and evaluation of an automated monitoring system using Kubernetes, Prometheus, and Grafana to optimize the performance of critical microservices within an ERP ecosystem, such as the OrderProcessingService and InventoryManagementService. Through continuous monitoring, the system collects real-time metrics, including CPU usage, memory consumption, and latency, enabling the detection of anomalies and performance regressions. Load testing with JMeter was conducted to simulate various system demands, identifying resource management issues and bottlenecks. The results show improvements in efficiency and stability, especially in memory management and reduced CPU usage in high-demand scenarios. OrderProcessingService demonstrated consistent performance, while InventoryManagementService showed variability, requiring further optimization. The developed system provides a foundation for continuous performance improvement, contributing to the scalability, reliability, and resilience of microservices-based architectures. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"Á., Leiter, Ákos; D., Matusovits, Döme; L., Bokor, László","Evaluation of traditional and eBPF-based packet processing in Kubernetes for network slicing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002118422&partnerID=40&md5=35465317f9d0f8f191eeef417e4cc023","In recent years, the proliferation of cloud-native technology enablers, such as microservice deployment and management with Kubernetes, have presented new challenges for telecommunications service providers. Strict data transmission requirements have emerged in various areas, such as immediate interventions in intelligent transportation, video conferencing, etc. With the advent of 5G networks, this demand can also be fulfilled thanks to an innovative technology called Network Slicing. In terms of its operation, we can separate networks into individual segments to continuously satisfy the desired service requirements. However, packet processing on top of Kubernetes may need to be changed to support the emerging number of microservices during slicing. This is where the Extended Berkeley Packet Filter (eBPF) comes into the picture to boost the capacity of data centers and keep service guarantees. This paper presents how eBPF can support network slicing through its performance evaluation in a Kubernetes environment. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"J., Park, Jinwoo; J., Park, Jaehyeong; Y., Jung, Youngmok; H., Lim, Hwijoon; H., Yeo, Hyunho; D., Han, Dongsu","TopFull: An Adaptive Top-Down Overload Control for SLO-Oriented Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202301555&partnerID=40&md5=984704d324063b1f7e77eff74a6c462a","Microservice has become a de facto standard for building large-scale cloud applications. Overload control is essential in preventing microservice failures and maintaining system performance under overloads. Although several approaches have been proposed, they are limited to mitigating the overload of individual microservices, lacking assessments of interdependent microservices and APIs.This paper presents TopFull, an adaptive overload control at entry for microservices that leverages global observations to maximize throughput that meets service level objectives (i.e., goodput). TopFull makes adaptive load control on a per-API basis, exercises parallel control on each independent subset of microservices, and applies RL-based rate controllers that adjust the admitted rates of the APIs at entry according to the severity of overload. Our experiments on various open-source benchmarks demonstrate that TopFull significantly increases goodput in overload scenarios, outperforming DAGOR by 1.82x and Breakwater by 2.26x. Furthermore, the Kubernetes autoscaler with TopFull serves up to 3.91x more requests under traffic surge and tolerates traffic spikes with up to 57% fewer resources than the standalone Kubernetes autoscaler. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"D., Faustino, Diogo; N., Gonçalves, Nuno; M., Portela, Manuel; A., Rito Silva, António","Stepwise migration of a monolith to a microservice architecture: Performance and migration effort evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187961660&partnerID=40&md5=21af871a50b57152f3d5ddc1a5160789","Due to scalability requirements and the split of large software development projects into small agile teams, there is a current trend toward the migration of monolith systems to the microservice architecture. However, the split of the monolith into microservices, its encapsulation through well-defined interfaces, and the introduction of inter-microservice communication add a cost in terms of performance. In this paper, we describe a case study of the migration of a monolith to a microservice architecture, where a modular monolith architecture is used as an intermediate step. The impact on migration effort and performance is measured for both steps. Current state-of-the-art analyses the migration of monolith systems to a microservice architecture, but we observed that migration effort and performance issues are already significant in the migration to a modular monolith. Therefore, a clear distinction is established for each of the steps, which may inform software architects on the planning of the migration of monolith systems. In particular, we consider the trade-offs of doing all the migration process or just migrating to a modular monolith. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"Q., Cooper, Quinn; D., Krishnamurthy, Diwakar; Y., Amannejad, Yasaman","Budget Aware Performance Test Selection for Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203265386&partnerID=40&md5=c845cf4b59bab7badc8b2a4bb5e82aa8","The microservice architecture is being increasingly used to build complex applications. This software architecture offers scalability, modularity, agility to development processes. However, ensuring optimal performance prior to deployment emerges as a significant challenge, especially within the fast-paced environments of Continuous Integration/Continuous Deployment (CI/CD) pipelines. Traditional performance testing processes are often reliant on synthetic scenarios and lengthy testing processes, and can be challenging to adopt in such an environment where testing needs to be both realistic and quick. Addressing this need, our paper proposes and implements an innovative framework that leverages real-world usage traces to identify and execute a small yet essential set of performance tests. This approach aims to seamlessly integrate with CI/CD workflows, offering developers quick feedback on performance issues and scalability constraints that may arise from changes to one or more microservices. Through a series of empirical evaluations, we compare the ability of different techniques in identifying a set of performance tests that can capture historically observed system behaviour and be executed within a specified time budget. In our paper, we successfully replicated the response time distribution of a 24-hour test on our custom testbench within just a five-minute test, achieving a relative percent error of only 0.96% and 1.40% at the 95th and 99th percentile, respectively. This considerable decrease in time and resources necessary for load testing and response time modeling demonstrates the efficiency and effectiveness of our approach. © 2024 Elsevier B.V., All rights reserved.",Focus on performance test design. Given a statistically extracted load test it compacts it to achieve short tests. Does not generate load tests from scratch. Intersting as an orthogonal technique.
"K.Q., Pham, Khanh Quan; T., Kim, Taehong","Elastic Federated Learning with Kubernetes Vertical Pod Autoscaler for edge computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192211225&partnerID=40&md5=a9258a4a1dd64b1a20cc32a3f12c4861","Federated Learning (FL) is an emerging paradigm for training machine learning models across decentralized edge devices, ensuring data privacy and reducing computational tasks on the central cloud. However, the dynamic and resource-constrained nature of edge environments raises significant challenges in deploying FL applications efficiently. This paper introduces an Elastic Federated Learning framework that leverages Kubernetes Vertical Pod Autoscaler to improve the performance of FL applications. The framework enables dynamic adjustment of computational resources, facilitating effective resource utilization for model training and allowing quicker attainment of the targeted accuracy levels for the trained models. Our experiments demonstrate that the proposed framework significantly enhances FL efficiency, maintains model training progress during resource adjustments, and effectively addresses the resource allocation challenges in edge environments. This work advances the capabilities of FL in edge computing, opening doors to more efficient and scalable AI applications while preserving data privacy. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"K., Chang, Kaidi; J., Wang, Jingjing; J., Tian, Jiayi","The design and implementation of a resource scheduling and management system for electric vehicle charging stations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014239095&partnerID=40&md5=0ab1c801e44ee38e71445a114cacf81a","The rapid development of the electric vehicle industry has raised higher demands for charging management systems. This paper designs and implements an electric vehicle fast charging management system based on a microservice architecture. The system adopts a four-layer architecture consisting of the data layer, service layer, application layer, and access layer, supporting functions such as charging reservation, real-time monitoring, and fee calculation. The system is built on the Spring Cloud framework, combined with distributed caching and asynchronous processing to enhance performance. It also optimizes operational efficiency through containerized deployment. Functional and performance testing shows that the system performs stably and reliably under high-concurrency scenarios, with significant advantages compared to similar systems on the market. It can provide efficient and convenient services for electric vehicle users. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"M., Femminella, Mauro; G., Reali, Gianluca","Comparison of Reinforcement Learning Algorithms for Edge Computing Applications Deployed by Serverless Technologies","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202340690&partnerID=40&md5=916cf9c186c30d40952bf4669eac9b7d","Edge computing is one of the technological areas currently considered among the most promising for the implementation of many types of applications. In particular, IoT-type applications can benefit from reduced latency and better data protection. However, the price typically to be paid in order to benefit from the offered opportunities includes the need to use a reduced amount of resources compared to the traditional cloud environment. Indeed, it may happen that only one computing node can be used. In these situations, it is essential to introduce computing and memory resource management techniques that allow resources to be optimized while still guaranteeing acceptable performance, in terms of latency and probability of rejection. For this reason, the use of serverless technologies, managed by reinforcement learning algorithms, is an active area of research. In this paper, we explore and compare the performance of some machine learning algorithms for managing horizontal function autoscaling in a serverless edge computing system. In particular, we make use of open serverless technologies, deployed in a Kubernetes cluster, to experimentally fine-tune the performance of the algorithms. The results obtained allow both the understanding of some basic mechanisms typical of edge computing systems and related technologies that determine system performance and the guiding of configuration choices for systems in operation. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"M., Femminella, Mauro; G., Reali, Gianluca","Application of Proximal Policy Optimization for Resource Orchestration in Serverless Edge Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205088035&partnerID=40&md5=3b136287c7d579190a83a1a1738fbc2d","Serverless computing is a new cloud computing model suitable for providing services in both large cloud and edge clusters. In edge clusters, the autoscaling functions play a key role on serverless platforms as the dynamic scaling of function instances can lead to reduced latency and efficient resource usage, both typical requirements of edge-hosted services. However, a badly configured scaling function can introduce unexpected latency due to so-called “cold start” events or service request losses. In this work, we focus on the optimization of resource-based autoscaling on OpenFaaS, the most-adopted open-source Kubernetes-based serverless platform, leveraging real-world serverless traffic traces. We resort to the reinforcement learning algorithm named Proximal Policy Optimization to dynamically configure the value of the Kubernetes Horizontal Pod Autoscaler, trained on real traffic. This was accomplished via a state space model able to take into account resource consumption, performance values, and time of day. In addition, the reward function definition promotes Service-Level Agreement (SLA) compliance. We evaluate the proposed agent, comparing its performance in terms of average latency, CPU usage, memory usage, and loss percentage with respect to the baseline system. The experimental results show the benefits provided by the proposed agent, obtaining a service time within the SLA while limiting resource consumption and service loss. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"M., Straesser, Martin; P., Haas, Patrick; S., Frank, Sebastian; A., Hakamian, Alireza; A.V., Van Hoorn, André V.; S., Kounev, Samuel","Kubernetes-in-the-Loop: Enriching Microservice Simulation Through Authentic Container Orchestration","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181977399&partnerID=40&md5=27d0e1382e7a4c99fabc0fcfadb9054f","Microservices deployed and managed by container orchestration frameworks like Kubernetes are the bases of modern cloud applications. In microservice performance modeling and prediction, simulations provide a lightweight alternative to experimental analysis, which requires dedicated infrastructure and a laborious setup. However, existing simulators cannot run realistic scenarios, as performance-critical orchestration mechanisms (like scheduling or autoscaling) are manually modeled and can consequently not be represented in their full complexity and configuration space. This work combines a state-of-the-art simulation for microservice performance with Kubernetes container orchestration. Hereby, we include the original implementation of Kubernetes artifacts enabling realistic scenarios and testing of orchestration policies with low overhead. In two experiments with Kubernetes’ kube-scheduler and cluster-autoscaler, we demonstrate that our framework can correctly handle different configurations of these orchestration mechanisms boosting both the simulation’s use cases and authenticity. © 2024 Elsevier B.V., All rights reserved.",Focus on simulation.
"T., Vondra, T.; J., Šedivý, Jan; J.M., Castro, J. M.","Modifying CloudSim to accurately simulate interactive services for cloud autoscaling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014667279&partnerID=40&md5=f443a0cea9fd1f2ddc519d83c698b2dc","We study the problem of autoscaling in cloud deployments and issues related to autoscaling strategies and their formal evaluation. With this goal, we propose modifications of the CloudAnalyst distribution of CloudSim for improved autoscaler simulations. We extend the CloudSim platform with virtual machine addition/removal at simulation run time as well as more granular load reporting and implement a simple autoscaler simulation. We point out and fix multiple problems in CloudSim job duration computation methods and make it consistent with both theory and reality, improving the relative errors of simulation from as high as 333% to single-figure percentages. Our main contribution is a formally evaluated autoscaling strategy and improved version of CloudAnalyst capable of realistic evaluations. © 2021 Elsevier B.V., All rights reserved.",Focus on simulation.
"T., Vondra, T.; J., Šedivý, Jan","Cloud autoscaling simulation based on queueing network model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993990136&partnerID=40&md5=5909592a2cebc0f3146c7fc45af2fde1","For the development of a predictive autoscaler for private clouds, an evaluation method was needed. A survey of available tools was made, but none were found suitable. The CloudAnalyst distribution of CloudSim was examined, but it had accuracy and speed issues. Therefore, a new method of simulation of a cloud autoscaler was devised, with a queueing network model at the core. This method's outputs match those of a load test experiment. It is then evaluated with basic threshold-based algorithms on traces from e-commerce websites taken during Christmas. Algorithms based on utilization, latency, and queue length are assessed and compared, and two more algorithms combining these metrics are proposed. The combination of scaling up based on latency and down based on utilization is found to be very stable and cost-efficient. The next step will be the implementation of predictive methods into the autoscaler, which were already evaluated in the same R language environment. © 2017 Elsevier B.V., All rights reserved.",Focus on simulation.
"X., Zhang, Xuangong; M., Wang, Menglong; X., Gao, Xin; Y., Guan, Yong","Design of an Overall Software Framework for a Plan Summary Evaluation Model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216010273&partnerID=40&md5=294d9ea85303e9f7f1d04cc16a9cb3f6","An overall software framework for a plan summary evaluation model is designed in this paper. The theoretical basis of the proposed design is first illustrated, that is, the objective-capability value chain (project-system-capability-objective) as the plan performance evaluation framework. Subsequently, the functions and key techniques of the software are analyzed including intelligent generalization and extensive optimization of an indicator system dominated by strategic objectives, strategic capability evolution and situation prediction under the digital-analog hybrid drive, system construction deduction and explorative evaluation based on system dynamics, performance evaluation in the whole implementation process of a planned project based on knowledge graph, and holographic risk assessment based on autonomous and controllable supply chain. The basic architecture of the software is further presented after analyzing the characteristics of the microservices architecture. In the end, conclusions are drawn with the future focuses outlined to lay a foundation for the specific software design. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"T., Menouer, Tarek; C., Cérin, Christophe; P., Darmon, Patrice","Reactive Autoscaling of Kubernetes Nodes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201236338&partnerID=40&md5=881514a3277e6fa2fa13be2d304a273e","Kubernetes is undoubtedly the most effective container orchestration system that automates container management with high scalability. It allows running containerized applications on a Kubernetes cluster composed of a set of computing nodes. According to the native Kubernetes operating mode, all nodes in the cluster are used. This massive use of computing resources can lead to resource waste. To address this limitation, we present in this paper a new reactive Kubernetes autoscaler mechanism that allows the number of active computing nodes in a Kubernetes cluster to be controlled dynamically based on several factors. The goal is to reduce resource waste, energy consumption, and the cost of renting a Kubernetes cluster. The idea is to have a pilot that dynamically checks the state of the Kubernetes cluster and scales up or down computing nodes. To select the most pertinent node to add or remove from the Kubernetes cluster, a multi-criteria decision-making (MCDM) algorithm is used. The proposed autoscaler mechanism is offered on top of the Kubernetes framework with minimal changes to make it easy to use with future versions of Kubernetes. Experiments have demonstrated the effectiveness of our solution under different scenarios. The package we provided for the experiments is generic and ready for current Kubernetes flavors. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"C., Courageux-Sudan, Clément; A.Ć., Orgerie, Anne Ćecile; M., Quinson, Martin","Automated performance prediction of microservice applications using simulation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123177041&partnerID=40&md5=25f770af19adbfcbbcd57c25e44a912a","Microservices transform monolithic applications into simple, scalable, and interacting services. It allows for faster development and fine-grained deployments. However, the cooperation of several services leads to intricate dependencies, hindering the detection of performance bottlenecks. Current microservice performance analysis methods require real deployments, a costly process both in time and resources, while performance prediction through simulation relies on models that are complex to develop and instantiate. In this paper, we propose a microservice performance analysis approach based on simulation. Our contribution first introduces a microservice performance model requiring few instantiation parameters. We then propose a methodology to automatically derive model instantiation values from a single execution trace. We evaluate this methodology on two benchmarks from the literature. Our approach accurately predicts the deployment performance of large-scale microservice applications in various configurations from a single execution trace. This provides valuable insights on the performance of an application prior to its deployment on real platform. © 2022 Elsevier B.V., All rights reserved.",Focuses on performance simulation. Does not address load test generation through the proposed model.
"S.M., Sajal, Sultan Mahmud; R., Hasan, Rubaba; T., Zhu, Timothy; B., Urgaonkar, Bhuvan; S., Sen, Siddhartha","TraceSplitter: A new paradigm for downscaling traces","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105355153&partnerID=40&md5=fc6c0041002f844b39a91e6cca318122","Realistic experimentation is a key component of systems research and industry prototyping, but experimental clusters are often too small to replay the high traffic rates found in production traces. Thus, it is often necessary to downscale traces to lower their arrival rate, and researchers/practitioners generally do this in an ad-hoc manner. For example, one practice is to multiply all arrival timestamps in a trace by a scaling factor to spread the load across a longer timespan. However, temporal patterns are skewed by this approach, which may lead to inappropriate conclusions about some system properties (e.g., the agility of auto-scaling). Another popular approach is to count the number of arrivals in fixed-sized time intervals and scale it according to some modeling assumptions. However, such approaches can eliminate or exaggerate the fine-grained burstiness in the trace depending on the time interval length. The goal of this paper is to demonstrate the drawbacks of common downscaling techniques and propose new methods for realistically downscaling traces. We introduce a new paradigm for scaling traces that splits an original trace into multiple downscaled traces to accurately capture the characteristics of the original trace. Our key insight is that production traces are often generated by a cluster of service instances sitting behind a load balancer; by mimicking the load balancing used to split load across these instances, we can similarly split the production trace in a manner that captures the workload experienced by each service instance. Using production traces, synthetic traces, and a case study of an auto-scaling system, we identify and evaluate a variety of scenarios that show how our approach is superior to current approaches. © 2021 Elsevier B.V., All rights reserved.",Focuses on realistic experimentation with limited resources by modifying workload traces. Does not work on load test generation per se.
"S., Frank, Sebastian; M., Straesser, Martin; L., Wagner, Lion; P., Haas, Patrick; A., Hakamian, Alireza; S., Kounev, Samuel; A.V., Van Hoorn, André V.","Simulating Microservice-based Architectures for Resilience Assessment Enriched by Authentic Container Orchestration","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217061378&partnerID=40&md5=93da3e828bd74e7958601fca2470c618","This summary outlines the resilience simulation approach MiSim [Fr22b] published at the International Conference on Software Quality, Reliability and Security (QRS 2022) and its extension for authentic container orchestration [St23] presented at the International Conference on Performance Evaluation Methodologies and Tools (VALUETOOLS 2023). © 2025 Elsevier B.V., All rights reserved.",Focuses on simulation. No load test generation.
"Z., Collins, Zakery; G., De Luca, Gennaro; Y., Chen, Yinong","GPU-accelerated cloud computing services and performance evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011089981&partnerID=40&md5=a5ccf01795f8d4327e894ef64f272f68","This paper explores the feasibility of replacing traditional CPU-based cloud computing with Graphic Processing Unit GPU-accelerated services. Using NVIDIA's CUDA GPU-accelerated C/C++ and Python libraries, we benchmark the performance of GPU computing against multithreaded CPU computing across several domains, including machine learning and large-scale image processing. A novel contribution of this work is an intelligent autoscaling system that maximizes single-GPU resource utilization before scaling to additional GPUs, improving efficiency in cloud-based deployments. Our simulation experiments demonstrate significant performance gains for GPU-accelerated computing and highlight the impact of optimized resource allocation in cloud environments. For example, in a machine learning experiment, using a dataset with 8.790 entries, the execution of a GeForce 3060 ti GPU is 3.42 times faster than a 16-thread CPU computer. Compared with the same 16-thread CPU, Tesla K80 GPU is 4.17 times faster. Furthermore, we provide an analysis of GPU performance optimization strategies, including memory management, concurrency techniques, and workload distribution methodologies, offering insights into the long-term scalability and cost-effectiveness of GPU-accelerated cloud infrastructure. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"A., Alanda, Alde; H.A., Mooduto, Hanriyawan Adnan; H., Amnur, Hidra; M., Fadhel, Muhammad","Scaling Kubernetes Architectures for High Availability in Cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010142992&partnerID=40&md5=b10e0206d09a49da5d57247f5144bfe3","The shift from monolithic to microservices architectures has amplified the adoption of containerized applications, driving the need for efficient container orchestration platforms like Kubernetes. This paper presents a high-availability (HA) architecture for Kubernetes deployed on Amazon Web Services (AWS) to ensure seamless and resilient application performance in cloud environments. Key components of this architecture include load balancing, autoscaling, and centralized configuration management. The deployment leverages Amazon RDS for database reliability, Prometheus and Grafana for monitoring, and Horizontal Pod Autoscaling (HPA) to dynamically adjust resource allocation. Testing indicates that the system maintains high performance under various load conditions, with minimal downtime and rapid recovery from node failures. The proposed framework offers an adaptable and scalable solution for managing high-demand applications, providing a foundation for future optimizations under extreme user loads. The results shows that the system maintains a high success rate across various user loads and time intervals, with a minor drop under maximum load conditions. This behavior aligns with expected performance scaling in distributed systems, where resource allocation adjustments (Pods and worker nodes) enable high reliability across different user demands. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"A., Jindal, Anshul; V., Podolskiy, Vladimir; M., Gerndt, Michael","Performance modeling for cloud microservice applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064819069&partnerID=40&md5=c644d5d34d0005d91c9c83c369ae2c4e","Microservices enable a fine-grained control over the cloud applications that they constitute and thus became widely-used in the industry. Each microservice implements its own functionality and communicates with other microservices through language- and platform-agnostic API. The resources usage of microservices varies depending on the implemented functionality and the workload. Continuously increasing load or a sudden load spike may yield a violation of a service level objective (SLO). To characterize the behavior of a microservice application which is appropriate for the user, we define a MicroService Capacity (MSC) as a maximal rate of requests that can be served without violating SLO. The paper addresses the challenge of identifying MSC individually for each microservice. Finding individual capacities of microservices ensures the flexibility of the capacity planning for an application. This challenge is addressed by sandboxing a microservice and building its performance model. This approach was implemented in a tool Terminus. The tool estimates the capacity of a microservice on different deployment configurations by conducting a limited set of load tests followed by fitting an appropriate regression model to the acquired performance data. The evaluation of the microservice performance models on microservices of four different applications shown relatively accurate predictions with mean absolute percentage error (MAPE) less than 10%. The results of the proposed performance modeling for individual microservices are deemed as a major input for the microservice application performance modeling. © 2019 Elsevier B.V., All rights reserved.",Identification of maximum capacity for each microservice through performance models through deployment configurations. No generation of time-dependent tests.
"W., Yang, Wentao; J., Zhang, Jiaxin; L., Jin, Lianwen","AutoScaler: Self scale alignment for handwritten mathematical expression recognition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013657810&partnerID=40&md5=44e45a8a1d5eb624b2916039524355c0","Handwritten mathematical expression recognition (HMER) remains a challenging task in computer vision due to the complex spatial structures and diverse handwriting styles of mathematical expressions. While current approaches focus heavily on decoder modeling, the critical influence of input scale variations on recognition accuracy has been largely overlooked. This paper introduces AutoScaler, a novel scale-adaptive framework that addresses the issue of scale misalignment between input images and decoders for the first time in HMER. Our method comprises two novel components: (1) Stochastic Scale Training, a training strategy that enhances the model's robustness to scale variations by enabling it to perceive mathematical expressions across diverse scales; (2) Optimal Scale Estimation, an inference technique that identifies the most suitable scale for each input image, ensuring proper alignment of visual features with decoder. Additionally, we propose Cross-scale Joint Approximation, which leverages complementary information between optimal and sub-optimal scales to further improve recognition performance. Extensive experiments on real-world datasets show our method achieves state-of-the-art results for both single-line and multi-line expressions, underscoring its effectiveness in advancing HMER. The code and model are available at https://github.com/SCUT-DLVCLab/AutoScaler. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"M.M., Bayas, Marcia M.; Á.P., Fernández, Ángela Parra; R.H., Rovira, Ronald H.; M., Montaño-Blacio, Manuel; Ó.W., Gómez-Morales, Óscar Wladimir; J., Figueroa, Junior","IOT AND MQTT-BASED CARDIOVASCULAR PARAMETER MONITORING SYSTEM FOR MEDICAL ALERTS; SISTEMA DE MONITOREO DE PARÁMETROS CARDIOVASCULARES BASADO EN IOT Y MQTT PARA ALERTAS MÉDICAS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010896118&partnerID=40&md5=c016b0a20f2cd514bd21a92de4a12784","This paper presents the development of a computing platform for the real-time monitoring of cardiovascular parameters derived from bioelectrical signals. A comprehensive analysis of primary users was con-ducted, leading to the identification of both technical and functional requirements. The interface design was guided by Sommerville’s methodology. The system architecture is based on a microservices model, incorporating a relational database and enabling inte-gration with data transmitted from Internet of Things (IoT) devices. The platform was evaluated through incremental stress testing, starting with zero users and increasing in steps of 100 up to 5,000. A total of 22,132 requests were processed at a peak rate of 440.4 requests per second, with an average response time of 930 ms and 95% of responses occurring within 2,300 ms. The system demonstrated error-free performance with up to 1,700 concurrent users. At 5,000 users and 26,393 total requests, a minimal error rate of 0.16% was recorded, confirming the platform’s sta-bility under high workloads. These findings validate the feasibility of the proposed solution for remote biomedical monitoring, offering an efficient, scalable, and robust tool for real-time health supervision. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"A., de Camargo, André; R.S., Dos Santos Mello, Ronaldo S.; I.L., Salvadori, Ivan Luiz; F.A., Siqueira, Frank Augusto","An Architecture to Automate Performance Tests on Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014922947&partnerID=40&md5=d8da434195d3736c2276600615658922","The microservices architecture provides a new approach to develop applications. As opposed to monolithic applications, in which the application comprises a single software artifact, an application based on the microservices architecture is composed by a set of services, each one designed to perform a single and well-defined task. These services allow the development team to decouple several parts of the application using different frameworks, languages and hardware for each part of the system. One of the drawbacks for adopting the microservices architecture to develop applications is testability. In a single application test boundaries can be more easily established and tend to be more stable as the application evolves, while with microservices we can have a set of hundreds of services that operate together and are prone to change more rapidly. Each one of these services needs to be tested and updated as the service changes. In addition, the different characteristics of these services such as languages, frameworks or the used infrastructure have to be considered in the testing phase. Performance tests are applied to assure that a particular software complies with a set of non-functional requirements such as throughput and response time. These metrics are important to ensure that business constraints are respected and to help finding performance bottlenecks. In this paper, we present a new approach to allow the performance tests to be executed in an automated way, with each microservice providing a test specification that is used to perform tests. Along with the architecture, we also provide a framework that implements some key concepts of this architecture. This framework is available as an open source project1. © 2017 Elsevier B.V., All rights reserved.","Load test automation, No load test generation."
"B.M., Beena, B. M.; P.C., Ranga, Prashanth Cheluvasai; V., Chowdary, Vinitha; R., Gamidi, Rohan; M., Hemasri, Marisetti; T., Muppala, Tejaswi","Green Video Transcoding in Cloud Environments Using Kubernetes: A Framework With Dynamic Renewable Energy Allocation and Priority Scheduling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010057693&partnerID=40&md5=e4f5ec353734d1441b7afcf4b5b31d8d","Video content continues to be a major source of Internet traffic, with a growing demand for high-quality, on-demand videos. This leads to significant energy consumption across cloud servers. Conserving energy and improving energy efficiency in cloud servers is a major challenge. The growing demand for video transcoding services and increasing concerns over energy consumption necessitate systems that balance processing power with energy usage. The research addresses these challenges by developing a green, energy-aware video transcoding system that predicts energy availability from renewable sources (solar and wind) using machine learning techniques and optimizes tasks allocation. The system utilizes a Kubernetes-managed backend to dynamically scale resources for FFmpeg-based transcoding while prioritizing renewable energy, minimizing grid usage utilizing the advanced machine learning models, including Random Forest, XGBoost, and CatBoost, predict energy production and guide task assignments. The integration of predictive analytics with Kubernetes’ Horizontal Pod Autoscaler (HPA) allows dynamic workload distribution, ensuring optimal energy utilization. Additionally, the system incorporates real-time energy monitoring to adjust task scheduling based on fluctuations in renewable energy availability. Two novel scheduling algorithms, Dynamic Renewable Energy Allocation (DREA) and Energy-Aware Priority Scheduling (EAPS), enhance energy efficiency. DREA allocates tasks to energy zones based on real-time renewable availability, while EAPS prioritizes tasks by urgency and energy needs, deferring low-priority tasks to periods of high renewable availability. These green strategies minimize reliance on non-renewable sources while maintaining performance and scalability. The system’s modular design allows easy integration with various cloud platforms, increasing its applicability in real-world scenarios. Furthermore, extensive scalability tests demonstrate that the proposed approach maintains efficient task execution even under high workloads, making it suitable for large-scale cloud environments. By reducing energy consumption and carbon footprint, this framework contributes to the advancement of sustainable cloud computing solutions. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Ding, Zhijun; Y., Xu, Yuehao; B., Feng, Binbin; C., Jiang, Changjun","Microservice Extraction Based on a Comprehensive Evaluation of Logical Independence and Performance","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188943693&partnerID=40&md5=0350efeb000564b96f2acb88f3e96ca2","Monolithic architectures are becoming increasingly difficult to cope with complex applications, and microservice architectures, which offer flexibility and logical independence in development and maintenance, are the new choice for companies and developers. Migrating a legacy monolithic architecture application to a microservice architecture rather than building it from scratch is considered an easy way to use it. To ensure that the migrated microservice applications can take advantage of their benefits, we need to propose a reasonable and effective microservice extraction method. Considering the single responsibility principle in the microservice design principle, most existing microservice extraction methods only pursue the high logical independence of the extraction results and pay little attention to whether the extraction results have good performance. Applications need to perform well, and studies have shown that poor microservice extraction schemes can negatively impact the performance of the migrated application. As a result, when extracting, we should also consider the performance of the results. A few studies consider the performance of extraction results, but only in terms of a few factors affecting performance, such as network overhead, rather than considering all factors affecting performance comprehensively, which leads to an inaccurate evaluation of performance. Therefore, oriented toward the most widely used managed languages today, we propose an effective Microservice Extraction method based on a Comprehensive Evaluation of logical independence and performance (MECE). Firstly, we propose a workflow-based approach to evaluate the performance of microservice extraction results by considering multiple influencing factors, focusing on the management cost ignored in existing studies, and designing an effective management cost evaluation model. After that, we propose a meta-heuristic search-based algorithm to obtain feasible microservice extraction results. In experiments based on actual deployments, the extraction results of the MECE method obtained a performance improvement of up to 46.15% without significant loss of logical independence compared to existing methods, which verifies the effectiveness of the method. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"D., Kim, Donggyun; H., Kim, Hyungjun; E., Lee, Eunyoung; H., Yu, Heonchang","LARE-HPA: Co-optimizing Latency and Resource Efficiency for Horizontal Pod Autoscaling in Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213010994&partnerID=40&md5=8573a619fe0c2a4299f1fc8fa29da029","Autoscaling is a technology that dynamically adjusts computing resources based on fluctuating demand without user intervention. It plays a crucial role in optimizing resources while maintaining Quality of Service (QoS). Kubernetes, an open-source container orchestration platform, manages resource scaling through a technique known as Horizontal Pod Autoscaler (HPA). However, HPA is a reactive scaler that makes scaling decisions based on static threshold values, which are challenging to set without profiling the application’s characteristics and workload volatility in advance. Incorrect configuration values can lead to over- or under-provisioning, resulting in excessive costs or Service Level Objective (SLO) violations. To address these challenges, we propose LARE-HPA, an adaptive scaling solution that adjusts thresholds and stabilization windows to optimize scaling timing, ensuring QoS while minimizing resource over-provisioning. Our experimental results based on real-world workloads demonstrate that LARE-HPA reduces average latency by 50.34%, 39.52%, and 46.18% while improving SLO satisfaction rates by 3.61%, 4.5%, and 2.21% compared to existing HPA and other state-of-the-art techniques, respectively. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"N., Fu, Nan; G., Cheng, Guang; Y., Teng, Yue; G., Dai, Guangye; S., Yu, Shui; Z., Chen, Zihan","Intelligent Root Cause Localization in MicroService Systems: A Survey and New Perspectives","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011248019&partnerID=40&md5=2b93705456a37b7a375116361986e1bf","Root cause localization is the process of monitoring system behavior and analyzing fault patterns from behavioral data. It is applicable in software development, network operations, and cloud computing. However, with the advent of microservice architectures and cloud-native technologies, root cause localization becomes an arduous task. Frequent updates in systems result in large-scale data and complex dependencies. Traditional analysis methods relying on manual experience and predefined rules have limited data processing and cannot learn new fault patterns from historical knowledge. Artificial Intelligence techniques have emerged as powerful tools to leverage historical knowledge and are now widely used in root cause localization. In this article, we provide a structured overview and a qualitative analysis of root cause localization in microservice systems. To begin with, we review the literature in this area and abstract a workflow of root cause localization, including multimodal data collection, intelligent root cause analysis, and performance evaluation. In particular, we highlight the role played by Artificial Intelligence techniques. Finally, we discuss some open challenges and research directions and propose an end-to-end framework from a new perspective, providing insights for future works. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"P., Zheng, Pengcheng; J., Liu, Jialong; Y., Liao, Yuquan; B., Zhang, Beilei; R., Wang, Ruolin","Intelligent Detection and Processing Technology of Abnormal Data in Low-Voltage Distribution Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008336770&partnerID=40&md5=0f098f83a105b6959e6407c005d6f185","Anomaly detection in distribution networks is crucial for ensuring the stable operation of power systems. To improve the accuracy and efficiency of anomaly identification, we designed a distribution network anomaly detection system based on deep learning. This system employs an improved LSTM model, incorporating attention mechanisms and residual connections, significantly enhancing model performance. The application of a microservice architecture achieved high scalability and real-time processing capabilities. Experimental results show that the improved model reduces the root mean square error by 23.5% on actual datasets, with an average detection time of only 50 milliseconds per instance. During 30 days of actual operation, the system achieved an accuracy of 98.7% and an F1 score of 98.3%. Performance evaluations demonstrate the system's excellent real-time capabilities, scalability, and reliability. These achievements provide strong support for the intelligent management of distribution networks, significantly improving grid operation efficiency and reliability. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"J., Zerwas, Johannes; P., Kramer, Patrick; R.M., Ursu, Razvan Mihai; N., Asadi, Navidreza; P., Rodgers, Phil; L., Wong, Leon; W., Kellerer, Wolfgang","KAPETÁNIOS: Automated Kubernetes Adaptation through a Digital Twin","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142938898&partnerID=40&md5=1180173c00f1ab1b08ad3926f2497d8e","This demo presents a self-operating Kubernetes (K8s) cluster that uses digital twinning and machine learning to autonomously adapt its Horizontal Pod Autoscaler (HPA) to workload changes. The demo uses a digital twin of a K8s cluster to gather performance statistics and learn a model for the workload. With the model, the cluster autonomously adjusts HPA parameters for better performance. The demo illustrates this process and shows that the requested pod seconds decrease by 37 %, while the request latency stays mostly unaffected. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"L.M.D., da Silva, Lucileide M.D.; P.V.A., Alves, Pedro V.A.; S.N., Silva, Sergio N.; M.A., Fernandes, M. A.C.","Adaptive horizontal scaling in kubernetes clusters with ANN-based load forecasting","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217279068&partnerID=40&md5=c3fff703574fee953b3a7aae67cfbaf4","In modern cloud environments, efficient management of computational resources is a critical challenge due to the growing demand for scalable and high-performance applications. Horizontal scaling in Kubernetes (K8s) clusters is essential for dynamically adjusting resources to match workload demands. However, their reactive nature often limits traditional autoscaling methods like K8s Horizontal Pod Autoscaler (HPA), leading to inefficiencies under variable loads. To overcome these limitations, more advanced and adaptive scaling approaches are needed. Thus, this study introduces an adaptive approach to horizontal scaling in K8s clusters using Artificial Neural Networks (ANNs) for load forecasting, referred to as ANN-HS. The proposed method aims to enhance the efficiency of resource consumption and optimize replica allocation compared to the standard HPA. By leveraging pre-trained regression models, ANN-HS dynamically adjusts resources to meet varying demands, ensuring adherence to latency requirements and improving overall system performance. Experimental results demonstrate that ANN-HS outperforms traditional HPA methods, offering a scalable and flexible solution for managing microservices in cloud environments. This approach provides a robust framework for optimizing horizontal scaling in Kubernetes, contributing to the advancement of intelligent resource management in cluster computing. Experimental results show that ANN-HS significantly improves resource utilization compared to Kubernetes’ HPA. Specifically, ANN-HS reduces CPU consumption by approximately 50% while maintaining Service Level Agreement (SLA) compliance with an average violation rate of less than 10%. Additionally, ANN-HS reduces the number of replicas needed by 66.67%, optimizing resource allocation under varying load conditions. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"K., Peng, Kai; J., Rao, Jie; H., Li, Hao; Y., Hu, Yi; B., Jin, Bo; T., Zheng, Tianyue; M., Hu, Menglan","Global Microservice Autoscaling Over Heterogeneous Edge Environments for Internet Applications: A Reinforcement Learning Approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011838730&partnerID=40&md5=29556c4f9c8eebf81a8e1d3811aae69a","The integration of microservice architecture and edge computing offers innovative solutions for highly interactive, low-latency Internet applications. To manage the dynamic nature of requests in edge computing, microservice autoscaling techniques are frequently employed. However, the resource limitation of individual edge servers and the heterogeneity among edge servers present significant challenges for autoscaling in edge computing. Meanwhile, few studies have considered the long-term optimization and the joint optimization of instance adjustment and request routing in edge computing. This paper aims to fill these gaps. First, we propose Global Horizontal Pod Autoscaler (GHPA), a novel framework that addresses microservice autoscaling from the perspective of edge server clusters. Second, we consider the joint optimization of instance adjustment and request routing, and formulate a long-term optimization problem. Third, we transform the long-term optimization problem into a Markov Decision Problem (MDP) and use reinforcement learning techniques to solve it. Finally, we conduct extensive experiments using both real and synthetic data. The experiment results demonstrate that our algorithm achieves at least a 10% performance improvement in various test environments compared to state-of-the-art algorithms. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"J., Sychowiec, Jakub; Z.E., Zieliński, Zbigniew E.","A Blockchain-Based Framework for Secure Data Stream Dissemination in Federated IoT Environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006731582&partnerID=40&md5=12446c74866daaeab5a5d490f6a83ccd","An industrial-scale increase in applications of the Internet of Things (IoT), a significant number of which are based on the concept of federation, presents unique security challenges due to their distributed nature and the need for secure communication between components from different administrative domains. A federation may be created for the duration of a mission, such as military operations or Humanitarian Assistance and Disaster Relief (HADR) operations. These missions often occur in very difficult or even hostile environments, posing additional challenges for ensuring reliability and security. The heterogeneity of devices, protocols, and security requirements in different domains further complicates the requirements for the secure distribution of data streams in federated IoT environments. The effective dissemination of data streams in federated environments also ensures the flexibility to filter and search for patterns in real-time to detect critical events or threats (e.g., fires and hostile objects) with changing information needs of end users. The paper presents a novel and practical framework for secure and reliable data stream dissemination in federated IoT environments, leveraging blockchain, Apache Kafka brokers, and microservices. To authenticate IoT devices and verify data streams, we have integrated a hardware and software IoT gateway with the Hyperledger Fabric (HLF) blockchain platform, which records the distinguishing features of IoT devices (fingerprints). In this paper, we analyzed our platform’s security, focusing on secure data distribution. We formally discussed potential attack vectors and ways to mitigate them through the platform’s design. We thoroughly assess the effectiveness of the proposed framework by conducting extensive performance tests in two setups: the Amazon Web Services (AWS) cloud-based and Raspberry Pi resource-constrained environments. Implementing our framework in the AWS cloud infrastructure has demonstrated that it is suitable for processing audiovisual streams in environments that require immediate interoperability. The results are promising, as the average time it takes for a consumer to read a verified data stream is in the order of seconds. The measured time for complete processing of an audiovisual stream corresponds to approximately 25 frames per second (fps). The results obtained also confirmed the computational stability of our framework. Furthermore, we have confirmed that our environment can be deployed on resource-constrained commercial off-the-shelf (COTS) platforms while maintaining low operational costs. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"N., Ramachandran, Nivetha; M., Thirumaran, Murthy","AI-Enhanced Pod Scheduling: Optimizing MERN and MEAN Stack Performance in Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007989090&partnerID=40&md5=c9fa680036db096100abbdf298f99ff2","Microservices architecture has become the foundation of modern application development, offering enhanced scalability, flexibility, and maintainability. This paper presents a comparative performance evaluation of two widely used JavaScript-based stacks, MERN (MongoDB, Express, React, Node.js) and MEAN (MongoDB, Express, Angular, Node.js), deployed as microservices applications in Minikube, a local Kubernetes environment. While both stacks share a common backend technology stack, they differ in their front-end frameworks - React in MERN and Angular in MEAN - which influences their performance, scalability, and suitability for different application types. This study not only compares the architectural differences between MERN and MEAN but also introduces machine learning (ML)-based microservice optimization to enhance service instance selection and resource allocation in Kubernetes. The optimization leverages RF (Random Forest) and XGBoost ML algorithms for dynamic intelligent POD selection and scheduling, ensuring dynamic routing of microservice requests based on real-time performance metrics. Key evaluation criteria include latency, response time, CPU and memory utilization, scalability, and fault tolerance, monitored using Prometheus and Istio. The results provide insights into the efficiency of each stack in handling microservices workloads, helping developers and system architects choose the most suitable stack based on performance demands, resource constraints, and deployment complexity. The paper concludes with recommendations for optimizing microservices-based applications in Kubernetes environments using ML-driven orchestration strategies. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"K.S., Pokkuluri, Kiran Sree; P., Sarkar, Paramita; V., Birchha, Vijay; S.K., Mathariya, Sandeep Kumar; V., Veeramachaneni, Vinod; S., Singh, Suman; V., Roy, Vandana","Intelligent Reasonable Optimization for Virtual Machine Provisioning in Hybrid Cloud Using Fuzzy AHP and Cost-Effective Autoscaling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013578516&partnerID=40&md5=ce0dd286a533603a475e514833c4d8cd","In this research paper, a new Fuzzy Analytic Hierarchy Process (FAHP) based Budget Optimized Virtual Machine Provisioning (BOPVM) framework is proposed to solve the problem of which CSP (cloud service providers) should be chosen to orchestrate resource provisioning in a hybrid cloud. The framework is divided into two phases. In Phase 1, the ratings of the CSPs are established depending on several parameters including the application workload and environment, the desired QoS parameters, and the cost considerations by applying the FAHP extent analysis method. In Phase 2, the BOPVM algorithm is used for resource allocation given pre-defined budgets, by proactively predicting the workload and identifying the optimal CSP and VM (Virtual Machine) combination. The decision criteria discussed include the performance of the identified CSPs and vendor scale, reliability, and cost optimization for evaluating CSPs about the proposed framework. It guarantees resource provisioning and scaling for varied application zones, production and non-production. The proposed methodology performs better than existing methods such as PRMF and DRPM by cutting virtual machine provisioning costs by up to 61.1% and time by up to 80%. Based on the above performance evaluation, the proposed BOPVM framework achieves the best result in terms of workload forecasts and the distribution of future workloads across the multiple CSPs and budget constraints. This novelty suggests its application can revolutionize hybrid cloud resources’ efficient allocation by prioritizing its effective and cost-efficient usage. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Zhang, Zhiyao; A., Megargel, Alan; L., Jiang, Lingxiao","Performance Evaluation of NewSQL Databases in a Distributed Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215230604&partnerID=40&md5=a876c2dba66436dff173f5d573c5a591","In the last decade, application architectures have evolved drastically, moving from monolithic architectures to distributed architectures where deployment has shifted from dedicated on-premises servers to the cloud. Distributed architectures and cloud computing has enabled businesses to scale their application components across different geographical locations. While it is easy to scale the application layer, scaling its database layer that relies on traditional SQL databases is challenging and often is a common source of bottlenecks when it comes to application performance. This paper evaluates the performance characteristics between two NewSQL databases solutions, MySQL NDB Cluster vs. TIBCO ActiveSpaces IMDG. Serving as an application layer, a simulation of banking microservices, handling loyalty rewards, is developed to interface with these databases to derive additional insights and comparison. In the context of a geographically distributed architecture, our experiment results show that MySQL NDB Cluster is more suited for critical business transactions requiring ACID properties while TIBCO ActiveSpaces is better suited for use cases that require big data ingestion, such as IoT platforms. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"Y., Liang, Yi; Y., Wang, Yinzhou; G., Xu, Guimei; N., Ruan, Nianyi","DyTrace: Capturing Dynamic Call Dependencies in Microservice Anomaly Using the Spatial-Temperal Attention Mechanism","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012107898&partnerID=40&md5=ee045212cc928301df3c9fc595b743ff","Microservice architecture is widely used in the Internet of Things (IoT) and cloud-native domains. As the scale of micro services systems increases and the inter-service call dependencies become more complex, anomaly detection in microservices has become a technical challenge for data centers. Most existing work relies on metric and trace for anomaly detection. However, these approaches do not account for the dynamic changes in the call dependencies between microservice instances in concurrent request scenarios, making it difficult to perceive the propagation patterns of anomalies between microservices and reducing detection accuracy. This paper proposes a spatial-temporal attention network called DyTrace for microservice anomaly detection. The proposed method first preprocesses metric and trace data to construct a microservice system state graph, then uses a graph attention neural network to learn the micro service spatial dependencies at each time point. A temporal attention network is employed to capture the time-dependent relationships of the system state, and finally, anomaly detection is performed based on Deep Support Vector Data Description (Deep SVDD). Performance evaluation results on the AIOps2022 dataset show that DyTrace outperforms the current state-of-the-art methods, with an average improvement of 20.9% in Precision, 22.8% in Recall, and 21.7% in F1 Score. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"A.A., Avritzer, Alberto A.; A.A., Janes, Andrea A.; C., Trubiani, Catia; H., Rodrigues, Helena; Y., Cai, Yuanfang; D.S., Menasché, Daniel Sadoc; A., Jose Abreu de Oliveira, Alvaro","Architecture and Performance Anti-patterns Correlation in Microservice Architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005026744&partnerID=40&md5=75d58af188b2205f548de6e65cd26f03","Microservice architecture design requires the architect to meet the needs of multiple stakeholders and to address their needs for maintainability, scalability, and availability. In the microservice architecture context, a comprehensive performance and scalability assessment is a dynamic activity, which is focused on the detection of service level metric deviations from objectives using a defined operational profile. Root cause analysis is focused on the identification of the activated microservice components given the defined load profile. Therefore, performance issues are identified by detecting dynamic deviations from the expected behaviors of the service level metric.In contrast, microservice architecture assessment focus is on identifying implicit relations among microservice components. Architecture anti-patterns are identified by detecting deviations from the defined formal design patterns. As the ultimate objective of microservice architecture design is to build high-quality applications it would be expected that architecture refactoring based on the removal of architecture anti-patterns will result in meeting stakeholder needs of better scalability and availability.In this paper we present an empirical assessment of architecture anti-pattern detection in combination with the identification of performance issues using two state of the art tools: DV8 for architecture and PPTAM for performance. We make use of Train Ticket, i.e., a benchmark microservice system, and we observed the co-occurrence of architectural (Clique) and performance (Blob) anti-patterns, noting that high coupling shows much worse performance scores. We have found strong correlation between the normalized distance performance metric and architecture coupling values using several similarity metrics. Our empirical results show that operational profile based performance testing and analysis can be used to help prioritize architecture refactoring. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"M., Adeppady, Madhura; A., Conte, Alberto; P., Giaccone, Paolo; H., Karl, Holger; C.F., Chiasserini, C. Fabiana","Dynamic Management of Constrained Computing Resources for Serverless Services","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003922178&partnerID=40&md5=cbad47ea0b2266b7a1288fdb14c11929","In resource-constrained cloud systems, e.g., at the network edge or in private clouds, serverless computing is increasingly adopted to deploy microservices-based applications, leveraging its promised high resource efficiency. Provisioning resources to serverless services, however, poses several challenges, due to the high cold-start latency of containers and stringent Service Level Agreement (SLA) requirements of the microservices. In response, we investigate the behavior of containers in different states (i.e., running, warm, or cold) and exploit our experimental observations to formulate an optimization problem that minimizes the energy consumption of the active servers while reducing SLA violations. In light of the problem complexity, we propose a low-complexity algorithm, named AiW, which utilizes a multi-queueing approach to balance energy consumption and system performance by reusing containers effectively and invoking cold-starts only when necessary. To further minimize the energy consumption of data centers, we introduce the two-timescale COmputing resource Management at the Edge (COME) framework, comprising an orchestrator running our proposed AiW algorithm for container provisioning and Dynamic Server Provisioner (DSP) for dynamically activating/deactivating servers in response to AiW’s decisions on request scheduling. COME addresses the mismatch in timescales for resource provisioning decisions at the container and server levels. Extensive performance evaluation through simulation shows AiW’s close match to the optimum and COME’s significant reduction in power consumption by 22–64% compared state-of-the-art alternatives. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"A., Marchese, Angelo; O., Tomarchio, Orazio","SLO and Cost-Driven Container Autoscaling on Kubernetes Clusters","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003532682&partnerID=40&md5=4008be8c0816ed8c6fc3a9e14c9d1fb1","Modern web services must meet critical non-functional requirements such as availability, responsiveness, scalability, and reliability, which are formalized through Service Level Agreements (SLAs). These agreements specify Service Level Objectives (SLOs), which define performance targets like uptime, latency, and throughput, essential for ensuring consistent service quality. Failure to meet SLOs can result in penalties and reputational damage. Service providers also face the challenge of avoiding over-provisioning resources, as this leads to unnecessary costs and inefficient resource use. To address this, autoscaling mechanisms dynamically adjust the number of service replicas to match user demand. However, traditional autoscaling solutions typically rely on low-level metrics (e.g., CPU or memory usage), making it difficult for providers to optimize both SLOs and infrastructure costs. This paper proposes an enhanced autoscaling methodology for containerized workloads in Kubernetes clusters, integrating SLOs with a cost-driven autoscaling policy. This approach overcomes the limitations of conventional autoscaling by making more efficient decisions that balance service-level requirements with operational costs, offering a comprehensive solution for managing containerized applications and their infrastructure in Kubernetes environments. The results, obtained by evaluating a prototype of our system in a testbed environment, show significant advantages over the vanilla Kubernetes Horizontal Pod Autoscaler. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"F., Xiao, Fu; W., Fan, Weibei; L., Han, Lei; T., Qiu, Tie; X.Z., Cheng, Xiuzheng Zhen","Joint Service Deployment and Task Offloading for Datacenters With Edge Heterogeneous Servers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003041472&partnerID=40&md5=52ed5b308a710fed0347e2e1248f33de","Mobile edge computing (MEC) can improve execution efficiency and reduce overhead for offloading computing tasks to edge servers with more resources. In the microservice system, the current research only considers the cross segment communication cost of computing tasks, does not consider the case of the same end, and ignores the discovery and invocation optimization of associated services. In this paper, we propose CACO, which is a novel content-aware classification offloading framework for MEC based on correlation matrix. CACO first designs an adaptive service discovery model, which can make timely response and adjustment to the changes of the external environment. It then investigates an efficient affinity matrix based service discovery algorithm, which expresses the association relationship between services by constructing a service association matrix. In addition, CACO constructs a relational model by giving different weight coefficients to the delay and energy loss, which improves the delay and energy loss of message processing in a satisfying manner. Simulation results indicate that CACO reduces the total traffic of redundant messages by 46.2% ~76.5%, respectively compared with state-of-the-art solutions. Testbed benchmarks show that it can also improve the stability by reducing control overhead by 34.5% ~81.6% . © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"V., Sachidananda, Vighnesh; A., Sivaraman, Anirudh","Erlang: Application-Aware Autoscaling for Cloud Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191955612&partnerID=40&md5=1c3ac60f52979d499844ea930ede78fb","As cloud applications shift from monoliths to loosely coupled microservices, application developers must decide how many compute resources (e.g., number of replicated containers) to assign to each microservice within an application. This decision affects both (1) the dollar cost to the application developer and (2) the end-to-end latency perceived by the application user. Today, individual microservices are autoscaled independently by adding VMs whenever per-microservice CPU or memory utilization crosses a configurable threshold. However, an application user's end-to-end latency consists of time spent on multiple microservices and each microservice might need a different number of VMs to achieve an overall end-to-end latency. We present Erlang, an autoscaler for microservice-based applications, which collectively allocates VMs to microservices with a global goal of minimizing dollar cost while keeping end-to-end application latency under a given target. Using 5 open-source applications, we compared Erlang to several utilization and machine learning based autoscalers. We evaluate Erlang across different compute settings on Google Kubernetes Engine (GKE) in which users manage compute resources, GKE standard, and a new mode of operation in which the cloud provider manages compute infrastructure, GKE Autopilot. Erlang meets a desired median or tail latency target on 53 of 63 workloads where it provides a cost reduction of 19.3%, on average, over the next cheapest autoscaler. Erlang is the most cost effective autoscaling policy for 48 of these 53 workloads. The cost savings from managing a cluster with Erlang result in Erlang paying for its training cost in a few days. On smaller applications, for which we can exhaustively search microservice configurations, we find that Erlang is optimal for 90% of cases and near optimal otherwise. Code for Erlang is available at https://github.com/vigsachi/erlang © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"O., Pozdniakova, Olesia; D., Mažeika, Dalius; A., Cholomskis, Aurimas","SLA-Adaptive Threshold Adjustment for a Kubernetes Horizontal Pod Autoscaler","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190151355&partnerID=40&md5=2ff2e9223259f9d4992b263b6ff3c13d","Kubernetes is an open-source container orchestration system that provides a built-in module for dynamic resource provisioning named the Horizontal Pod Autoscaler (HPA). The HPA identifies the number of resources to be provisioned by calculating the ratio between the current and target utilisation metrics. The target utilisation metric, or threshold, directly impacts how many and how quickly resources will be provisioned. However, the determination of the threshold that would allow satisfying performance-based Service Level Objectives (SLOs) is a long, error-prone, manual process because it is based on the static threshold principle and requires manual configuration. This might result in underprovisioning or overprovisioning, leading to the inadequate allocation of computing resources or SLO violations. Numerous autoscaling solutions have been introduced as alternatives to the HPA to simplify the process. However, the HPA is still the most widely used solution due to its ease of setup, operation, and seamless integration with other Kubernetes functionalities. The present study proposes a method that utilises exploratory data analysis techniques along with moving average smoothing to identify the target utilisation threshold for the HPA. The objective is to ensure that the system functions without exceeding the maximum number of events that result in a violation of the response time defined in the SLO. A prototype was created to adjust the threshold values dynamically, utilising the proposed method. This prototype enables the evaluation and comparison of the proposed method with the HPA, which has the highest threshold set that meets the performance-based SLOs. The results of the experiments proved that the suggested method adjusts the thresholds to the desired service level with a 1–2% accuracy rate and only 4–10% resource overprovisioning, depending on the type of workload. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"M., Sabuhi, Mikael; P., Musílek, Petr; C.P., Bezemer, Cor Paul","Micro-FL: A Fault-Tolerant Scalable Microservice-Based Platform for Federated Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188666774&partnerID=40&md5=26db29bcdfefff6c2864a9b55bd70b46","As the number of machine learning applications increases, growing concerns about data privacy expose the limitations of traditional cloud-based machine learning methods that rely on centralized data collection and processing. Federated learning emerges as a promising alternative, offering a novel approach to training machine learning models that safeguards data privacy. Federated learning facilitates collaborative model training across various entities. In this approach, each user trains models locally and shares only the local model parameters with a central server, which then generates a global model based on these individual updates. This approach ensures data privacy since the training data itself is never directly shared with a central entity. However, existing federated machine learning frameworks are not without challenges. In terms of server design, these frameworks exhibit limited scalability with an increasing number of clients and are highly vulnerable to system faults, particularly as the central server becomes a single point of failure. This paper introduces Micro-FL, a federated learning framework that uses a microservices architecture to implement the federated learning system. It demonstrates that the framework is fault-tolerant and scalable, showing its ability to handle an increasing number of clients. A comprehensive performance evaluation confirms that Micro-FL proficiently handles component faults, enabling a smooth and uninterrupted operation. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"M., Xu, Min","Training Management System Based on Microservice Framework Technology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000011287&partnerID=40&md5=cf72962582d6d85e9cd4d9760d246b50","To address the shortcomings of existing training management systems, such as high complexity, poor scalability, and inefficient development and configuration, a specialized training management system has been designed and implemented. In this system, the microservice architecture is the core technology of the platform. Microservices were developed and deployed using Spring Cloud and Docker, enabling each service to be independently developed, tested, and deployed. Network topology balancing was achieved through Nginx, and service discovery and registration were handled by Eureka. On this basis, a message queuing mechanism was established using the Kafka protocol, and Redis caching technology was used to improve data access efficiency. For the database aspect, MySQL and MongoDB were utilized for data storage, and Flyway was employed for managing different versions of the database. Additionally, OAuth2 was used for user authentication and authorization to ensure the overall security of the system. In performance testing, JMeter software was used to simulate real-time attendance scenarios for 50, 100, and 500 students. The experimental results demonstrated that the algorithm did not encounter any errors under three different levels of concurrency, and the response time was within 0.5 seconds, showing stable performance. Overall, this project fully leverages the flexibility and scalability of the microservice framework, greatly improving the operational efficiency of educational institutions and the learning experience of students, while laying a solid theoretical foundation for future expansion and maintenance. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"D., Edirisinghe, Dasith; K., Rajapakse, Kavinda; P., Abeysinghe, Pasindu; S., Rathnayake, Sunimal","SpotKube: Cost-Optimal Microservices Deployment with Cluster Autoscaling and Spot Pricing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217038003&partnerID=40&md5=9555883461c1a2bfaa6e82e2f3649981","Microservices architecture, known for its agility and efficiency, is an ideal framework for cloud-based soft-ware development and deployment, particularly when inte-grated with containerization and orchestration systems that streamline resource management. However, the cost of cloud computing remains a critical concern for many organizations, prompting the need for effective strategies to minimize expenses without compromising performance. While cloud platforms like AWS offer transient pricing options, such as Spot Pricing, to reduce operational costs, the unpredictable demand and abrupt termination of spot VMs introduce considerable challenges. By leveraging containerization alongside intelligent orchestration, it is possible to optimize micro services deploy-ment costs while maintaining performance requirements. We present SpotKube, an open-source, Kubernetes-based so-lution that employs a genetic algorithm for cost optimization. Designed to dynamically scale clusters for micro service applications on public clouds using spot pricing, SpotKube analyzes application characteristics to recommend optimal resource al-locations, ensuring deployments remain cost -effective without sacrificing performance. Its elastic cluster autoscaler adapts to changing demands, gracefully managing node terminations to minimize disruptions in system availability. Evaluations conducted using real-world public cloud setups demonstrate SpotKube's superior performance and cost efficiency compared to alternative optimization strategies. GitHub: https://github.com/SvotKube/SvotKube © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"G., Antoniou, Georgia; H., Volos, Haris; Y., Sazeides, Yiannakis","Taming Performance Variability caused by Client-Side Hardware Configuration","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214579098&partnerID=40&md5=aab07a7855035ddbcf1de7c41c016d86","Many online services running in datacenters are implemented using a microservice software architecture characterized by strict latency requirements. Consequently, this popular software paradigm is increasingly used for the performance evaluation of server systems. Due to the scale and complexity of datacenters, the evaluation of server optimization techniques is usually done on a smaller scale using a client-server model. Although the experimental details of the server side are excessively described in most publications, the client side is often ignored. This paper identifies the hardware configuration of the client side as an important source of performance variation that can affect the accuracy and the correctness of the conclusions of a study that analyzes the performance of microservices. This is partially attributed to the strict latency requirements of microservices and the small scale of the experimental environment.In this work we present, using a widely used online-service, several examples where the accuracy and the trends of the conclusions differ based on the configuration of the client-side. At the same time we show that the experimental evaluation time can be significantly affected by the hardware configuration of the client. All these provoke the discussion of the right way to configure the experimental environment for assessing the performance of microservices. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"H., Zhou, Hanlin; H.Y., Chan, H. Y.","Implement HPA for Nginx Service Using Custom Metrics under Kubernetes Framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211598051&partnerID=40&md5=87722ed926fa5294ef9b057ec6862ad1","Kubernetes, as a container orchestration and management tool suitable for large-scale and complex scenarios, is essentially the recommended management tool for containers like Docker. Kubernetes includes a Horizontal Pod Autoscaler (HPA) component that can automatically scale the number of containers based on resource metrics. However, by default, it only supports CPU and memory metrics. In real-world business scenarios, such as web services, CPU and memory usage may not accurately reflect service quality. HTTP status codes can better represent service status; for example, a 5xx status code indicates an abnormal request result, which more accurately reflects the actual service situation compared to CPU metrics. To address these challenges, this paper proposes an HPA architecture based on HTTP status codes as custom metrics to improve system service availability. The paper designs two types of custom status code HPA: HHPA, implemented using Prometheus, and HHPA-2, implemented using Python code. The results show that under high-load conditions, HHPA and HHPA-2, which use 5xx status code custom metrics, improved response speed by 30%, reduced abnormal requests by over 50%, and provided more than 99% high availability compared to HPA using CPU metrics, This study contributes by demonstrating the advantages of using HTTP status codes as custom metrics in the HPA for more effective resource scaling. It provides models and data references aimed at improving HPA service performance and enhancing system service availability. Furthermore, the development of new components using Python code offers valuable insights for Kubernetes research. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"H., Wang, Han; X., Peng, Xinyong; H., Zhang, Haifeng; J., Su, Jie; H., Wang, Huiyong; C., Li, Chunhai","Microservices Crowdsourcing Platform for Industrial Control Systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211385836&partnerID=40&md5=0b14f7e3ba6ec5cb87e87bf42290786a","With the wide application of microservice architecture in industrial control systems, more and more security problems arise, such as unauthorized access and data leakage. For the generation of these problems, industrial control crowdsourcing testing is an effective solution. Therefore, this paper designs an industrial control crowdsourcing test platform based on microservice architecture. The enterprise-level microservice technology framework ensures the reliability of the architecture. At the same time, a microservice aggregation algorithm is introduced, which greatly reduces the time-consuming microservice subscription operations and improves the reuse rate of microservices through the secondary aggregation of microservices. In the experiments, this paper compares and analyses the performance of the microservice architecture integrated with the EMQX (Erlang MQTT X) framework, and the results show that the architecture performs well in terms of system response, and the stress test of 1,000 concurrent requests, the response time of the traditional architecture is in 1,400 milliseconds, while that of the microservice architecture is within 700 milliseconds, and the architecture of this paper improves the response speed and reliability of the overall architecture. It can be applied to various complex industrial IoT scenarios. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"V., Berry, Vincent; A., Castelltort, Arnaud; B., Lange, Benoit; J., Teriihoania, Joan; C.V., Tibermacine, Chouki V.; C., Trubiani, Catia","Is it Worth Migrating a Monolith to Microservices? An Experience Report on Performance, Availability and Energy Usage","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210263777&partnerID=40&md5=9ef5b58897666c14ac334cf6c9c08442","The microservice architecture (MSA) emerged as an evolution of existing architectural styles with the promise of improving software quality by decomposing an app into modules that can be maintained, deployed, and scaled independently. However, the transition from a monolithic to a microservice architecture is fraught with difficulties, especially when it comes to assessing qualitative aspects, as controversial results can arise. In this paper, we present an experience report on the migration of a monolithic web application and use performance, availability and energy efficiency as quality attributes to shed light on such an architectural transition. Horizontal scaling, i.e., distributing the workload across several service instances, is applied and we study its impact.Our main findings are: i) when no app component is replicated, MSA outperforms the monolithic architecture; ii) the monolithic architecture shows performance and availability improvement when replicating the entire app; iii) the replicated MSA version reaches a ceiling when not replicating its routing part (i.e., the API gateway), showing worse response times compared to the replicated monolith; iv) when replicating the API gateway, the MSA version reaches optimal performance with fewer replicates than the monolith; v) when not replicating services, MSA consumes more CPU resources than the monolithic architecture; vi) when scaling up, the MSA version is more efficient than the replicated monolith in terms of memory usage, and it can better exploit CPU resources; vii) when not replicating services, MSA consumes more energy than the monolithic architecture, whereas when scaling up, the MSA version is more efficient than the replicated monolith; MSA version reaches a good balance between CPU and memory usage. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"J., Park, Jin; B., Jeong, Byeonghui; J., Jeon, Jueun; Y., Jeong, Youngsik","Burst-Aware Horizontal Autoscaling Based on Deep Learning for Stable Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210043275&partnerID=40&md5=05b0dcdfff571a132e032d3baa8fda48","In the cloud computing environment, container autoscaling is a key resource-management method that provides continuity and scalability for microservices. However, the autoscaling method based on a reactive mechanism is unable to respond instantly to workload changes, leading to resource wastage. Moreover, this method struggles to identify irregular burst states accurately in the workload, resulting in service disruptions. Accordingly, this study proposes a burst-aware horizontal autoscaling (BHAS) method that operates using a proactive mechanism to enhance the stability and resource efficiency of microservices under burst workloads. BHAS uses a time-series forecasting model that combines the reversible instance normalization method with the decomposition linear to predict future resource usage. Then, BHAS flexibly detects local and global bursts in the predicted future workload comprising heterogeneous resource usage. Finally, it performs scaling by calculating the number of efficient containers at each time point for the detected burst and nonburst states. The performance evaluation of BHAS in a cloud-native computing environment revealed that the average number of resource overload instances was reduced by up to 96.74% compared to the existing autoscaling techniques applying the actual container workload trace. In addition, resource utilization improved by approximately 3.3% compared with the reactive mechanismbased autoscaling method. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"C., Courageux-Sudan, Clément; A.Ć., Orgerie, Anne Ćecile; M., Quinson, Martin","Studying the end-to-end performance, energy consumption and carbon footprint of fog applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209208811&partnerID=40&md5=6bf5013a1a4edd9067f3ae3785ad9102","The deployment of applications closer to end-users through fog computing has shown promise in improving network communication times and reducing contention. However, the use of fog applications such as microservices necessitates intricate network interactions among heterogeneous devices. Consequently, understanding the impact of different application and infrastructure parameters on performance becomes crucial. Current literature either offers end-to-end models that lack granularity and validation or fine-grained models that only consider a portion of the infrastructure. Our research first compares experimentally the accuracy of the existing integrated frameworks. We then combine one of these tools with a collection of validated models to obtain comprehensive metrics regarding microservice applications operating in the fog. Through a use-case, we demonstrate the effectiveness of our approach in investigating fog environments, from examining application latencies to greenhouse gas emissions. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"S., Di Meglio, Sergio; L.L., Starace, Luigi L.L.","Evaluating Performance and Resource Consumption of REST Frameworks and Execution Environments: Insights and Guidelines for Developers and Companies","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208379810&partnerID=40&md5=82e7ff7fe48198216a31ff611331f962","The REST (REpresentational State Transfer) paradigm has become essential for designing distributed applications that leverage the HTTP protocol, enabling efficient data exchange and the development of scalable architectures such as microservices. However, selecting an appropriate framework among the myriad available options, especially given the diversity of emerging execution environments, presents a significant challenge. Often, this decision neglects crucial factors such as performance and energy efficiency, favoring instead developer familiarity and popularity within the industry. To address this, we conducted a comprehensive benchmark study using a prototype REST API application provided by an industry partner, which was implemented multiple times using different REST API frameworks. We evaluated five different REST API frameworks across three popular programming languages, incorporating both traditional and emerging execution environments, resulting in twelve distinct configurations. Our results reveal significant differences in performance and computational resource consumption across different frameworks and execution environments, highlighting the necessity of making informed technology choices based on thorough analysis rather than convenience or familiarity. In addition to our findings, we offer other contributions to the field: an automated pipeline that benchmarks different configurations with various frameworks and execution environments, and a reference benchmark REST API that can be used in other studies. This research provides valuable insights and tools for developers and organizations aiming to select high-performance, resource-efficient technologies that promote environmental sustainability and reduce operational costs. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"Y., He, Yi; Y., Zhang, Yanzhong; C., Wu, Che; M., Yang, Meng; W., Xu, Weidong; H., Wan, Haiyang; Z., Chen, Zhuyun","Architecture Design and Application of IIoT Platform in Automobile Manufacturing Based on Microservices and Deep Learning Techniques","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208260409&partnerID=40&md5=e8902d210fb787fed56e7be5357883ed","An Internet of Things (IoT) platform is a software architecture that enables the connection, management, and analysis of IoT devices, sensors, and data. It provides a centralized system for IoT devices to interact with each other and with the cloud, facilitating the collection, processing, and analysis of data from these devices. However, in the automotive manufacturing industry, traditional Internet of Things (IoT) platforms are facing challenges such as bottleneck issues due to business volume growth and system challenges. To address these challenges, we propose a design methodology for an IoT platform based on microservices. The platform's modules are divided into front end, database, security, and operation maintenance architecture, all effectively designed. Through practical applications, the platform enables interconnections between different information systems, production status monitoring, efficiency management, performance evaluation, energy consumption analysis, quality detection, and equipment asset evaluation. Finally, a data-driven deep learning algorithm, named Long Short-Term Memory Neural Network (LSTM) is developed for the state recognition of the industrial robot based on the Intelligent data services platform, which validate the effectiveness of the constructed IoT platforms. This platform offers advantages in extendibility, reusability, and provides methods for upgrading, expanding functions, and maintaining industrial IoT platforms in the discrete manufacturing industry. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"Y., Wang, Yixuan; A., Chandra, Abhishek; J.B., Weissman, Jon B.","Jingle: IoT-Informed Autoscaling for Efficient Resource Management in Edge Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207887918&partnerID=40&md5=35e38751792ef9696284a5321e7e2fd2","Edge computing is increasingly applied to various systems for its proximity to end-users and data sources. To facilitate the deployment of diverse edge-native applications, container technology has emerged as a favored solution due to its simplicity in development and resource management. However, deploying edge applications at scale can quickly overwhelm edge resources, potentially leading to violations of service-level objectives (SLOs). Scheduling edge containerized applications to meet SLOs while efficiently managing resources is a significant challenge. In this paper, we introduce Jingle, an autoscaler for edge clusters designed to efficiently scale edge-native applications. Jingle utilizes application performance metrics and domain-specific insights collected from IoT devices to construct a hybrid model. This hybrid model combines a predictive-reactive module with a lightweight learning model. We demonstrate Jingle's effectiveness through a real-world deployment in a classroom setting, managing two edge-native applications across edge configurations. Our experimental results show that Jingle can fulfill SLO requirements while requiring up to 50% fewer containers than a state-of-the-art cloud scheduler, which highlights its resource management efficiency and SLO compliance. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"R., Zeng, Ruiqi; Y., Chen, Yuxiang","A Novel Automatic Horizontal Extension Technology for Multi-instance Microservice System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205996071&partnerID=40&md5=54c7028e11fa73940f2243225e5f3b73","In microservice architecture, system deployment usually adopts container environment. The container deployment method solves problems of rapid iteration, rapid deployment and automatic recovery of software. In addition, during software operation, to meet the changing visits at any time, it is necessary to expand and shrink software, and improve resource utilization rate. In relevant fields, although there are many researches on automatic deployment, automatic operation, automatic maintenance, automatic upgrade, and many methodologies, there are few comprehensive schemes that integrate horizontal expansion of microservice software with automatic deployment. This paper focuses on automatic horizontal expansion of container multi-instance system, and design a scheme of system automation horizontal expansion. A simple verification plan integrating software developing, software building, image production, system deployment, system maintenance, and horizontal expansion is designed. Through construction and testing of actual prototype system, the scheme utility and stability are verified. Meanwhile, this scheme is simple and easy to manage, and can meet expansion requirements of most system. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"D., Kumar, Divyansh; U., Kumar, Utkarsh; A., Prakash, Abhigyan; A., Bandyopadhyay, Anjan; S., Swain, Sujata","Auction-Based Microservice Placement in Fog Computing: Algorithms, Analysis, and Performance Evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205561885&partnerID=40&md5=7c6801a9dddae11324740480ae39e043","This research paper introduces a distributed fog service placement (DFSP) algorithm for efficient microservice allocation in the field of fog computing environments, aiming to reduce power usage and data transfer costs. The algorithm utilizes an iterative combinatorial auction approach, where fog applications bid for resources on fog nodes to host their microservices. Two versions of the algorithm are proposed: DFSP-GF, which relies on global feedback, and DFSP-NF, which uses neighbor feedback. Both versions exhibit fast convergence, maintain privacy, and meet resource constraints. Simulation results showcase the efficacy of the DFSP algorithms in terms of convergence speed, resource utilization, allocation distribution, and total cost. DFSP-NF provides a scalable and practical solution for resource-constrained fog environments, while DFSP-GF offers further cost optimization at the cost of higher communication overhead. The DFSP algorithms present a distributed and privacy-preserving approach to solving the fog service placement problem, considering the challenges posed by the distributed nature of fog resources. The choice between DFSP-NF and DFSP-GF depends on specific requirements, such as scalability, communication efficiency, and the trade-off between global optimization and local decision-making.. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"P.K., Erdelt, Patrick K.","A Cloud-Native Adoption of Classical DBMS Performance Benchmarks and Tools","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205393519&partnerID=40&md5=cc983cf7a8245d63a3d3cebb13cd2b46","Classical DBMS benchmarks cover a variety of use cases, for example: microbatch in-line insertion and highly concurrent row-level access (YCSB), batch offline loading into a data warehouse and concurrently running complex analytical queries (TPC-H) and business transactions (TPC-C). These use cases are still relevant in the cloud era, where we build data pipelines of microservices. In this paper we adopt the above benchmarks and four popular tools to the cloud-native pattern. On the one hand, this helps in assessing the performance of data pipelines that have a DBMS at their core. On the other hand, it makes benchmarking a scalable, elastic and observable process that can be automated. In a series of experiments, we (1) inspect Kubernetes jobs and benchmarking tools and whether they are suitable for combination, (2) monitor resource consumption of all components, i.e., also the drivers, (3) inspect scaling behaviour and look for peak performance points. We show that tools and workloads respond differently to scale-out and that the cloud-native pattern is fruitful for benchmarking. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"P., Mishra, Pratik; S., Hans, Sandeep; D., Saha, Diptikalyan; P., Moogi, Pratibha","Optimizing Cloud Workloads: Autoscaling with Reinforcement Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203294736&partnerID=40&md5=89951204423eb198938fcdd28dc9fe6b","By 2027, over 50 % of enterprises are expected to adopt industry cloud platforms [1], driving potential EBITDA value of 3 trillion by 2030 [2]. In this landscape, software providers rely on Infrastructure-as-a-Service (IaaS) providers to access tailored virtualized resources based on usage. Optimizing resource utilization is crucial to reducing operating costs and maintaining quality standards for SaaS and IaaS providers. This creates an essential need for dynamic scaling mechanisms to adjust resources according to workload variations. The Kubernetes resource Horizontal Pod Autoscaler (HPA) has limitations in scaling applications. However, AI-based algorithms, particularly Reinforcement Learning (RL), offer promising solutions. AI-based methods excel in overcoming fixed parameter constraints, handling sudden load spikes, and supporting custom parameters. We present an RL-based framework for auto scaling applications, demonstrating results from experimental evaluation. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"N., Chen, Norman; A.N., Toosi, Adel N.; B., Javadi, Bahman; D.K., Alqahtani, Daghash K.; M.S., Aslanpour, Mohammad Sadegh; M., Xu, Minxian","An Empirical Study on Edge-to-Cloud Continuum for Smart Applications: Performance, Design Patterns, and Key Factors","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203248027&partnerID=40&md5=37809670f058efaa4958c93236e09a71","The rapid evolution of cloud-native technologies has facilitated seamless application deployment and execution across the entire edge-to-cloud continuum. This continuum offers a myriad of benefits, including reduced latency, optimized bandwidth utilization, enhanced data privacy, improved reliability, scalability, and flexibility. However, realizing a coherent edge-to-cloud continuum poses challenges especially in resource management, due to the heterogeneous and dynamic nature of computing resources such as resource scheduling and load balancing. This paper focuses on the Container-as-a-Service model enabling independent execution of functions/microservices anywhere on the continuum. We propose an architectural design for constructing a practical edge-to-cloud infrastructure and conduct comprehensive performance evaluations using a real edge-to-cloud testbed. Through an empirical study, we aim to identify key factors impacting application performance and resource management within the continuum, with a specific focus on AI-based IoT applications. Our experiments explore various design patterns including load balancing techniques, scheduling algorithms, invocation methods, gateway and data source location, and factors such as bandwidth and delay, providing practical insights for practitioners and researchers alike. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"A.M., Dincǎ, Ana Maria; S.D., Axinte, Sabina Daniela; I.C., Bacivarov, Ioan C.; G., Petrica, Gabriel","Horizontal Scaling Implementation with Container Orchestrator. Reliability Analysis during Stress and Performance Testing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202992366&partnerID=40&md5=20f18dd03cd31c8ae273d01328a1f7e1","Software platforms’ infrastructure encompasses the collection of physical resources used concomitantly for user interactions, computational or persistence operations. Virtualization facilitates dynamic allocation of resources to a platform, while staying within the infrastructure’s limits, enabling deployments of multiple environments on a server, maximizing the computational power. This paper focuses on practical applications of virtualization principles on software platforms, and the analysis of performance indicators. Section II is centered on the Docker feature that packages the source code into images, facilitating testing in local containers, preparing them for the container orchestrator, Kubernetes that designs, creates and scales infrastructure components. The platform was scaled horizontally, because this method accommodates the application’s technical requirements, as stated in Section V. To identify the underlying causes of an inefficient auto-scaling infrastructure, a Fishbone analysis was conducted, followed by a novel proposal of triggering scaling operations, using AI tools that decide when to rescale an infrastructure, instead of traditional alert levels. The simple and advanced health check mechanisms are studied and implemented in Section VI, through separate APIs that validate the system’s status and dependencies automatically and periodically using Docker commands, proving the misleadingness of the container for the former, demonstrating the importance of such progressive mechanisms. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"P., Kakati, Pranjit; A., Bora, Abhijit","Investigation on Performance Evaluation of Loosely Coupled Microservice for Scalable Computation in Agriculture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202778332&partnerID=40&md5=e3c20a1f4ec573acb3f77f5fb3a4356c","Deployment of computational logic through microservice architecture in different sectors can play a vital role. However, deploying microservice from the perspective of agricultural aspects and evaluating their quality metrics is key concern in the research community. As such, in this work, a novel methodology is proposed that can be used to evaluate the performance aspects of loosely coupled microservice for scalable computations in the agriculture domain. Here, the architecture of the experimental set up is discussed followed by identifying the correlation among performance metrics is evaluated. The statistical analysis is carried out to validate the hypothesis regarding the applicability of the performance metrics. It can be concluded that, the deployment of microservice for the domain of agricultural computation is valid and applicable where the correlation of the performance metrics can be achieved at 95% confidence level. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"K., Zhu, Kaile; S., Shen, Shihao; S., Lan, Shizhan; X., Wang, Xiaofei; C., Zhang, Cheng; C., Qiu, Chao; V.C., Leung, Victor C.M.","Proactive Hybrid Autoscaling for Container-Based Edge Applications in Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202342406&partnerID=40&md5=0544a16ca3117c01d8d3a98fd1375b61","As the rising of the Internet of Things (IoT), edge computing is widely adopted in numerous applications. However, current autoscaling tools are not designed for edge applications and can not utilize the heterogeneous resources of edge nodes efficiently. In this paper, we propose a proactive hybrid autoscaler specifically optimized for edge computing scenario. With the Bidirectional Long Short Term Memory (Bi-LSTM) based load prediction model, the proposed autoscaler is able to predict the future workload and perform scaling operation before it arrives. In addition, a overload compensation algorithm is implemented to mitigate the Quality of Service (QoS) decreasing due to under-prediction. Then, a hybrid scaling method is applied to simultaneously modify the number of pods and their resource quotas without restarting. Experimental results with a real-world workload dataset shows the proposed load prediction model has better accuracy compared with the Long Short Term Memory model and the state-of-the-art statistical analysis model, Autoregressive Integrated Moving Average (ARIMA), which is also more than 350 times slower than our model in prediction speed. Finally, evaluation in a real Kubernetes cluster shows that the proposed proactive hybrid autoscaler outperforms the default Horizontal Pod Autoscaler (HPA) of Kubernetes in terms of both QoS and resource utilization efficiency. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"A., Robles-Enciso, Alberto; A.F., Skarmeta, Antonio F.","Adapting Containerized Workloads for the Continuum Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200222704&partnerID=40&md5=f6d97abc449180a4e4c827ab731a47d5","Container and microservices management platforms are currently one of the most important tools for cloud computing, but since the scope of these tools is homogeneous cloud architectures they have serious limitations in adapting to new computing paradigms. Therefore, using the default scheduler in heterogeneous node systems faces significant limitations when tasked with orchestrating workloads in a Continuum Computing environment, as the nodes have very different characteristics and restrictions. To solve this limitation we decided to use Kubernetes as it is the most popular Container management tool and we propose to replace the native scheduler with a reimplementation that gives us complete flexibility for the process of assigning pods to nodes, providing a framework to design algorithms that considers all the necessary parameters for the deployment of services in a Continuum. In addition, we address one of the most limiting aspects of the K8s scheduler, its pod-by-pod allocation approach, which makes it difficult to optimise the complete set of allocations. To test our proposal we design a use case and perform several tests on a real environment based on virtual machines, in which stress tests are conducted to measure the performance of each method. We then present a series of results to justify the benefits of our proposal, including the reduced performance provided by the pod-pod approach and how a batch-based approach greatly improves efficiency. The results show the usefulness of using batch-based approaches and how the Kubernetes scheduler extension points are not enough to support the requirements of the Continuum. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"A.A.A., Rage, Abdirazak Ali Asir; N., Wang, Ning; R.P., TAFAZOLLI, Rahim P.","NFScaler: AI-Powered 5G-and-Beyond Network Function Scaler for QoS Assurance and Energy Efficiency","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199575323&partnerID=40&md5=bfa50708cd9e40c14d8cf7688e92f49b","Efficient resource management and orchestration are essential to maximise the benefits of network slicing in 5G and beyond networks. This paper presents NFScaler, an Artificial Intelligence (AI)-powered 5G network function scaler that can perform a zero-shot sim2real transfer. The proposed NFScaler automatically adjusts the number of 5G network function instances running in a network slice based on changes in user traffic to meet the QoS requirement of the slice while reducing the resource consumption (i.e. CPU and power). The NFScaler consists of two main parts: a domain randomiser and a deepreinforcement learning (DRL) agent. The domain randomiser randomises the dynamic parameters of a simulation to provide the DRL agent with a range of simulation environments. The DRL agent interacts with both the original simulation and the randomised simulation to learn a generalisable autoscaling policy. The performance of the proposed NFScaler is evaluated in a live 5G network and compared with the threshold-based KEDA scaler, which is the industry standard event-based autoscaler used in cloud native environments, as well as a DRL method that does not involve sim2real transfer. The experimental findings demonstrate that NFScaler effectively guarantees the QoS requirements of the network slice, outperforming the benchmark methods. In particular, NFScaler achieves an improvement of almost 40% in QoS (throughput) performance compared to KEDA Scaler. Furthermore, the proposed NFScaler allocates a significantly lower number of CPU cores to the user plane of the network slice, with an average of 75% fewer cores than when the user plane is hosted on a dedicated bare metal server. In addition, the NFScaler reduces the power consumption of the user plane of the network slice by an average of 45% compared to hosting the user plane on a dedicated bare metal server. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"G.M., Freire, Gustavo Mota; H., Paucar-Curasma, Herminio; J.Ć., Estrella, Júlio Ćezar","A Distributed Software Architecture for IoT: Container Orchestration Impact and Evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199573008&partnerID=40&md5=6c8041de74f0c0c7948e02d3e96c0c50","This paper proposes a Distributed Software Architecture (DSA) for Smart Building (SB) based on the Reactive Manifesto (RM) principles. To follow the RM principles, we analyze the usage of different deployment approaches, particularly the impact of using a container orchestrator on the application layer. After running performance tests on the different configurations, the container orchestrator usage led to enhanced distributed processing, lowering the latency, increasing flexibility, enhancing security, and providing cost-effectiveness and scalability. We introduce the implementation of a modern DSA, developed following state-of-the-art cloud patterns and compliant with the RM for the SB context. Furthermore, we have ensured the reproducibility of this implementation by making the initial tests and overall architecture code available in public repositories. The research follows the Design Science Research (DSR) methodology for elaborating each phase until we get the artifact (DSA) and, with this, contribute to the Knowledge Base. The architecture was properly tested, considering the performance as the principal test layer. This solution is tailored for application in domains of the Internet of Things (IoT), focusing on the SB and a case study involving the Laboratory of Distributed Systems and Concurrent Programming (LaSDPC) at Sao Paulo University. Moreover, its applicability extends to IoT domains like smart home, smart campus, smart city, and health-related applications. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"L., Loechel, Louis; S.R., Akbayin, Siar Remzi; E., Grünewald, Elias; J., Kiesel, Jannis; I., Strelnikova, Inga; T., Janke, Thomas; F., Pallas, Frank","Hook-in Privacy Techniques for gRPC-Based Microservice Communication","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197812793&partnerID=40&md5=8904f2b6af788f61ff84e19f6e79a8c2","gRPC is at the heart of modern distributed system architectures. Based on HTTP/2 and Protocol Buffers, it provides highly performant, standardized, and polyglot communication across loosely coupled microservices and is increasingly preferred over REST- or GraphQL-based service APIs in practice. Despite its widespread adoption, gRPC lacks any advanced privacy techniques beyond transport encryption and basic token-based authentication. Such advanced techniques are, however, increasingly important for fulfilling regulatory requirements. For instance, anonymizing or otherwise minimizing (personal) data before responding to requests, or pre-processing data based on the purpose of the access may be crucial in certain usecases. In this paper, we therefore propose a novel approach for integrating such advanced privacy techniques into the gRPC framework in a practically viable way. Specifically, we present a general approach along with a working prototype that implements privacy techniques, such as data minimization and purpose limitation, in a configurable, extensible, and gRPC-native way utilizing a gRPC interceptor. We also showcase how to integrate this contribution into a realistic example of a food delivery use case. Alongside these implementations, a preliminary performance evaluation shows practical applicability with reasonable overheads. Altogether, we present a viable solution for integrating advanced privacy techniques into real-world gRPC-based microservice architectures, thereby facilitating regulatory compliance “by design”. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"S., Chen, Shuangwu; Q., Yuan, Qifeng; J., Li, Jiangming; H., He, Huasen; S., Li, Sen; X., Jiang, Xiaofeng; J., Yang, Jian","Graph Neural Network Aided Deep Reinforcement Learning for Microservice Deployment in Cooperative Edge Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196742236&partnerID=40&md5=0f3cc7a9645bfeb1f37e9403b4430e19","Deploying microservices on the cooperative edge computing system greatly shortens the interaction delay between users and service and alleviates the traffic burden on the backbones, which has emerged as a new paradigm for service provision. However, it is challenging to embed microservices, having diverse resource demands and heterogeneous invocation relationships, into a distributed edge computing system with irregular network topology. In order to characterize the invocation relationship, we conceive a graph attention network based model to capture the structural features of microservices. Similarly, we propose a multi-channel directed graph convolutional network model to capture the spatial dynamic of edge resources distribution, which jointly considers the heterogeneity of the edge nodes and the links between them. Then, we develop a sequence-to-sequence based multi-step decision model, which maps the feature sequence of the current state to a sequence of deployment actions. Using this model, we further propose a microservice deployment algorithm based on graph neural network aided deep reinforcement learning, where a parallel asynchronous training process is used to accelerate convergence. The performance evaluation shows that the proposed algorithm can improve the deployment success ratio and resource utilization, while ensuring the load balance of edge nodes. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"J.R., Chirre Escate, Jashir R.; G.G., Gamarra Gómez, Giorgio G.; H.D., Calderon-Vilca, Hugo D.; F.C., Cárdenas-Mariño, Flor C.","Microservices-based Architecture to Improve the Enrollment Process of State Schools","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195289479&partnerID=40&md5=aab2900719718f05c6b72c4f87d2eb42","This research proposes a microservices-based architecture to improve the enrollment process of state schools in times of pandemic. Microservices allow an application to support higher request concurrency and fault tolerance in a scenario where many users require to perform a web-based enrollment process. The development process and the tools used for the proposed architecture are also detailed. Through 4 types of performance tests, it is demonstrated that the architecture achieved an average efficiency of 177% compared to a traditional monolithic architecture. Therefore, the study concludes that the implementation of a microservices architecture for the enrollment process of state schools is reliable, efficient and functional. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"D., Khatri, Deepak; S.K., Khatri, Sunil Kumar; D., Mishra, Deepti","Intelligent Framework in a Serverless Computing for Serving using Artificial Intelligence and Machine Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195090680&partnerID=40&md5=d8fdc648a8735b21320d828916099faf","Serverless computing has grown in popularity as a paradigm for deploying applications in the cloud due to its ability to scale, cost-effectiveness, and simplified infrastructure management. Serverless architectures can benefit AI and Machine Learning (ML) models, which are becoming increasingly complex and resource-intensive. This study investigates the integration of AI/ML frameworks and models into serverless computing environments. It explains the steps involved, including model training, deployment, packaging, function implementation, and inference. Serverless platforms’ auto-scaling capabilities allow for seamless handling of varying workloads, while built-in monitoring and logging features ensure effective management. Continuous integration and deployment pipelines simplify the deployment process. Using serverless computing for AI/ML models offers developers scalability, flexibility, and cost savings, allowing them to focus on model development rather than infrastructure issues. The proposed model leverages performance forecasting and serverless computing model deployment using virtual machines, specifically utilizing the Knative platform. Experimental validation demonstrates that the model effectively predicts performance based on specific parameters with minimal data collection. The results indicate significant improvements in scalability and cost efficiency while maintaining optimal performance. This performance model can guide application owners in selecting the best configurations for varying workloads and assist serverless providers in setting adaptive defaults for target value configurations. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"J., Park, Jinwoo; B., Choi, Byungkwon; C., Lee, Chunghan; D., Han, Dongsu","Graph Neural Network-Based SLO-Aware Proactive Resource Autoscaling Framework for Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192144946&partnerID=40&md5=850d96963a143d292f0a5cce8185b238","Microservice is an architectural style widely adopted in various latency-sensitive cloud applications. Similar to the monolith, autoscaling has attracted the attention of operators for managing the resource utilization of microservices. However, it is still challenging to optimize resources in terms of latency service-level-objective (SLO) without human intervention. In this paper, we present GRAF, a graph neural network-based SLO-aware proactive resource autoscaling framework for minimizing total CPU resources while satisfying latency SLO. GRAF leverages front-end workload, distributed tracing data, and machine learning approaches to (a) observe/estimate the impact of traffic change (b) find optimal resource combinations (c) make proactive resource allocation. Experiments using various open-source benchmarks demonstrate that GRAF successfully targets latency SLO while saving up to 19% of total CPU resources compared to the fine-tuned autoscaler. GRAF also handles a traffic surge with 36% fewer resources while achieving up to 2.6x faster tail latency convergence compared to the Kubernetes autoscaler. Moreover, we verify the scalability of GRAF on large-scale deployments, where GRAF saves 21.6% and 25.4% for CPU resources and memory resources, respectively. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"D.R., Augustyn, Dariusz Rafał; Ł., Wycislik, Łukasz; M., Sojka, Mateusz","Tuning a Kubernetes Horizontal Pod Autoscaler for Meeting Performance and Load Demands in Cloud Deployments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190163143&partnerID=40&md5=1cb06e534a534c259b9eb6d8f7fd320c","In the context of scaling a business-critical medical service that involves electronic medical record storage deployed in Kubernetes clusters, this research addresses the need to optimize the configuration parameters of horizontal pod autoscalers for maintaining the required performance and system load constraints. The maximum entropy principle was used for calculating a load profile to satisfy workload constraints. By observing the fluctuations in the existing workload and applying a kernel estimator to smooth its trends, we propose a methodology for calculating the threshold parameter of a maximum number of pods managed by individual autoscalers. The results obtained indicate significant computing resource savings compared to autoscalers operating without predefined constraints. The proposed optimization method enables significant savings in computational resource utilization during peak loads in systems managed by Kubernetes. For the investigated case study, applying the calculated vector of maximum pod count parameter values for individual autoscalers resulted in about a 15% reduction in the number of instantiated nodes. The findings of this study provide valuable insights for efficiently scaling services while meeting performance demands, thus minimizing resource consumption when deploying to computing clouds. The results enhance our comprehension of resource optimization strategies within cloud-based microservice architectures, transcending the confines of specific domains or geographical locations. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"A., Alelyani, Abdullah; A., Datta, Amitava; G.M., Hassan, Ghulam Mubashar","Optimizing Cloud Performance: A Microservice Scheduling Strategy for Enhanced Fault-Tolerance, Reduced Network Traffic, and Lower Latency","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187358043&partnerID=40&md5=e74c70e4e7fc24cb33e68fec72329782","The emergence of microservice architecture has brought significant advancements in software development, offering improved scalability and availability of applications. Cloud computing benefits from microservice architecture by mitigating the risks of single failures and ensuring compliance with service-level agreements. However, using microservice architecture presents two challenges: 1) managing network traffic, which leads to latency and network congestion; and 2) inefficient resource allocation for microservices. Current approaches have limitations in addressing these challenges. To overcome these limitations, we propose a novel scheduling strategy that schedules microservice replicas using a modified particle swarm optimization algorithm to place them on the most suitable physical machine. Additionally, we balance the load across physical machines in the cluster using a simple round-robin algorithm. Furthermore, our scheduling strategy integrates with Kubernetes to tackle resource allocation and deployment challenges. The proposed strategy has been evaluated by simulating two scenarios using Alibaba and Google datasets. The experimental results demonstrate the effectiveness of our strategy in reducing traffic, balancing load, and utilizing CPU and memory efficiently. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"A., Bhardwaj, Aditya; A.P., Singh, Amit Prakash; P., Sharma, Priya; K., Abid, Konika; U., Gupta, Umesh","Performance Evaluation of Virtual Machine and Container-Based Migration Technique","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184297320&partnerID=40&md5=12b53e61eeabfe4d327b9a9a5410badc","The transformation from hypervisor to microservice-based virtualization, i.e., containerization, is gaining considerable attention. This is because container virtualization offers a lightweight and efficient way to package and deploy software applications. Containers require minimum resources than virtual machines, which makes them more efficient and cost effective. The performance overhead of a container compared to a virtual machine has been explored by researchers, but support for migration, an essential technique of cloud virtualization, needs to be addressed. In this work, we proposed container-based migration technique and compared the performance with existing VM migration scheme. The results show that compared to the existing VM migration scheme, our proposed container migration technique reduces downtime, migration time, and the number of pages transferred by 72.8%, 54.94%, and 97.5%, respectively. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"E., Park, Eun-chan; K., Baek, Kyeong-deok; E., Cho, Eunho; I., Ko, Inyoung","DESA: Decentralized Self-adaptive Horizontal Autoscaling for Bursts of Load in Fog Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181978299&partnerID=40&md5=e3f4de766a9427f78bb6a663170d7aa9","With the increase of Web of Things devices, fog computing has emerged as a promising solution to lower the communication overhead and congestion in the cloud. In fog computing systems, microservices are deployed as containers, which usually require an orchestration tool like Kubernetes for service discovery, placement, and recovery. One key challenge in the orchestration of microservices is establishing service elasticity in case of unpredictable bursts of load. Commonly, a centralized autoscaler in the cloud dynamically adjusts the number of microservice instances depending on the metric values monitored from distributed fog nodes. However, monitoring an increasing number of microservice instances can cause excessive network overhead and delay the scaling reaction. We propose DESA, a DEcentralized Self-adaptive Autoscaler through which each microservice instance makes its own scaling decision adaptively, cloning or terminating itself through a self-monitoring process. We evaluate DESA in a simulated fog computing environment with different numbers of fog nodes. The results show that DESA successfully reduces the scaling reaction time in large-scale fog computing systems compared to the centralized approach while resulting in a similar maximum number of instances and average CPU utilization during a burst of load. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"Á., Ruiz-Zafra, Ángel; J., Pigueiras-Del-Real, Janet; M.L., Noguera, Manuel Linero; L., Chung, Lawrence; D., Griol, David; K., Benghazi, Kawtar","Servitization of Customized 3D Assets and Performance Comparison of Services and Microservices Implementations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179800653&partnerID=40&md5=ebf14730765428124feb5a433d9be07b","3D models (or assets) that are present in many of modern software applications are first modeled by graphic designers using dedicated computer graphic tools and then integrated into such software applications or apps by software developers. This simple workflow/procedure requires developers to have a basic grounding in computer graphics, since 3D engines, libraries and third-party software are needed for this kind of integrations. Oftentimes, 3D designers are also required to customize or produce versions of a 3D model and thus, they must re-model all the assets before they are returned back to the developers for integration into the applications. This procedure also occurs whenever a modification or customization is requested. One possible significant improvement to this traditional, poorly automated workflow is to use services-oriented technology and features servitization to carry out the customization of 3D assets on-demand. In this article, we introduce μS3D, an open-source microservices-based platform designed to support features relating to the customization of 3D models. μS3D not only enables 3D assets to be customized without the need for computer graphic tools or designers, but also allows 3D models to be visualized through web technologies (e.g., HTML, Javascript and web component to visualize and interact with 3D models), thereby avoiding the development of computer graphics libraries or components in final software products. The article describes the elements that μS3D comprises, explains how it works and presents a series of load tests to compare the performance (time consumption, CPU and memory utilization) of μS3D when implemented and deployed as a microservices platform against a monolithic-based implementation, showing similar results with a low number of users (and requests) but reducing, on average, 64.32% the response time in the microservice-based implementation for a large number of users; reducing CPU utilization on microservice-based implementation and remaining the memory usage more or less constant in both implementations. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"V., Yuvaraj, V.; R., Krishna Priya, R.; S., Panyaram, Sudheer; V., Sudha, V.; V.S., Kumar, V. Senthil; M., Swami, Mahesh","Enhancing IoT service deployment in fog computing using metaheuristic methods: A novel approach","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011287348&partnerID=40&md5=16fb0d1f721d9b243949b65caf1da2cc","The present study investigates the application of metaheuristic schemes as a novel method to improve the implementation of IoT services in fog computing. Fog computing, an extension of cloud services at the network edge, provides a promising opportunity to serve time-sensitive applications by enabling computations on the collection of IoT devices. Leveraging the principles of the microservice architecture, this study focusses on the NP-hard task of allocating IoT services to fog nodes. The objectives of SPP-DEA include cost, time to serve, resources consumed, number of services supplied, resource consumption distribution, service deployment priority during placement, and effective control of computational complexity. Empirical assessment in synthetic fog validates the feasibility of SPP-DEA, demonstrating significant enhancements in service cost and response time when compared to conventional models. Analysis of algorithms in an experimental fog scenario demonstrates that SPP-DEA achieves a 16% reduction in service cost and a 10% reduction in waiting time when compared to current approaches. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"J., Shin, Juhun; S., Baik, Sunkee; S., Bahk, Saewoong","Performance Evaluation of OpenAirInterface-based 5G Standalone Testbed","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007513319&partnerID=40&md5=05a8cec651de1e35b1b07bf76f6f3f0e","RAN virtualization is a technology that provides flexibility to network operators by enabling the implementation of RAN functions in software rather than being limited to hardware. O-RAN Alliance suggests new Open RAN specifications based on RAN virtualization, open interfaces, and microservices. To create an experimental environment for testing Open RAN, we have set up a 5G standalone testbed based on the OpenAirInterface. We evaluate the throughput in this testbed and identify its current limitations. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"B., Qi, Bolin; Y., Yu, Yanchun; X., Li, Xu","The Utilisation and Implementation of the Istio Framework in the Architectonic Progression of Online Learning Platform","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000949246&partnerID=40&md5=9fd0ad546589888b3de7b6b8a59a8889","Online education platforms have become increasingly significant with the rapid development of ""Internet + Education"". This paper details the evolution of an online learning platform from a traditional monolithic application architecture to a Service Mesh architecture, incorporating the use of the Istio framework. Though efficient in its initial stages, the monolithic structure encountered performance bottlenecks in high concurrency and high throughput scenarios. By implementing the Service Mesh architecture, the platform reduced its latency under identical hardware conditions. The performance tests reveal that, in high concurrency and throughput scenarios, the Service Mesh architecture outperforms traditional microservices and monolithic structures, exhibiting considerable advantages. This highlights the Service Mesh architecture's potential in improving the platform's performance, scalability, and maintainability. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"A., Bento, Andre; F., Araujo, Filipe; R., Barbosa, Raul","Cost-Availability Aware Scaling: Towards Optimal Scaling of Cloud Services","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178921545&partnerID=40&md5=3ed05c0c086c4666939471de6c25767e","Cloud services have become increasingly popular for developing large-scale applications due to the abundance of resources they offer. The scalability and accessibility of these resources have made it easier for organizations of all sizes to develop and implement sophisticated and demanding applications to meet demand instantly. As monetary fees are involved in the use of the cloud, one of the challenges for application developers and operators is to balance their budget constraints with crucial quality attributes, such as availability. Industry standards usually default to simplified solutions that cannot simultaneously consider competing objectives. Our research addresses this challenge by proposing a Cost-Availability Aware Scaling (CAAS) approach that uses multi-objective optimization of availability and cost. We evaluate CAAS using two open-source microservices applications, yielding improved results compared to the industry standard CPU-based Autoscaler (AS). CAAS can find optimal system configurations with higher availability, between 1 and 2 nines on average, and reduced costs, 6% on average, with the first application, and 1 nine of availability on average, and reduced costs up to 18% on average, with the second application. The gap in the results between our model and the default AS suggests that operators can significantly improve the operation of their applications. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Karagiannakis, Stilianos; I., Theodoropoulos, Ioannis; A., Anagnostopoulos, Apostolos; B., Mamalis, Basilis","Smart Waste Management Using Microservices and Container-based Virtualization","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187555017&partnerID=40&md5=5532ab79e4d9d3b350a4f4fa42f6995b","One of the most significant issues associated with smart city applications is solid waste management, which has a negative impact on our society's health and the environment. In this paper, a smart waste bin management approach with novel architectural design is proposed, based on modern container-based virtualization and microservices technologies. Its objective is twofold; first, efficient monitoring/tracking of waste bins, analyzing their data and taking further actions in case of out-of-limits values (i.e. with respect to fire danger, fill level etc.) for improving citizens health and daily life, and second, building a flexible application that can easily be deployed and managed in a distributed manner in any relevant environment, with use of modern orchestrators. Beyond the detailed architectural design, several implementation issues of our approach are thoroughly analyzed and discussed, as well as further extensions and initial performance evaluation. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"E., Sisinni, Emiliano; A., Flammini, Alessandra; M., Gaffurini, Massimiliano; P., Ferrari, Paolo","Exploiting Container-Based Microservices for Reliable Smart Mobility Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208571461&partnerID=40&md5=24a938d6a0147abf311843236763a574","Smart mobility is emerging, addressing heterogeneous scenarios with high impact on technology infrastructures, solutions, and people. Safety and availability are mandatory, forcing the design of new reliable services for localization, health monitoring of the user, maintenance of vehicle, and protection of the environment. This paper proposes a container-based microservice approach to the edge computing in IoT smart mobility scenarios. Since smart mobility backends must manage a large heterogeneity of applications, the proposed approach is promising with respect to the classical solutions (based on “monolithic hardware+software” devices), from the point of view of flexibility, upgradability, security, scalability, and reliability. A demo use case, based on industry-grade hardware and Docker, has been realized and multiple implementations of the same services have been executed in parallel, showing strong independence between them. Moreover, average delays of less than 10 ms are obtained, confirming the usability in several smart mobility (and smart city) applications. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"Y., Wang, Yute; S., Ma, Shangpin; Y., Lai, Yuejun; Y.C., Liang, Yan Cih","Qualitative and quantitative comparison of Spring Cloud and Kubernetes in migrating from a monolithic to a microservice architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160345220&partnerID=40&md5=934bb1ba176edb0946beebd65b607700","The microservice architecture has several advantages over conventional monolithic architectures, such as the ability to develop and deploy services independently and flexibility in dealing with bottlenecks. Most existing works in this field have focused on methodologies by which to divide monolithic systems into microservices, paying little or no attention to the target platform or migration tools, despite the profound effects that they can have on system migration. For a monolithic and service-oriented application, the go-to frameworks and platforms for migration to a microservice architecture are Spring Cloud and Kubernetes. This paper compares the pros and cons of the two approaches and presents experiments involving a stress-testing tool to elucidate performance and scalability under various conditions. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"C., Lira, Cleber; E., Batista, Ernando; F.C., Delicato, Flavia C.; C.V.S., Prazeres, Cássio Vinicius Serafim","Architecture for IoT applications based on reactive microservices: A performance evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151283107&partnerID=40&md5=9b189f74aef91234658998910ac74a86","The Internet has evolved from a network interconnecting computers to a complex ecosystem integrating devices of the most varied types, and enabling the amalgamation of the physical and virtual worlds. Integrating these heterogeneous devices fosters novel services and applications that generate value-added information and actionable knowledge for the end-user. Several challenges are involved in the design and building of IoT ecosystems, which have fostered research in the field, in search of patterns, guidelines, methods and tools that support such activities. In terms of architectural patterns, the microservice architectural style has been increasingly adopted in the development of IoT applications and services. Its adoption promotes some essential properties in IoT, such as scalability and extensibility. As there are always tradeoffs involved in every architectural decision, it is important to analyze whether the benefits brought by the use of microservices do not come at the expense of some loss or degradation of application performance. Recent research has analyzed the performance interference of microservices based on edge computing applications. However, a comprehensive assessment of the performance impact of characteristics inherent to the use of reactive microservices on IoT applications is still missing in the literature. In this paper, we present an experimental evaluation of the performance of IoT applications that make use of an architecture based on reactive microservices. The architecture was proposed by our group in a previous work and was tailored for reliable IoT applications running at the edge of the network. The experiments presented in this paper analyze the application performance based on various benchmark scenarios. In addition, we performed load and scalability testing of an IoT application that adopts the architecture components in a hybrid scenario (real devices and emulated devices). The results obtained were promising. The architecture had a good response to the increase in the workload, not presenting errors, crashes or instabilities due to the increase in IoT data traffic. Moreover, analyzing the overhead generated by the architecture components, there was no performance reduction or service unavailability. Such results point to the fact that the adoption of microservices in the construction of IoT systems can bring effective benefits without jeopardizing their performance due to eventually generated overheads. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S.K., Mondal, Subrota Kumar; X., Wu, Xiaohai; H.D., Kabir, HM Dipu; H., Dai, Hongning; K., Ni, Kan; H., Yuan, Honggang; T., Wang, Ting","Toward Optimal Load Prediction and Customizable Autoscaling Scheme for Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164191859&partnerID=40&md5=39cbcc37de26a42ca93c4dbebe37ccea","Most enterprise customers now choose to divide a large monolithic service into large numbers of loosely-coupled, specialized microservices, which can be developed and deployed separately. Docker, as a light-weight virtualization technology, has been widely adopted to support diverse microservices. At the moment, Kubernetes is a portable, extensible, and open-source orchestration platform for managing these containerized microservice applications. To adapt to frequently changing user requests, it offers an automated scaling method, Horizontal Pod Autoscaler (HPA), that can scale itself based on the system’s current workload. The native reactive auto-scaling method, however, is unable to foresee the system workload scenario in the future to complete proactive scaling, leading to QoS (quality of service) violations, long tail latency, and insufficient server resource usage. In this paper, we suggest a new proactive scaling scheme based on deep learning approaches to make up for HPA’s inadequacies as the default autoscaler in Kubernetes. After meticulous experimental evaluation and comparative analysis, we use the Gated Recurrent Unit (GRU) model with higher prediction accuracy and efficiency as the prediction model, supplemented by a stability window mechanism to improve the accuracy and stability of the prediction model. Finally, with the third-party custom autoscaling framework, Custom Pod Autoscaler (CPA), we packaged our custom autoscaling algorithm into a framework and deployed the framework into the real Kubernetes cluster. Comprehensive experiment results prove the feasibility of our autoscaling scheme, which significantly outperforms the existing Horizontal Pod Autoscaler (HPA) approach. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"H., Huang, Hsinyu; Y.Y., FanJiang, Yong Yi; C., Hung, Chihuang; H., Tsai, Hsingyu; B.H., Lin, Bing Hong","Evaluation of a Smart Intercom Microservice System Based on the Cloud of Things","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161495522&partnerID=40&md5=1935750b663c79eef7c772ac65fb2a81","This research migrates a monolithic smart intercom system to a microservice architecture, making the system more secure, stable, and scalable. The security mechanisms of the instant messaging platform are combined with microservices to improve the security of the system. The stability and performance of microservices are shown to be better than those of monolithic services through experimental tests. Residents can use different instant messaging software instead of the handset to improve the convenience of using this system. This system also implements community broadcasts, platform broadcasts, unit broadcasts, and family broadcasts to exchange messages across different instant messaging platforms. This paper proposes OpenAPI for other smart intercoms to integrate this system’s services and resources. In addition, this study deploys two microservice architectures using a native load balancer with a kube proxy and a service mesh load balancer with an istio proxy. Experiments were conducted during which residents tested the kube proxy and the istio proxy using stress testing tools on two microservice architectures, and the results showed that the kube proxy was slightly better than the istio proxy. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Chakraborty, Sheuli; D., De, Debashis; K., Mazumdar, Kaushik","DoME: Dew computing based microservice execution in mobile edge using Q-learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137031501&partnerID=40&md5=e66cd72a41ed9f9e1ed949d3039665c9","The microservice approach unlatched a new window of distributed service-oriented architecture over the computational horizon. Microservice coordination in a heterogeneous and distributed manner among different edge computing nodes with minimum delay is the challenge of interest. Service migration to vantage locations accompanying bulk data hinge on bandwidth and internet connectivity. This paper proposes a Dew Computing-based Microservice Execution (DoME) scheme using reinforcement learning to alleviate the comprehensive service delay and provide seamless and real-time responses with optimized costs. With internet connectivity, DoME elects the edge server for service migration in exigency. During deficient internet connection, the service execution comes to pass at the dew server of the novel dew-cloud framework. This study imitates the Q-learning method comprehending the dew-edge-cloud framework and develops the Dew-Q algorithm to execute microservices on the move without continuous internet connectivity. The Dew-Q algorithm uses an agent that intelligently assimilates from historical data to envisage the service migration-node selection with the uphold connectivity. The proposed scheme is compared with existing microservice execution policies like FWS and SDGA schemes. The performance evaluations exhibit that the DoME scheme excels 41% to 68% compared with FWS and 12% to 86% with SDGA in terms of execution and migration cost. The proposed scheme dynamically controls microservice execution from the perspective of migration and timeliness with intermittent internet connectivity. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"F., Klinaku, Floriment; S., Speth, Sandro; M., Zilch, Markus; S., Becker, Steffen","Hitchhiker's Guide for Explainability in Autoscaling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158819153&partnerID=40&md5=7d5e5b291ff2006c395f868c461c45f1","Cloud-native applications force increasingly powerful and complex autoscalers to guarantee the applications' quality of service. For software engineers with operational tasks understanding the autoscalers' behavior and applying appropriate reconfigurations is challenging due to their internal mechanisms, inherent distribution, and decentralized decision-making. Hence, engineers seek appropriate explanations. However, engineers' expectations on feedback and explanations of autoscalers are unclear. In this paper, through a workshop with a representative sample of engineers responsible for operating an autoscaler, we elicit requirements for explainability in autoscaling. Based on the requirements, we propose an evaluation scheme for evaluating explainability as a non-functional property of the autoscaling process and guide software engineers in choosing the best-fitting autoscaler for their scenario. The evaluation scheme is based on a Goal Question Metric approach and contains three goals, nine questions to assess explainability, and metrics to answer these questions. The evaluation scheme should help engineers choose a suitable and explainable autoscaler or guide them in building their own. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"F., Guo, Feiyan; B., Tang, Bing; M., Tang, Mingdong; W., Liang, Wei","Deep reinforcement learning-based microservice selection in mobile edge computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135337461&partnerID=40&md5=2eabde589c490696e901a045f9223fa5","In mobile edge computing environment, due to resources constraints of edge devices, when user locations continue changing, the network will be delayed or interrupted, which affects the quality of user’s service access. Previous studies have shown that deploying multiple microservice instances with the same function on multiple edge servers through container technology can solve this problem. However, how to choose the optimal microservice instance from multiple servers in a cloud-edge hybrid environment needs to be further investigated. This paper studies the selection of microservices problem based on the dynamic and heterogeneous characters of the cloud-edge collaborative environment, which is defined as a microservice selection and scheduling optimization problem (MSSP) to minimize users’ service access delay. To cope with the complexity of cloud-edge collaborative environment and improve learning efficiency, MSSP is regarded as a Markov decision-making process, a Deep Deterministic Policy Gradient algorithm for microservice selection called MS_DDPG is then proposed to solve this problem, and the microservice selection strategy experience pool is established in MS_DDPG. Performance evaluations of MS_DDPG based on a real dataset and some synthetic dataset have been conducted, and the results show that MS_DDPG outperforms the other three baseline algorithms. In terms of average access delay, MS_DDPG is reduced by 23.82%. We also validate the performance of MS_DDPG by increasing the number of user requests, and the results also show that MS_DDPG obtains better performance in scalability. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"P., Mangwani, Poonam; N., Mangwani, Niti; S., Motwani, Sachin","Evaluation of a Multitenant SaaS Using Monolithic and Microservice Architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147117782&partnerID=40&md5=0abc86c9eeeeedf8abcc7922374eca58","In the contemporary era, cloud computing has emerged as an eminent technology that offers on-demand services anytime and anywhere over the internet. Cloud environment allows organizations to scale their application based on demand. Traditionally, monolithic approach of application development has begun to face various bottlenecks and challenges. This has promoted a shift to a new paradigm, micro-service architecture for the development of cloud-based applications, which is gaining popularity due to decoupled independent services. Micro-service architecture contemplates overcoming the limited scalability of monolithic architecture. In this paper, a multitenant booking application is designed and developed using both monolithic and micro-service architecture as a case study. The application is deployed as Docker container images on Google cloud platform. The comparison of various factors, such as performance, scalability, load balancing, reliability, resource utilization, and infrastructure cost is performed. JMeter is used as a load generation and performance testing tool. Performance analysis in terms of response time is done for the multitenant booking application. Results indicate that independent scaling of micro-services leads to effective utilization of resources unlike the monolithic approach. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Kim, Seong-hyun; T., Kim, Taehong","Local Scheduling in KubeEdge-Based Edge Computing Environment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147895085&partnerID=40&md5=678b9775f50e7c947d73d1edd92aa637","KubeEdge is an open-source platform that orchestrates containerized Internet of Things (IoT) application services in IoT edge computing environments. Based on Kubernetes, it supports heterogeneous IoT device protocols on edge nodes and provides various functions necessary to build edge computing infrastructure, such as network management between cloud and edge nodes. However, the resulting cloud-based systems are subject to several limitations. In this study, we evaluated the performance of KubeEdge in terms of the computational resource distribution and delay between edge nodes. We found that forwarding traffic between edge nodes degrades the throughput of clusters and causes service delay in edge computing environments. Based on these results, we proposed a local scheduling scheme that handles user traffic locally at each edge node. The performance evaluation results revealed that local scheduling outperforms the existing load-balancing algorithm in the edge computing environment. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"A., Amlou, Abderrahim; A., Abane, Amar; M., Merzouki, Mheni; L.A., Oucheggou, Lydia Ait; Z., Maasaoui, Zineb; A., Battou, Abdella","Automated Network Programmability Using OpenConfig YANG Models and NETCONF Protocol","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190147486&partnerID=40&md5=6cc43a89f48b628283a4e2b89cca4285","This paper introduces a microservice-based architecture designed to enable automation of network programmability and management. Amid the complexity of today's networks and the diversity of equipment, achieving efficient and reliable network programmability poses a significant challenge. Our architecture leverages the OpenConfig YANG models and the NETCONF protocol to simplify network configuration, automate tasks, and avoid errors. We demonstrate how our solution streamlines the collection and configuration workflows, enabling network operators to efficiently manage complex networks. The paper further presents a performance evaluation of the proposed design, which confirms its efficacy in handling complex configurations and its fault tolerance capabilities. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"N.N., Nayim, Nahid Nawal; A., Karmakar, Ayan; M.R., Ahmed, Md Razu; M., Saifuddin, Mohammed; M.H., Kabir, Md Humayun","Performance Evaluation of Monolithic and Microservice Architecture for an E-commerce Startup","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187354427&partnerID=40&md5=5e3b40628851d310370a8f4f2f5d358a","Today's businesses must be flexible enough to develop scalable applications that can influence emerging models of production and management. The prevailing imperative for organizations to adopt new technologies and business strategies stems from the need to enhance the efficiency and optimization of both human and technology resources. The concept of ""software architecture""has been employed to define the overall blueprint of a software system. Software architecture has been proposed as a strategy to decrease complexity and enhance reusability by facilitating system decomposition into its high-level components and their interrelationships. Within the technology sector, the phrase ""e-commerce""is frequently employed to denote the activities encompassing acquiring, vending, and promoting goods and services. The necessity for an e-commerce enterprise lies in reusable code that pays minimal costs and possesses a streamlined infrastructure. Implementing a suitable architectural framework to meet the existing demands of e-commerce is the most optimal strategy presently. This study undertakes a comparative analysis of monolithic and microservice architectures in the context of an e-commerce startup. Two separate software applications are developed, each following one of the mentioned architectures while utilizing the same technology platform. The findings of the tests suggest that monolithic architectures have the potential to decrease infrastructure expenses and enhance performance when compared to traditional microservice architectures. In conclusion, we clarify the challenges encountered during the deployment and implementation phases of microservices and monolithic web applications. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"J., Liao, Jianxiong; Z., Zhou, Zhi; F., Xu, Fei; X., Chen, Xu","STAAF: Spatial-Temporal Correlations Aware AutoScaling Framework for Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187325784&partnerID=40&md5=b2ebd36a3f845c0deec5844f20ade049","Microservice architecture has gained widespread popularity for simplifying the development of online applications. However, the complicated dependencies among microservices and fluctuating nature of online application workloads can pose significant challenges in efficiently provisioning resources to microservices to improve the performance of applications and optimize resource utilization. In this paper, we present STAAF, a proactive microservice autoscaling framework that takes the spatial-temporal correlations of microservices into account to globally provision resources. STAAF predicts the workloads of the entering microservice by analyzing the temporal pattern of workloads and estimates the actual workloads of all microservices to solve the problems brought by spatial correlations of microservices. In addition, STAAF profiles the mapping between resource utilization and request arrival rate to proactively allocate resources while optimizing resource usage. Experiments conducted with a microservice benchmark indicate that STAAF can make appropriate autoscaling decisions during traffic surges and reduce tail latency by 74% on average compared to the Kubernetes autoscaler. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"X., Wang, Xiaoyun; Y., Wu, Yanjun; J., Yu, Jun; Y., Wang, Yunzhi; E., Liu, Erxi; D., Zhao, Disheng; G., Liu, Guoan","TeleRobot: Design and Implementation of a Live Remote Interaction Platform for Robots","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187294662&partnerID=40&md5=376eed4be59b24143ac2ac66ceb5f5c1","With the increasing number and variety of robots, there are still many problems and limitations in the offline communication and cooperation platform. To address this issue, we present a universal ROS robot live interaction platform named TeleRobot that offers features such as multi-angle robot live streaming, remote interaction with robots, and online discussions. The implementation of TeleRobot is based on microservice architecture and WebRTC technology, utilizing Kurento Media Server (KMS) for streaming media transmission, and rosbridge for real-time remote interaction with robots. To enable compatibility with the communication between web, WeChat mini-programs and robots, we have developed a custom high-level text transfer protocol called HIKER based on WebSocket. We also provide web UI for users to manage robots, allowing them to quickly configure the IP, port, and control commands to remotely access their robots for interaction in the live broadcast room. This paper describes in detail the architecture design, system components and system implementation of TeleRobot. Finally, we applied the TeleRobot platform on the XBot-ARM robotic arm and the human-robot interaction (HRI) performance evaluation system of the platform was established using Analytic Hierarchy Process (AHP). © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"H., Zhang, Hang","Performance Evaluation of Digital Traffic Microservice Chain Application of YOLOv5 Model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186763772&partnerID=40&md5=892a96b76710cdce5acbaf07e773a447","In the realm of digital transformation, the utilization of AI-powered digital technology encounters a multitude of challenges, among which lies the imperative need to restructure the IT framework. This endeavor centers on a digital intelligent transportation system, leveraging YOLOv5, Kafka, and Flink big data technologies as its pillars of support. Its core objective is to conceptualize a microservices architecture tailored for digital technology applications while introducing the 3D (Dimensional Factors in Software Architecture) architectural model for a comprehensive assessment of microservice interconnections. Empirical findings underscore that the architectural design and execution of digital technology applications unfold on a substantial scale. Moreover, it culminated in the successful evaluation of both individual microservice performance and the holistic performance of interlinked microservices, thereby offering pragmatic insights for the efficient deployment of digital technologies. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"E., Park, Eun-chan; K., Baek, Kyeong-deok; E., Cho, Eunho; I., Ko, Inyoung","Fully Decentralized Horizontal Autoscaling for Burst of Load in Fog Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182723575&partnerID=40&md5=70fd5fdf9cb28a9198fba82877092278","With the increasing number of Web of Things devices, the network and processing delays in the cloud have also increased. As a solution, fog computing has emerged, placing computational resources closer to the user to lower the communication overhead and congestion in the cloud. In fog computing systems, microservices are deployed as containers, which require an orchestration tool like Kubernetes to support service discovery, placement, and recovery. A key challenge in the orchestration of microservices is automatically scaling the microservices in case of an unpredictable burst of load. In cloud computing, a centralized autoscaler can monitor the deployed microservice instances and make scaling actions based on the monitored metric values. However, monitoring an increasing number of microservices in fog computing can cause excessive network overhead and thereby delay the time to scaling action. We propose DESA, a fully DEcentralized Self-adaptive Autoscaler through which microservice instances make their own scaling decisions, cloning or terminating themselves through self-monitoring. We evaluate DESA in a simulated fog computing environment with different numbers of fog nodes. Furthermore, we conduct a case study with the 1998 World Cup website access log, examining DESA’s performance in a realistic scenario. The results show that DESA successfully reduces the scaling reaction time in large-scale fog computing systems compared to the centralized approach. Moreover, DESA resulted in a similar maximum number of instances and lower average CPU utilization during bursts of load. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"E., Incerto, Emilio; R., Pizziol, Roberto; M., Tribastone, Mirco","μOpt: An Efficient Optimal Autoscaler for Microservice Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181766241&partnerID=40&md5=c7ed83ffdaf554a07165a7ad24068e8b","Microservices are a popular architecture for cloudbased applications subject to stringent performance requirements. To effectively serve variable workloads, autoscaling allocates computational resources ideally at the lowest possible cost. Although several autoscaling techniques have already been proposed in the literature, they suffer from high computational complexity. Here we propose μOpt as a computationally efficient model-based autoscaler for microservices. By solving a nonlinear optimization problem that embeds a layered queueing network (LQN) model, μOpt computes optimal configurations maximizing performance while minimizing allocated resources. We validate μOpt on a benchmark microservice application, reporting fast solution times (~10-1 s) that enable prompt reactions to highly variable workloads. Compared to a state-of-the-art autoscaler based on LQN and genetic algorithms, μOpt achieves higher performance (~6%-8%) with significantly fewer allocated resources (~15%-35%) in the presence of both synthetic and real-world workloads. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"J., Sychowiec, Jakub; Z.E., Zieliński, Zbigniew E.","An Experimental Framework for Secure and Reliable Data Streams Distribution in Federated IoT Environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179177552&partnerID=40&md5=32746d46d2debe904a688da69619fddf","An increasing number of Internet of Things (IoT) applications are based on a federated environment. Examples include the creation of federations of NATO countries and non- NATO entities participating in missions (Federated Mission Networking) or the interaction of civilian services and the military when providing Humanitarian Assistance And Disaster Relief. Federations are often formed on an ad hoc basis, with the primary goal of combining forces in a federated mission environment at any time, on short notice, and with optimization of the resources involved. One of the leading security challenges in a federated environment of separate IoT administrative domains is effective identity and access management, which is the basis for establishing a relationship of trust and secure communication between IoT devices belonging to different partners. When carrying out missions involving the military and ensuring security, meeting requirements for immediate inter operability is important. In the paper, an attempt has been made to develop a system architecture framework for secure and reliable data streams distribution in a multi-organizational federation environment, where data authentication is based on IoT device identity (fingerprint). Moreover, a hardware-software IoT gateway has been proposed for the verification process and the integration of Hyperledger Fabric's distributed ledger technology, the Apache Kafka message broker, and data-processing microservices implemented using the Kafka Streams API library. The performance tests conducted confirm the suitability of the developed system framework for processing and distributing audiovideo data in a federation IoT environment. Also, a high-level security and reliability assessment was conducted in the paper. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Jhingran, Sushant; N., Rakesh, Nitin","Application Deployment and Performance Measurement in Serverless Cloud for Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179135300&partnerID=40&md5=c1291e4a4c7fa89978a00eea9006cec0","The effectiveness of Cloud technology relies heavily on its ability to perform at a high level. To measure this performance, it is necessary to conduct a performance evaluation based on specific aims and applications and assess the capabilities of the cloud services. In the case of enterprise applications deployed on the cloud, the service provider must consider the application's deployment model, security, networking, and operational constraints. This evaluation involves identifying benchmarks, configuring the system, running tests, analyzing results, and providing recommendations. There are various performance metrics that can be applied to different aspects of the cloud services to evaluate their performance. The figures below display data on resource utilization and the impact of the load on the application. Microservices offer organizations the opportunity to deploy applications on the cloud by providing web service functions and an architecture that enables scaling and updating of applications with minimal inconsistency. Through public cloud technology such as Amazon Web Services, organizations can deploy secure and valuable applications to the cloud. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Di Meglio, Sergio; L.L., Starace, Luigi L.L.; S., Di Martino, Sergio","Starting a new REST API project? A performance benchmark of frameworks and execution environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177037115&partnerID=40&md5=cba298318fd8e5cee97056f73edec897","REST APIs have become widely adopted in the software industry, finding extensive usage for businesscritical purposes such as data exchange, mobile app development, and microservice architectures. Such popularity has led to a proliferation of dedicated frameworks, making it challenging for developers and organizations to choose which one to use for developing resource-efficient solutions. Adding to the complexity is the possibility of adopting also different execution environments, such as GraalVM, that offer advantages such as faster startup times, lower memory footprint, and polyglot capabilities. Some prior works have investigated the performance of various frameworks for REST APIs. Still, these studies often consider simplistic scenarios with a single endpoint, which fail to capture the complexity and diversity of real-world REST API applications. Furthermore, the impact of different execution environments on performance was often overlooked. Consequently, there remains a significant knowledge gap in comprehensively assessing the combined influence of frameworks and execution environments on the performance of REST APIs. This study aims to move a first step towards bridging that gap, by conducting a thorough performance benchmark that encompasses real-world REST APIs and considers also the effects of different execution environments. More in detail, the study focuses on two of the most popular programming language and framework combinations for REST APIs, namely JavaScript with the Express framework and Java with the Spring framework. As for the execution environment, we consider both mainstream execution environments for JavaScript and Java (Node and OpenJDK, respectively), and GraalVM, which can execute both Java and JavaScript software. The benchmarking process involves conducting realistic load and stress tests using state-of-The-Art tools. Results reveal significant differences in performance across the considered combinations, providing insights that could support developers and system architects in making more informed decisions on the technologies to use for their REST API projects. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"T., Theodoropoulos, Theodoros; A., Makris, Antonios; I., Korontanis, Ioannis; K., Tserpes, Konstantinos","GreenKube: Towards Greener Container Orchestration using Artificial Intelligence","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174897113&partnerID=40&md5=9fdb5ebfbac06d9d9a60372383f5114c","This paper introduces the GreenKube framework, which aims to reduce energy consumption while meeting Quality of Service (QoS) requirements through the use of various AI methodologies such as deep learning time-series forecasting, deep reinforcement learning, and graph neural networks. The paper also explores the limitations of contemporary container orchestration frameworks, including Kubernetes, and describes how GreenKube aims to progress beyond them. Additionally, the paper presents a prototype of the GreenKube framework that was evaluated in an extensive simulation using CloudSim Plus, and compares its performance to Kubernetes' Horizontal Pod Autoscaler. The results demonstrate that GreenKube outperforms Kubernetes in terms of latency and task execution time while requiring fewer computational resources. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Kakade, Soham; G., Abbigeri, Gurutej; O., Prabhu, Om; A., Dalwayi, Akash; D.G., Narayan, D. G.; S., Patil, Somashekar; B., Sunag, Bhagya","Proactive Horizontal Pod Autoscaling in Kubernetes using Bi-LSTM","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174746735&partnerID=40&md5=c5695b353064aa6a7ff8ba220daae732","The management and implementation of distributed applications has been completely transformed by software containerization technologies. Before providing the containers to the clients, they must be correctly scaled. It is crucial that cloud service providers efficiently scale and distribute their resources and avoid under and over-provisioning of resources. Particularly, additional care should be taken for under provisioning of resources to avoid crashing of distributed applications. It is hard to manually assign resources to customers based on their continuously fluctuating workloads. The resource provisioning must be quick and automatic. Kubernetes provides a feature called autoscaler which allocates resources dynamically. However, the default autoscaler in Kubernetes is reactive as it will scale resources when load comes and allocation takes some time to get ready. This reactive nature may decrease the overall performance of application deployed. Hence, in this work, we present a proactive autoscaler mechanism that employs Bi-LSTM model to anticipate future demands and scale the containers automatically. The results using 3 node Kubernetes setup reveals that Bi-LSTM performs better than stacked LSTM and proactive autoscaler performs better than default Kubernetes autoscaler. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"A., Alabbas, Areej; A.K., Kaushal, Ashish Kumar; O., Almurshed, Osama; O.F., Rana, Omer F.; N., Auluck, Nitin; C., Perera, Charith","Performance Analysis of Apache OpenWhisk Across the Edge-Cloud Continuum","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174310916&partnerID=40&md5=07a9928a3eb5f5c907c3004b8646fcb3","Serverless computing offers opportunities for auto-scaling, a pay-for-use cost model, quicker deployment and faster updates to support computing services. Apache OpenWhisk is one such open-source, distributed serverless platform that can be used to execute user functions in a stateless manner. We conduct a performance analysis of OpenWhisk on an edge-cloud continuum, using a function chain of video analysis applications. We consider a combination of Raspberry Pi and cloud nodes to deploy OpenWhisk, modifying a number of parameters, such as maximum memory limit and runtime, to investigate application behaviours. The five main factors considered are: cold and warm activation, memory and input size, CPU architecture, runtime packages used, and concurrent invocations. The results have been evaluated using initialization, and execution time, minimum memory requirement, inference time and accuracy. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Zhang, Shengnan; H., Xu, Hanchuan; L., Nie, Lanshun; D., Zhan, Dechen","Microservice-Based Computation Offloading in Mobile Edge Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172699901&partnerID=40&md5=3282dc8148aed8ba22576e529a0de1b6","Microservices are an emerging service architecture that, when combined with mobile edge computing (MEC), can offer low latency to nearby mobile users. Several instances of microservices hosted on the server can be started or stopped flexibly to address computational requests from users at various times of the day or night thanks to the characteristics of dynamic deployment, quick start-up, and easy transfer of microservices. from the perspective of the application provider, we need to ensure the quality of service for end-users while minimizing the number of leased edge servers. To enable efficient use of MEC resources and provide reliable performance for mobile devices, we developed an Ant colony Optimization algorithm for computational offloading based on Microservices in MEC (ACO_MMCO). then we simulate the scenario using the simulation program iFogSim2 and real data sets. According to the experimental findings, this method’s generated offloading policy outperforms the benchmark method in a number of performance evaluation criteria. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"P.S., Moreira, Pedro Silva; A.N., Ribeiro, António Nestor; J.M.C., Silva, João Marco C.","AGE: Automatic Performance Evaluation of API Gateways","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171994962&partnerID=40&md5=f0b91a63d7bbf9fdb4ae7b9d834c3951","The increasing use of microservices architectures has been accompanied by the profusion of tools for their design and operation. One relevant tool is API Gateways, which work as a proxy for microservices, hiding their internal APIs, providing load balancing, and multiple encoding support. Particularly in cloud environments, where the inherent flexibility allows on-demand resource deployment, API Gateways play a key role in seeking quality of service. Although multiple solutions are currently available, a comparative performance assessment under real workloads to support selecting the more suitable one for a specific service is time-consuming. In this way, the present work introduces AGE, a service capable of automatically deploying multiple API Gateways scenarios and providing a simple comparative performance indicator for a defined workload and infrastructure. The designed proof of concept shows that AGE can speed up API Gateway deployment and testing in multiple environments. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"H., Luo, Helin; Y., Lin, Yibing; C., Liao, Chenchi; Y., Huang, Yunghui","An IoT-Based Microservice Platform for Virtual Puppetry Performance","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171566490&partnerID=40&md5=9fcd8f6f2b2a523937735ccdd803d1ca","This study proposes a web-based real-time remote-controlled virtual puppetry (avatar) performance platform called AvatarTalk to combine traditional Taiwanese puppetry, the Internet of Things (IoT), and 3D avatar models. AvatarTalk enables puppeteers to control avatar puppets on the web using either motion capture gloves or camera-based image recognition. This transformative performance approach not only facilitates remote performances and multi-screen presentations across various devices but also introduces a fresh gesture interpretation technique for dance artists. AvatarTalk supports a mechanism that can accommodate new control and puppet devices from other approaches through the IoT-based microservice concept. We develop a calibration procedure to enable the accurate capture of hand gestures to manipulate the movements of virtual puppets, empowering them to perform fundamental traditional puppetry poses such as nodding, bowing, and synchronized hand movements. We have conducted experiments to show the accuracy of AvatarTalk calibration. Our study indicates that AvatarTalk can almost detect the right gestures (98.75%-100% recall) and very seldom mistake the wrong gestures (90.8%-100% precision). Additionally, we also provide the mechanism to measure the delays of controlling the puppets. An analytic model is proposed to design the delay times of the messages between the control device of a puppeteer and the AvatarTalk server. In the current AvatarTalk implementation, if the message delay does not exceed 0.1 seconds, four puppeteers can synchronize their actions if the elapsed time between two actions of a puppeteer is longer than 0.3 second. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Ris, Simone; J.C.T.D., Araujo, Jean Carlos Teixeira De; D.W.S.C., Beserra, David Willians Santos Cavalcanti","A Systemic Mapping of Methods and Tools for Performance Analysis of Data Streaming with Containerized Microservices Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169796889&partnerID=40&md5=d0a3c230860a0c6826989ee9304ab524","With the Internet of Things (IoT) growth and customer expectations, the importance of data streaming and streaming processing has increased. Data Streaming refers to the concept where data is processed and transmitted continuously and in real-time without necessarily being stored in a physical location. Personal health monitors and home security systems are examples of data streaming sources. This paper presents a systematic mapping study of the performance analysis of Data Streaming systems in the context of Containerization and Microservices. The research aimed to identify the main methods, tools, and techniques used in the last five years for the execution of this type of study. The results show that there are still few performance evaluation studies for this system niche, and there are gaps that must be filled, such as the lack of analytical modeling and the disregard for communication protocols' influence. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"M., Mekki, Mohamed; B., Bouziane, Brik; A., Ksentini, Adlen; C.V., Verikoukis, Christos V.","XAI-Enabled Fine Granular Vertical Resources Autoscaler","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166479369&partnerID=40&md5=8a789a314b24748a333a302f7601608a","Fine-granular management of cloud-native computing resources is one of the key features sought by cloud and edge operators. It consists in giving the exact amount of computing resources needed by a microservice to avoid resource over-provisioning, which is, by default, the adopted solution to prevent service degradation. Fine-granular resource management guarantees better computing resource usage, which is critical to reducing energy consumption and resource wastage (vital in edge computing). In this paper, we propose a novel Zero-Touch management (ZSM) framework featuring a fine-granular computing resource scaler in a cloud-native environment. The proposed scaler algorithm uses Artificial Intelligence (AI)/Machine Learning (ML) models to predict microservice performances; if a service degradation is detected, then a root-cause analysis is conducted using eXplainable AI (XAI). Based on the XAI output, the proposed framework scales only the needed (exact amount) resources (i.e., CPU or memory) to overcome the service degradation. The proposed framework and resource scheduler have been implemented on top of a cloud-native platform based on the well-known Kubernetes tool. The obtained results clearly indicate that the proposed scheduler with lesser resources achieves the same service quality as the default scheduler of Kubernetes. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"H., Jang, Hung-Chin; S., Luo, Shihyu","Enhancing Node Fault Tolerance through High-Availability Clusters in Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166373589&partnerID=40&md5=fd17a2c822299d6a92cad872bd8b5d97","Microservices architecture and containerization technology have become ubiquitous recently. Docker containers have emerged as a standardized software packaging unit, offering rapid deployment, flexible scaling, and cross-platform operability. This enables the industry to focus on innovation and business needs while managing underlying infrastructure effortlessly. With the advent of technologies such as the Internet of Things, big data, and machine learning, there is a growing demand for parallel processing of large amounts of data across multiple hosts. Ensuring system resource availability and stability is essential, especially when services experience unexpected interruptions. As the number of containers increases, docker has introduced a container management platform called Docker Swarm to manage and schedule containers across multiple hosts and adjust their operational scale based on workload. If a container unexpectedly stops operating, the Docker Swarm cluster generates new containers automatically, ensuring the high availability of container services. Meanwhile, Google has introduced a container scheduling system called Kubernetes. The Horizontal Pod Autoscaler in Kubernetes automatically adjusts the number of service Pods based on node target memory usage, improving overall resource utilization. While Kubernetes can simplify application management and deployment, the cluster's performance after deployment has yet to be effectively evaluated and compared. This study aims to optimize and adjust cluster node resource configuration and parameter settings using tools such as Vertical Pod Autoscaler, Descheduler, Ingress Controller, and Scheduling Framework. The performance of Kubernetes is compared with the Docker Swarm architecture to analyze the average response time, longest response time, connection success rate, success count, and failure count of the overall cluster's web service traffic workload. The optimization is carried out to ensure the high availability of container services in case of node failures. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"A., Khichane, Abderaouf; I., Fajjari, Ilhem; N., Aitsaadi, Nadjib; A.M., Guéroui, Abdelhak Mourad","5GC-Observer: a Non-intrusive Observability Framework for Cloud Native 5G System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164712975&partnerID=40&md5=d98cd440fca4aed6fe1c39420108ca6c","Telco stakeholders are developing a deeper understanding of cloud native technologies and adopting them faster than few years ago. It is undeniable that migrating legacy telco applications to microservice-based architectures accelerates and facilitates the development of new network services while offering a high level of granularity. However, cloud native raises new operational challenges. In order to achieve an efficient management of network services, new solutions are required to monitor and track widely distributed cloud native network functions while considering their specificity. In this paper, we propose an innovative framework, 5GC-Observer, for the observability of cloud native 5G network services. To the best of our knowledge, no such a solution has been found to date. To achieve its goal, 5GC-Observer relies on the eBPF technology to monitor the network traffic circulating between the 5G core components and report telemetry data. Besides, we leverage a statistical method to detect Quality of Service degradation based on reported telemetry data. Such an approach highlights the richness of the data acquired by our solution and its capability to detect unexpected network-related anomalies. The latter are not detectable through standard observability solutions. Performance evaluation shows that our solution generates low overhead while giving insight into the 5G core system and its internal and external exchanges. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Sithiyopasakul, Saran; T., Archevapanich, Tuanjai; B., Purahong, Boonchana; P., Sithiyopasakul, Paisan; A., Lasakul, Attasit; C., Benjangkaprasert, Chawalit","Performance Evaluation of Infrastructure as a Service across Cloud Service Providers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162951395&partnerID=40&md5=2ce2cdf154375fe11184ecfeb44be763","The purpose of this research aims to monitor, analyze, and compare the performance of infrastructure as a service (IaaS) between the selective cloud providers. To assure which cloud provider has more stability, reliability, and scalability. This paper focuses on performance testing based on a deployed web server in the cloud environment. The main feature of cloud computing is scalability thus most common IaaS cloud service providers (CSPs) have Auto Scaling features for instances or virtual machines. Not only does this paper gives the experimental results of the scaling scalability testing, but it also provides the results of recovery testing to inspect how long a web server is able to recover from failures and load testing which simulated traffic requests. Testing was conducted in the major public clouds of Google Cloud Platform (GCP), Microsoft Azure, and Amazon Web Services (AWS). Azure performs the most efficiently of almost all testing but hardest to configure. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"P., Liu, Peng; C., Deng, Chunyu; D., Wang, Dazhong","Design and Implementation of Work Order Acceptance System Based on Microservice Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162654128&partnerID=40&md5=8533fb811bbe18e1621a0889ad230554","Professional customer service to solve or circulate corresponding customer problems can not only solve customer problems quickly, improve the service image of the enterprise, but also lay a solid foundation for the future business development of the enterprise. By analyzing the demand of the work order system of the sports lottery customer service call center, a work order processing system based on a microservice architecture is constructed in the article. This article mainly explains and implements the five modules ofincoming call management, outgoing call management, permission management, questionnaire management, and customer management. The system is implemented using software development methods such as the Spring Cloud distributed system framework and Redis. The service deployment methods of cluster services and master-slave serviceswere adopted. The Docker platform technology was used to configure the system's operating environment, and the JMeter stress test tool was used to test the system's high-availability aspects during the test. Strongly expandable work order system. At the same time, the system uses the Word2vec module in Python to construct a short text keyword extraction method, exposes the service API, and registers it with the service registration center for use by the work order acceptance system, improving the agent's tagging of work orders's efficiency. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"G., Coviello, Giuseppe; K., Rao, Kunal; C.G., De Vita, Ciro Giuseppe; G., Mellone, Gennaro; P., Benedetti, Priscilla; S.T., Chakradhar, Srimat T.","Content-aware auto-scaling of stream processing applications on container orchestration platforms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162239577&partnerID=40&md5=0775b7fde69a7f40ce82a34acc7994b4","Modern applications are designed as an interacting set of microservices, and these applications are typically deployed on container orchestration platforms like Kubernetes. Several attractive features in Kubernetes make it a popular choice for deploying applications, and automatic scaling is one such feature. The default horizontal scaling technique in Kubernetes is the Horizontal Pod Autoscaler (HPA). It scales each microservice independently while ignoring the interactions among the microservices in an application. In this paper, we show that ignoring such interactions by HPA leads to inefficient scaling, and the optimal scaling of different microservices in the application varies as the stream content changes. To automatically adapt to variations in stream content, we present a novel system called DataX AutoScaler that leverages knowledge of the entire stream processing application pipeline to efficiently auto-scale different microservices by taking into account their complex interactions. Through experiments on real-world video analytics applications, such as face recognition and pose classification, we show that DataX AutoScaler adapts to variations in stream content and achieves up to 43% improvement in overall application performance compared to a baseline system that uses HPA. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Wei, Sixiao; H., Huang, Hui; G., Chen, Genshe; E.P., Blasch, Erik Philip; Y., Chen, Yu; R., Xu, Ronghua; K.D., Pham, Khanh D.","RODAD: Resilience Oriented Decentralized Anomaly Detection for Urban Air Mobility Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160641039&partnerID=40&md5=19aa19efc0db7d0c7e936afbf64422dd","Urban air mobility (UAM) helps ease traffic congestion and offers cleaner, faster, and safer transportation, especially for densely populated areas. Recent events have shown that modern unmanned aerial vehicles (UAVs) are vulnerable to attacks through buggy or malicious devices, which raise concerns regarding performance, security, and privacy on UAM networks. Existing Air Traffic Service (ATS) providers mainly rely on a centralized system (e.g., Information Display System) for data aggregation, sharing, and security policy enforcement; and it incurs critical issues related to a bottleneck of data analysis, provenance, and consistency in terms of less efficiency with large computational resources, and high false positive with low flexibility. In this paper, we develop a Resilience Oriented Decentralized Anomaly Detection (RODAD) framework to maximize UAM capability to secure data access among aircraft and ATS service providers based on microservices technologies in an edge-fog-cloud computing paradigm. Machine learning based anomaly detection (MLAD) is developed to detect anomaly behaviors (e.g., aircraft route anomaly) against both single-feature and multi-feature spoofing attacks across avionics mission data. Two GPS spoofing attack scenarios (e.g., restricted and generalized) with four attacking types (e.g., continuous, interim, biased, random) are crafted for the performance evaluation. A hardware-in-the-loop (HITL) implementation is also developed to demonstrate the effectiveness of RODAD for supporting real-time resilient analysis. Our experiments validate the performance of RODAD in detection accuracy and efficiency against spoofing attacks for UAM. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Ren, Ziheng; J., Xu, Jiangfeng","Research on the Implementation of Kubernetes Automatic Scaling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159774534&partnerID=40&md5=23418a88db3c99f4c0b3f7cdaca55100","Elastic scaling is one of the important features of cloud native. In the actual production environment, because the business requirements and application resource load are in a dynamic change, the deployment of business and allocation of resources can be adjusted in real time through elastic scaling to ensure the overall quality of service of the system. As the current mainstream container orchestration technology, Kubernetes' built-in elastic scaling strategy HPA (Horizontal Pod Autoscaler) obtains the corresponding indicators by monitoring the component metrics and calculates the expected value of the replica by comparing it with the user-defined threshold, thus realizing the elastic scaling function. Although this scaling strategy can solve the problem of dynamic scaling, there are problems of response delay and scaling jitter, which makes the quality of service of the system unable to be guaranteed in a specific scaling period. In view of the above problems, the predictive elastic scaling strategy is improved, and the fuzzy AR (p) time series model is built to predict the specified load indicators, so as to achieve predictive elastic scaling. The experimental results show that this strategy can solve the response delay problem well and reduce the unnecessary jitter scaling. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"A., Karthikeyan, Ajaykrishna; N., Natarajan, Nagarajan; G., Somashekar, Gagan; L., Zhao, Lei; R., Bhagwan, Ranjita; R., Fonseca, Rodrigo; T., Racheva, Tatiana; Y., Bansal, Yogesh","SelfTune: Tuning Cluster Managers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158850658&partnerID=40&md5=9204d00f47ed8701bf486f9c6565f9b2","Large-scale cloud providers rely on cluster managers for container allocation and load balancing (e.g., Kubernetes), VM provisioning (e.g., Protean), and other management tasks. These cluster managers use algorithms or heuristics whose behavior depends upon multiple configuration parameters. Currently, operators manually set these parameters using a combination of domain knowledge and limited testing. In very large-scale and dynamic environments, these manually-set parameters may lead to sub-optimal cluster states, adversely affecting important metrics such as latency and throughput. In this paper we describe SelfTune, a framework that automatically tunes such parameters in deployment. SelfTune piggybacks on the iterative nature of cluster managers which, through multiple iterations, drives a cluster to a desired state. Using a simple interface, developers integrate SelfTune into the cluster manager code, which then uses a principled reinforcement learning algorithm to tune important parameters over time. We have deployed SelfTune on tens of thousands of machines that run a large-scale background task scheduler at Microsoft. SelfTune has improved throughput by as much as 20% in this deployment by continuously tuning a key configuration parameter that determines the number of jobs concurrently accessing CPU and disk on every machine. We also evaluate SelfTune with two Azure FaaS workloads, the Kubernetes Vertical Pod Autoscaler, and the DeathStar microservice benchmark. In all cases, SelfTune significantly improves cluster performance. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"M., Niazi, Mushtaq; S., Abbas, Sagheer; A.H.A., Soliman, Abdel Hamid Ali; T., Alyas, Tahir; S., Asif, Shazia; T., Faiz, Tauqeer","Vertical Pod Autoscaling in Kubernetes for Elastic Container Collaborative Framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139029564&partnerID=40&md5=569654b3af7baa89677dde44bb72fc1e","Kubernetes is an open-source container management tool which automates container deployment, container load balancing and container(de)scaling, including Horizontal Pod Autoscaler (HPA), Vertical Pod Autoscaler (VPA). HPA enables flawless operation, interactively scaling the number of resource units, or pods, without downtime. Default Resource Metrics, such as CPU and memory use of host machines and pods, are monitored by Kubernetes. Cloud Computing has emerged as a platform for individuals beside the corporate sector. It provides cost-effective infrastructure, platform and software services in a shared environment. On the other hand, the emergence of industry 4.0 brought new challenges for the adaptability and infusion of cloud computing. As the global work environment is adapting constituents of industry 4.0 in terms of robotics, artificial intelligence and IoT devices, it is becoming eminent that one emerging challenge is collaborative schematics. Provision of such autonomous mechanism that can develop, manage and operationalize digital resources like CoBots to perform tasks in a distributed and collaborative cloud environment for optimized utilization of resources, ensuring schedule completion. Collaborative schematics are also linked with Bigdata management produced by large scale industry 4.0 setups. Different use cases and simulation results showed a significant improvement in Pod CPU utilization, latency, and throughput over Kubernetes environment. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Yao, Zhiyuan; Y., Desmouceaux, Yoann; J.A., Cordero-Fuertes, Juan Antonio; M., Townsley, Mark; T.H., Clausen, Thomas Heide","Aquarius - Enable Fast, Scalable, Data-Driven Service Management in the Cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136052726&partnerID=40&md5=b9916960db8e5caebcfd45b5d166536e","In order to dynamically manage and update networking policies in cloud data centers, Virtual Network Functions (VNFs) use, and therefore actively collect, networking state information - and in the process, incur additional control signaling and management overhead, especially in larger data centers. In the meantime, VNFs in production prefer distributed and straightforward heuristics over advanced learning algorithms to avoid intractable additional processing latency under high-performance and low-latency networking constraints. This paper identifies the challenges of deploying learning algorithms in the context of cloud data centers, and proposes Aquarius to bridge the application of machine learning (ML) techniques on distributed systems and service management. Aquarius passively yet efficiently gathers reliable observations, and enables the use of ML techniques to collect, infer, and supply accurate networking state information - without incurring additional signaling and management overhead. It offers fine-grained and programmable visibility to distributed VNFs, and enables both open- and close-loop control over networking systems. This paper illustrates the use of Aquarius with a traffic classifier, an auto-scaling system, and a load balancer - and demonstrates the use of three different ML paradigms - unsupervised, supervised, and reinforcement learning, within Aquarius, for network state inference and service management. Testbed evaluations show that Aquarius suitably improves network state visibility and brings notable performance gains for various scenarios with low overhead. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"J., Liu, Jianshu; S., Zhang, Shungeng; Q., Wang, Qingyang; J., Wei, Jinpeng","Coordinating Fast Concurrency Adapting With Autoscaling for SLO-Oriented Web Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124845516&partnerID=40&md5=835d45e44802231c2c3aa362e358cd23","Cloud providers tend to support dynamic computing resources reallocation (e.g., Autoscaling) to handle the bursty workload for web applications (e.g., e-commerce) in the cloud environment. Nevertheless, we demonstrate that directly scaling a bottleneck server without quickly adjusting its soft resources (e.g., server threads and database connections) can cause significant response time fluctuations of the target web application. Since soft resources determine the request processing concurrency of each server in the system, simply scaling out/in the bottleneck service can unintentionally change the concurrency level of related services, inducing either under- or over-utilization of the critical hardware resource. In this paper, we propose the Scatter-Concurrency-Throughput (SCT) model, which can rapidly identify the near-optimal soft resource allocation of each server in the system using the measurement of each server's real-time throughput and concurrency. Furthermore, we implement a Concurrency-aware autoScaling (ConScale) framework that integrates the SCT model to quickly reallocate the soft resources of the key servers in the system to best utilize the new hardware resource capacity after the system scaling. Based on extensive experimental comparisons with two widely used hardware-only scaling mechanisms for web applications: EC2-AutoScaling (VM-based autoscaler) and Kubernetes HPA (container-based autoscaler), we show that ConScale can successfully mitigate the response time fluctuations over the system scaling phase in both VM-based and container-based environments. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"D., Sokolowski, Daniel","Infrastructure as code for dynamic deployments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143081480&partnerID=40&md5=856026cddff49c603c9dde8dfe626e44","Modern DevOps organizations require a high degree of automation to achieve software stability at frequent changes. Further, there is a need for flexible, timely reconfiguration of the infrastructure, e.g., to use pay-per-use infrastructure efficiently based on application load. Infrastructure as Code (IaC) is the DevOps tool to automate infrastructure. However, modern static IaC solutions only support infrastructures that are deployed and do not change afterward. To implement infrastructures that change dynamically over time, static IaC programs have to be (updated and) re-run, e.g., in a CI/CD pipeline, or configure an external orchestrator that implements the dynamic behavior, e.g., an autoscaler or Kubernetes operator. Both do not capture the dynamic behavior in the IaC program and prevent analyzing and testing the infrastructure configuration jointly with its dynamic behavior. To fill this gap, we envision dynamic IaC, which augments static IaC with the ability to define dynamic behavior within the IaC program. In contrast to static IaC programs, dynamic IaC programs run continuously. They re-evaluate program parts that depend on external signals when these change and automatically adjust the infrastructure accordingly. We implement DIaC as the first dynamic IaC solution and demonstrate it in two realistic use cases of broader relevance. With dynamic IaC, ensuring the program's correctness is even harder than for static IaC because programs may define many target configurations in contrast to only a few. However, for this reason, it is also more critical. To solve this issue, we propose automated, specialized property-based testing for IaC programs and implement it in ProTI. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"K., Bajaj, Karan; B., Sharma, Bhisham; R.N., Singh, Raman Nitin","Implementation analysis of IoT-based offloading frameworks on cloud/edge computing for sensor generated big data","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117085125&partnerID=40&md5=035bab8a6538275cceca0b874405a2de","The Internet of Things (IoT) applications and services are increasingly becoming a part of daily life; from smart homes to smart cities, industry, agriculture, it is penetrating practically in every domain. Data collected over the IoT applications, mostly through the sensors connected over the devices, and with the increasing demand, it is not possible to process all the data on the devices itself. The data collected by the device sensors are in vast amount and require high-speed computation and processing, which demand advanced resources. Various applications and services that are crucial require meeting multiple performance parameters like time-sensitivity and energy efficiency, computation offloading framework comes into play to meet these performance parameters and extreme computation requirements. Computation or data offloading tasks to nearby devices or the fog or cloud structure can aid in achieving the resource requirements of IoT applications. In this paper, the role of context or situation to perform the offloading is studied and drawn to a conclusion, that to meet the performance requirements of IoT enabled services, context-based offloading can play a crucial role. Some of the existing frameworks EMCO, MobiCOP-IoT, Autonomic Management Framework, CSOS, Fog Computing Framework, based on their novelty and optimum performance are taken for implementation analysis and compared with the MAUI, AnyRun Computing (ARC), AutoScaler, Edge computing and Context-Sensitive Model for Offloading System (CoSMOS) frameworks. Based on the study of drawn results and limitations of the existing frameworks, future directions under offloading scenarios are discussed. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"V., Nitin, Vikram; S., Asthana, Shubhi; B., Ray, Baishakhi; R., Krishna, Rahul","CARGO: AI-Guided Dependency Analysis for Migrating Monolithic Applications to Microservices Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146944265&partnerID=40&md5=a38f6cb6101b7ed0262e5eb8554da62e","Microservices Architecture (MSA) has become a de-facto standard for designing cloud-native enterprise applications due to its efficient infrastructure setup, service availability, elastic scalability, dependability, and better security. Existing (monolithic) systems must be decomposed into microservices to harness these characteristics. Since manual decomposition of large scale applications can be laborious and error-prone, AI-based systems to decompose applications are gaining popularity. However, the usefulness of these approaches is limited by the expressiveness of the program representation and their inability to model the application's dependency on critical external resources such as databases. Consequently, partitioning recommendations offered by current tools result in architectures that result in (a) distributed monoliths, and/or (b) force the use of (often criticized) distributed transactions. This work attempts to overcome these challenges by introducing CARGO (short for Context-sensitive lAbel pRopaGatiOn) - a novel un-/semi-supervised partition refinement technique that uses a context- and flow-sensitive system dependency graph of the monolithic application to refine and thereby enrich the partitioning quality of the current state-of-the-art algorithms. CARGO was used to augment four state-of-the-art microservice partitioning techniques (comprised of 1 industrial tool and 3 open-source projects). These were applied on five Java EE applications (comprised of 1 proprietary and 4 open source projects). Experiments show that CARGO is capable of improving the partition quality of all four partitioning techniques. Further, CARGO substantially reduces distributed transactions, and a real-world performance evaluation of a benchmark application (deployed under varying loads) shows that CARGO also lowers the overall the latency of the deployed microservice application by 11% and increases throughput by 120% on average. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"A., Das, Anirban; S., Chakraborty, Sandip; S., Chakraborty, Suchetana","Where do all my smart home data go? Context-aware data generation and forwarding for edge-based microservices over shared IoT infrastructure","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129245821&partnerID=40&md5=7b2b52bc6fb3f61979c2d0bf043d2479","With the explosion of the Internet of Things (IoT) devices, the advent of the edge computing paradigm, and the rise of intelligent applications for smart infrastructure surveillance, in-network data management is gaining a lot of research attention these days. The challenge lies in accommodating multiple application microservices with varying Quality of Service (QoS) requirements to share the sensing infrastructure for better resource utilization. In this work, we propose a novel data collection framework, CaDGen (Context-aware Data Generation) for such a shared IoT infrastructure that enables integrated data filtration and forwarding towards minimizing the resource consumption footprint for the IoT infrastructure. The proposed filtration mechanism utilizes the contextual information associated with the running application for determining the relevance of the data. Whereas the proposed forwarding policy aims to satisfy the diverse QoS requirements for the running applications by selecting the suitable next-hop forwarder based on the microservices distribution across different edge devices. A thorough performance evaluation of CaDGen through a testbed implementation as well as a simulation study for diverse setups reveals promising results concerning network resource utilization, scalability, energy conservation, and distribution of computation for optimal service provisioning. It is observed that the CaDGen can achieve nearly 35% reduction in the generated data for a moderately dynamic scenario without compromising on the data quality. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"M., Ezzeddine, Mazen; G., Migliorini, Gael; F., Baudé, Fran¸Coise; F., Huet, Fabrice","Cost-Efficient and Latency-Aware Event Consuming in Workload-Skewed Distributed Event Queues","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142100538&partnerID=40&md5=f4e78df9e8a37e4081471e09ed78fca9","Distributed event queues have emerged as a central component in building large scale cloud applications. In distributed event queues, guaranteeing a maximum event processing latency for high percentile of events in a cost-efficient manner is of paramount interest. This is achieved through efficient and accurate solutions to autoscale event consumers to meet the incoming workload. However, most of current solutions to autoscale event consumers are threshold-based that add/remove consumer replicas based on a metric of interest. These autoscalers lack an accurate estimation on the number of replicas that is just enough to keep up with the arrival rate of events and are not cost-efficient. Moreover, threshold-based autoscalers are not designed with workload-skewness in mind. When the workload is skewed few partitions of the distributed queue will receive higher percentile of the events produced. In such cases, the autoscale process must be complemented with a load-aware assignment of event consumer replicas to queue partitions. However, load-aware assignment is not performed by threshold-based autoscalers as they assume a uniform event load across the partitions of the queue. Hence, in this work, we first express the problem of cost-efficient scaling of event consumers to achieve a desired latency as a bin pack problem. This bin pack problem depends on the arrival rate of events, consumption rate of consumers, and on the events backlog in the queues. Next, we show that the process of scaling event consumers in face of skewed workload is performed by a controller/autoscaler and by one of the consumer replicas namely the leader. The controller monitors the cluster state and launches the appropriate number of consumer replicas. Next, the leader consumer performs a load-aware assignment of partitions to consumer replicas. In face of skewed workloads, observed results show order of magnitude gains in terms of latency guarantee as compared to an autoscale methodology that is not complemented by a load-aware assignment. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"V., Kjorveziroski, Vojdan; S., Filiposka, Sonja","Kubernetes distributions for the edge: serverless performance evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127088770&partnerID=40&md5=c89b71bd3fb12f70fa00beff4942472a","Serverless computing, especially when deployed at the edge of the network, is seen as an enabling technology for the future development of more complex Internet of Things systems. However, special care must be taken when deploying new edge infrastructures for serverless workloads in terms of resource usage and network connectivity. Inefficient utilization of the available computing resources might easily cancel out the benefits acquired by moving the equipment closer to the edge, namely the reduced communication latency. Containers, together with the Kubernetes container orchestrator, are used by many serverless platforms today. We evaluate the performance of three different Kubernetes distributions—full-fledged Kubernetes, K3s, and MicroK8s when deployed in a resource constrained environment at the edge. We use the OpenFaaS serverless platform and employ 14 different benchmarks divided into three separate categories to evaluate various aspects of the execution performance of the distributions. Four different test types are performed focusing on cold start latency, serial execution performance, parallel execution using a single replica, and parallel execution utilizing different autoscaling strategies. Our results show that the edge-oriented K3s and MicroK8s distributions offer better performance in the majority of the tests, while a full-fledged deployment exhibits noticeable advantages for sustained loads such as parallel function invocation using a single replica. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"C., Zhu, Changpeng; B., Han, Bo; Y., Zhao, Yinliang","A bi-metric autoscaling approach for n-tier web applications on kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121570754&partnerID=40&md5=4cfbe9b578bb9f928fcc132700a20d99","Container-based virtualization techniques are becoming an alternative to traditional virtual machines, due to less overhead and better scaling. As one of the most widely used open-source container orchestration systems, Kubernetes provides a built-in mechanism, that is, horizontal pod autoscaler (HPA), for dynamic resource provisioning. By default, scaling pods only based on CPU utilization, a single performance metric, HPA may create more pods than actually needed. Through extensive measurements of a containerized n-tier application benchmark, RUBBoS, we find that excessive pods consume more CPU and memory and even deteriorate response times of applications, due to interference. Furthermore, a Kubernetes service does not balance incoming requests among old pods and new pods created by HPA, due to stateful HTTP. In this paper, we propose a bi-metric approach to scaling pods by taking into account both CPU utilization and utilization of a thread pool, which is a kind of important soft resource in Httpd and Tomcat. Our approach collects the utilization of CPU and memory of pods. Meanwhile, it makes use of ELBA, a milli-bottleneck detector, to calculate queue lengths of Httpd and Tomcat pods and then evaluate the utilization of their thread pools. Based on the utilization of both CPU and thread pools, our approach could scale up less replicas of Httpd and Tomcat pods, contributing to a reduction of hardware resource utilization. At the same time, our approach leverages preStop hook along with liveness and readiness probes to relieve load imbalance among old Tomcat pods and new ones. Based on the containerized RUBBoS, our experimental results show that the proposed approach could not only reduce the usage of CPU and memory by as much as 14% and 24% when compared with HPA, but also relieve the load imbalance to reduce average response time of requests by as much as 80%. Our approach also demonstrates that it is better to scale pods by multiple metrics rather than a single one. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"A.R., Lakhan, Abdullah Raza; M.S., Memon, Muhammad Suleman; Q.U.A., Mastoi, Qurat Ul Ain; M., Elhoseny, Mohamed; M.A., Mohammed, Mazin Abed; M., Qabulio, Mumtaz; M., Abdel-Basset, Mohamed","Cost-efficient mobility offloading and task scheduling for microservices IoVT applications in container-based fog cloud network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108253802&partnerID=40&md5=42a5b78c174147bc496a227862e6df03","These days, the usage of the internet of Vehicle Things (IVoT) applications such as E-Business, E-Train, E-Ambulance has been growing progressively. These applications require mobility-aware delay-sensitive services to execute their tasks. With this motivation, the study has the following contribution. Initially, the study devises a novel cooperative vehicular fog cloud network (VFCN) based on container microservices which offers cost-efficient and mobility-aware services with rich resources for processing. This study devises the cost-efficient task offloading and scheduling (CEMOTS) algorithm framework, which consists of the mobility aware task offloading phase (MTOP) method, which determines the optimal offloading time to minimize the communication cost of applications. Furthermore, CEMOTS offers Cooperative Task Offloading Scheduling (CTOS), including task sequencing and scheduling. The goal is to reduce the application costs of communication cost and computational costs under a given deadline constraint. Performance evaluation shows the CTOS and MTOP outperform existing task offloading and scheduling methods in the VCFN in terms of costs and the deadline for IoT applications. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"N.M., Dang-Quang, Nhat Minh; M., Yoo, Myungsik","An Efficient Multivariate Autoscaling Framework Using Bi-LSTM for Cloud Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128185483&partnerID=40&md5=88ea2e24f3f63feea0909cd9c9d045da","With the rapid development of 5G technology, the need for a flexible and scalable realtime system for data processing has become increasingly important. By predicting future resource workloads, cloud service providers can automatically provision and deprovision user resources for the system beforehand, to meet service level agreements. However, workload demands fluctuate continuously over time, which makes their prediction difficult. Hence, several studies have proposed a technique called time series forecasting to accurately predict the resource workload. However, most of these studies focused solely on univariate time series forecasting; in other words, they only analyzed the measurement of a single feature. This study proposes an efficient multivariate autoscaling framework using bidirectional long short-term memory (Bi-LSTM) for cloud computing. The system framework was designed based on the monitor–analyze–plan–execute loop. The results obtained from our experiments on different actual workload datasets indicated that the proposed multivariate Bi-LSTM exhibited a root-mean-squared error (RMSE) prediction error 1.84-times smaller than that of the univariate one. Furthermore, it reduced the RMSE prediction error by 6.7% and 5.4% when compared with the multivariate LSTM and convolutional neural network-long shortterm memory (CNN-LSTM) models, respectively. Finally, in terms of resource provisioning, the multivariate Bi-LSTM autoscaler was 47.2% and 14.7% more efficient than the multivariate LSTM and CNN-LSTM autoscalers, respectively. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"G., Ortiz, Guadalupe; J., Boubeta-Puig, Juan; J., Criado, Javier; D., Corral-Plaza, David; A., Garcia-De-Prado, Alfonso; I., Medina-Bulo, Inmaculada; L., Iribarne, L.","A microservice architecture for real-time IoT data processing: A reusable Web of things approach for smart ports","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120506286&partnerID=40&md5=ccd67caab315a73d803a0b33e770e5da","Major advances in telecommunications and the Internet of Things have given rise to numerous smart city scenarios in which smart services are provided. What was once a dream for the future has now become reality. However, the need to provide these smart services quickly, efficiently, in an interoperable manner and in real time is a cutting-edge technological challenge. Although some software architectures offer solutions in this area, these are often limited in terms of reusability and maintenance by independent modules —involving the need for system downtime when maintaining or evolving, as well as by a lack of standards in terms of the interoperability of their interface. In this paper, we propose a fully reusable microservice architecture, standardized through the use of the Web of things paradigm, and with high efficiency in real-time data processing, supported by complex event processing techniques. To illustrate the proposal, we present a fully reusable implementation of the microservices necessary for the deployment of the architecture in the field of air quality monitoring and alerting in smart ports. The performance evaluation of this architecture shows excellent results. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"I., Tzanettis, Ioannis; C.M., Androna, Christina Maria; A., Zafeiropoulos, Anastasios; E., Fotopoulou, Eleni; S., Papavassiliou, Symeon","Data Fusion of Observability Signals for Assisting Orchestration of Distributed Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125945981&partnerID=40&md5=ccb53a79e4eb4ab66d4f4f503ae33c2a","Nowadays, various frameworks are emerging for supporting distributed tracing techniques over microservices-based distributed applications. The objective is to improve observability and management of operational problems of distributed applications, considering bottlenecks in terms of high latencies in the interaction among the deployed microservices. However, such frameworks provide information that is disjoint from the management information that is usually collected by cloud computing orchestration platforms. There is a need to improve observability by combining such information to easily produce insights related to performance issues and to realize root cause analyses to tackle them. In this paper, we provide a modern observability approach and pilot implementation for tackling data fusion aspects in edge and cloud computing orchestration platforms. We consider the integration of signals made available by various open-source monitoring and observability frameworks, including metrics, logs and distributed tracing mechanisms. The approach is validated in an experimental orchestration environment based on the deployment and stress testing of a proof-of-concept microservices-based application. Helpful results are produced regarding the identification of the main causes of latencies in the various application parts and the better understanding of the behavior of the application under different stressing conditions. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"I.D.S., Fe, Iure De Sousa; R.D.S., Matos, Rubens De S.; J.R., Dantas, Jamilson Ramalho; C., Melo, Carlos; T.A., Nguyen, Tuan Anh; D., Min, Dugki; E., Choi, Eunmi; F.A., Silva, Francisco Airton; P.R.M., MacIel, Paulo Romero Martins","Performance-Cost Trade-Off in Auto-Scaling Mechanisms for Cloud Computing†","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123924638&partnerID=40&md5=aa341669e3bc12e23049e82edd97cdc3","Cloud computing has been widely adopted over the years by practitioners and companies with a variety of requirements. With a strong economic appeal, cloud computing makes possible the idea of computing as a utility, in which computing resources can be consumed and paid for with the same convenience as electricity. One of the main characteristics of cloud as a service is elasticity supported by auto-scaling capabilities. The auto-scaling cloud mechanism allows adjusting resources to meet multiple demands dynamically. The elasticity service is best represented in critical web trading and transaction systems that must satisfy a certain service level agreement (SLA), such as maximum response time limits for different types of inbound requests. Nevertheless, existing cloud infrastructures maintained by different cloud enterprises often offer different cloud service costs for equivalent SLAs upon several factors. The factors might be contract types, VM types, auto-scaling configuration parameters, and incoming workload demand. Identifying a combination of parameters that results in SLA compliance directly in the system is often sophisticated, while the manual analysis is prone to errors due to the huge number of possibilities. This paper proposes the modeling of auto-scaling mechanisms in a typical cloud infrastructure using a stochastic Petri net (SPN) and the employment of a well-established adaptive search metaheuristic (GRASP) to discover critical trade-offs between performance and cost in cloud services.The proposed SPN models enable cloud designers to estimate the metrics of cloud services in accordance with each required SLA such as the best configuration, cost, system response time, and throughput.The auto-scaling SPN model was extensively validated with 95% confidence against a real test-bed scenario with 18.000 samples. A case-study of cloud services was used to investigate the viability of this method and to evaluate the adoptability of the proposed auto-scaling model in practice. On the other hand, the proposed optimization algorithm enables the identification of economic system configuration and parameterization to satisfy required SLA and budget constraints. The adoption of the metaheuristic GRASP approach and the modeling of auto-scaling mechanisms in this work can help search for the optimized-quality solution and operational management for cloud services in practice. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"H., Ma, Hongjun; M., Zhu, Min; D., Qi, Desheng; J., Yang, Jifang; R., Pan, Rouxian","A Cloud-Native-Oriented AI Epidemic Four-Color Situation Decision Support System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175960106&partnerID=40&md5=0800bce90fe2090fc94df2cd3951dbdf","Aiming at the prevention and control of epidemic diseases in the community, a four-color situation decision support system for cloud native is proposed, Through a single situation prediction algorithm and comprehensive situation prediction, the cloud native AI epidemic prevention and control four-color situation decision support system is realized. Taking cloud native as the technical base, based on the environment of microservice+Kubernetes+container, the system first adopts a two category learner to solve the multi category problem model, Finally, the performance evaluation of the trained model on the test set has achieved an ideal result; At the same time, based on the stack generalization model, the integration of four prediction models is realized, including the aggregation degree of high-risk personnel, regional mobility, the aggregation degree of regional key sites, and the epidemic prevention and control support capability. In order to control the spread of the epidemic, we have predicted how the virus will spread based on environmental conditions in a variety of ways, and optimized the allocation of medical resources. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"X., Li, Xiaoling; T., Zeng, Tao; B., Liu, Biyong; H., Hu, Haichuan; Z., Xu, Zichen; S., Tan, Shuang; Y., Tan, Yusong; C., Xu, Chenren","Nonintrusive Measurement on Temporal and Spatial Features of Microservice Inferences","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168122070&partnerID=40&md5=ca8f1db20aab6f7badff11eccdef54da","The high flexibility of microservice architecture provides notable divergence among the internal software stack within the same application. Microservice-based applications are commonly deployed in data centers by users yet they have no clue of what is exactly provided by the service provider. In this case, there could exist those providers could be able to replace the internal software without noticing, making a contract-level fault and offloading risks to users. To better profile the microservice, we propose a framework that provides nonintrusive measurements on microservice inference, or MEME, to detect fraudulent behaviors of cloud service providers on microservice-based applications. We design MEME using performance portrait and fast Fourier transform (FFT). We model the microservice-based application with a set of cohorts and use FFT to obtain the signal formed by the main frequency components of average response time. Our model represents the performance portrait of the microservice-based application. The performance portrait is similar to a fingerprint that carries internal software information. In our experiments, we take a two-tier microservice-based application containing databases as an example. Empirical results show that MEME can provide a distinguishable profile of the performance portrait of data services in a temporal and spatial manner, which allows us to identify the software type. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Yao, Zhiyuan; Y., Desmouceaux, Yoann; J.A., Cordero-Fuertes, Juan Antonio; M., Townsley, Mark; T.H., Clausen, Thomas Heide","Efficient Data-Driven Network Functions","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149995100&partnerID=40&md5=ee9d1e43953e66c3d5ffd2370345996a","Cloud environments require dynamic and adaptive networking policies. It is preferred to use heuristics over advanced learning algorithms in Virtual Network Functions (VNFs) in production because of high-performance constraints. This paper proposes Aquarius to passively yet efficiently gather observations and enable the use of machine learning to collect, infer, and supply accurate networking state information - without incurring additional signaling and management overhead. This paper illustrates the use of Aquarius with a traffic classifier, an auto-scaling system, and a load balancer - and demonstrates the use of three different machine learning paradigms - unsupervised, supervised, and reinforcement learning, within Aquarius, for inferring network state. Testbed evaluations show that Aquarius increases network state visibility and brings notable performance gains with low overhead. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"D.H., Hussein, Dana Haj; M.A., Ibnkahla, Mohamed A.","An IoT Traffic Modeling Framework and its Application to Autonomous Edge Scaling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146917475&partnerID=40&md5=33670dc618a0fa9ba686bcf5f9d5b309","Future wireless networks will exhibit heterogeneity of traffic generating sources originated by numerous Internet of Things (IoT) nodes as well as traditional mobile phones. Moreover, the space of novel IoT services is expanding the simple monitoring tasks of IoT nodes to more complex services in which a node can be in a monitoring state and transition autonomously to an alarm state when predefined conditions are detected. The complexity of the envisioned future wireless networks is indeed new to the community with challenges affecting many aspects such as protocol design and network operation mechanisms. Traffic modeling lies at the core of these issues. As the advancement of technologies continues, faithful performance evaluation measures are dependent on the underlying traffic model. In this scope, we propose a Tiered Markov Modulated Poisson Process (TMMPP) that is capable of capturing IoT traffic characteristics, e.g. patterns and seasonality, which occur in long time spans, e.g days, with the flexibility of modeling different IoT service behaviors. Moreover, we study an autonomous edge scaling mechanism as a use case illustrating the benefits of the proposed TMMPP traffic model. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"S., Bhandari, Sujit; M., Patrou, Maria; N., Chahal, Nancy; P., Patros, Panos; K.B., Kent, Kenneth Blair; J.S., Siu, Joran S.; M.H., Dawson, Michael H.","Supervisory Event Loop-based Autoscaling of Node.js Deployments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146436709&partnerID=40&md5=ddb750b3b0b1c1749f14af644390fe73","Autoscaling mechanisms are used widely to scale computing instances, under varying load conditions. Containerized cloud applications managed by orchestrators-such as Kubernetes with the Horizontal Pod Autoscaler (HPA)-can be scaled based on CPU utilization. However, the prevalent approach may not be ideal for every language runtime, such as the event-driven Node.js. Language runtime-specific metrics can be utilized to accurately describe the state of the application and use it to monitor and affect the application scalability in control loop-based systems at the desired conditions dynamically. Hence, we investigate event loop lag as an alternative and Node.js-specific metric to drive autoscaling with HPA controllers. Additionally, we synthesize a three-Tier adaptive mechanism on top of the cluster autoscaler, which acts as a supervisory controller, to change the setpoint value dynamically based on the divergence from the service level objectives. We further extend the aforementioned architecture to re-evaluate the system model and the controller gain parameters at runtime. To assess the methodology, we evaluate and compare the performance of our adaptive autoscaling approach against the CPU-utilization-based autoscaling mechanism, under various load patterns and workloads. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"L.D.S.B., Weerasinghe, Lakshan D.S.B.; I., Perera, Indika","Evaluating the Inter-Service Communication on Microservice Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146432388&partnerID=40&md5=a37f588673bc1b3134ed3a260ee5c2da","Distributed computing concepts have proliferated with cloud computing concepts in the past decade. With the evolution in cloud computing, Microservice architecture has significantly become popular as a new architectural pattern and a software development architecture. Most enterprise software development has moved their monolithic software architecture to microservice-based architecture, as it can divide large applications into light-weighted, distributed components. However, this approach may be subject to certain downsides as well. With the modern convention, engineers have succeeded in achieving scalability and maintainability quality attributes and lack the performance attribute in terms of response time. This is because the microservice architecture has introduced inter-service communication over the network. The key challenge when developing a microservice-based application is choosing the correct inter-service communication mechanism to reduce the time taken when calling each service. This research has taken an experimental methodology to compare and contrast the most trending inter-service communication mechanisms. Industry-standard benchmark load test is run to collect quantitative data to evaluate the overall system performance in terms of response time. The testing observed that gRPC protocol performs well in terms of response time and throughput compared to the HTTP and Web Socket protocols. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"F.Z.A., Mudrikah, F. Zahra Aqilah; Istikmal; B., Aditya, Bagus","Design of a Geographic Information System for Forest and Land Fires Based on a Real-Time Database on Microservices Infrastructure","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145976652&partnerID=40&md5=3f264e41c95f402288621ea608fd6a9f","Forest and land fires are an increasingly common problem in Indonesia. Fire cases that often occur require a system that is able to detect fires and provide information to users remotely to reduce the impact of fires. Along with the development of hardware technology such as computers, the use of GIS seems to be an effective shortcut in analyzing an event. Kubernetes is an open source platform for managing containerized application workloads, offering declarative configuration and automation. This research designs a Google Maps API System tool for forest and land fires using a real-time database on microservices infrastructure with outputs in the form of fire locations and the results of sensor readings used. Broadly speaking, the processes that occur in the design of the location of forest fire points will be detected by sensors. Then firebase will store forest fire data which will simultaneously be updated on the website. Clients can see the point of forest fires through a browser on their respective desktops. Based on the results of the performance tests that have been carried out, it can be concluded that the use of Kubernetes microclusters can provide advantages when compared to those built monolithically, because Kubernetes microclusters have several advantages, namely having the Horizontal Pod Autoscaler feature, and the Kubernetes microcluster manages components and related services well. Then for each test performed, there was no significant change in memory usage. In the analysis of the results of the comparison data with 7 tests that have been carried out there are 6 tests which mean that the service built with the Kubernetes microcluster is superior to the monolithic one, namely hits per second 2354 ms, latency 3599 ms, response code 720 success code, cpu utilization 13.84%, error rate error rate 0.00%, and throughput 112/sec. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"C., Canali, Claudia; R., Lancellotti, Riccardo; P., Pedroni, Pietro","Microservice Performance in Container- and Function-as-a-Service Architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141613978&partnerID=40&md5=91fb9a62a06643f7f7bb2a21319189ff","Function-as-a-Service (FaaS) is a new cloud-based computing model that promises a more cost-efficient deployment of microservices with respect to other cloud paradigms, like Container-as-a-Service (CaaS). However, requests served under a FaaS approach often experience a cold start condition, that occurs when the execution of an inactive function occurs for the first time and a container environment has to be set up afresh. In such cases, performance deteriorates and response times increase. This paper proposes an analysis of the performance of the Function-as-a-Service model for two single offered microservices. Specifically, we carry out a performance evaluation of the Function-as-a-Service model, implemented through OpenWhisk, using as a baseline for comparison the Container-as-a-Service approach, implemented with Docker. Our analysis focuses on metrics related to the response time and to the usage of main server resources such as CPU and memory. For the performance comparison, we exploited two different microservices based on face recognition and image conversion, respectively, in order to evaluate the performance over popular and modern kinds of services included in artificial intelligence and multimedia applications. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"R., Lancellotti, Riccardo; S., Rossi, Stefano; G.C., Miano, Giuseppe Calogero; F., Miselli, Fabio","Performance Comparison of Technological Solutions for Spark Applications in AWS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141594858&partnerID=40&md5=34b6934afe732216292b68552c116804","Cloud computing is providing a pay-as-you-go in-frastructure for the deployment of complex applications, with auto-scaling support and the ability to manage and process huge amount of data. However, due to the underlying complexity of the cloud infrastructure, it is not trivial to evaluate the setup providing the best performance of such scenario. To this aim the present paper proposes a thorough performance evaluation of a real application in a Cloud platform, measuring the impact of several design choices and technological solution. The experimental results, based on a real application and on realistic data can provide a significant insight that can integrate the traditional approach of cloud performance evaluation based on synthetic benchmarks. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"G.L., Stavrinides, Georgios L.; H.D., Karatza, H. D.","The Effect of Laxity of Real-Time Workflow Applications on the Performance of Elastic Cloud Resources","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141541675&partnerID=40&md5=fda72eea1836bd8758eb42f2fc019afc","An increasing number of workflow applications with varying deadlines are processed on cloud platforms. One of the main characteristics of such environments is the elasticity of the virtualized resources, which allows them to be dynamically scaled in or out. Consequently, it is imperative to examine in this context how the laxity (slack time) of the workload affects performance. To this end, in this paper we investigate the effect of laxity of real-Time workflow jobs on the performance of elastic cloud resources. We formulate the problem and evaluate through simulation the impact of laxity on the deadline miss ratio, energy consumption and monetary cost associated with the resources, considering workloads with different slack times. The simulation results provide useful insights into how each performance metric is affected by the laxity of the real-Time workflow jobs. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"Q., Yunyun, Qiu; P., Chen, Pingping; D., Tan, Dingying","Research on Elastic Cloud Resource Management Strategies based on Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141364913&partnerID=40&md5=bfa8b530df48a50eb3d0d80082bb1016","Kubernetes' native horizontal scaling strategy, horizontal pod autoscaler(HPA), suffers from a single monitoring metric that does not accurately measure workload. In addition, there are response delays in the expansion stage, and the deletion of redundant Pods in the capacity reduction phase may adversely affect load balancing. In order to improve the efficiency of computing resource usage, this paper designs an automatic horizontal scaling system architecture based on the Monitor-Analyse-Planning-Execution (MAPE) loop to address the above problems. The system also equips the horizontal scaling system with the resource deletion strategy RRS, which enables the system to handle bursty workloads better. The main contribution of this paper is to propose a passive strategy based on weighted metric thresholds and an active strategy based on ARIMA prediction. The results of multiple experiments show that the ARIMA-prediction-based strategy has the problem of decreasing prediction accuracy in the late stage of the experiment, demonstrating the limitations of the ARIMA model in the real-time scaling system. The weighted-metric-threshold-based strategy outperforms HPA in that the timing, number, and replica variation of Pod scaling better matches workload variation and improves response latency. This paper provides a reference for researchers, developers, and others using Kubernetes to optimize the performance of HPA. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Liang, Zheheng; J., Zeng, Jijun; J., Zhang, Jinbo; G., Zhu, Gongfeng","Discussion on Operation and Maintenance Construction of Automation System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140910446&partnerID=40&md5=ccf04997394ce036c9f7a6aa7e75f0e8","With the rapid development of the Internet era, more and more companies choose to use the micro-service architecture as an operation and maintenance method in order to maintain system stability while maintaining a certain operating efficiency. Difficulty in operation and maintenance. The microservice architecture splits the original single architecture into different services, and the log information between each module is no longer continuous and failures occur. Sometimes the on-site information collection is insufficient, the error location is more and more complicated, and the location of the system performance bottleneck is also difficult due to the interaction of multiple services. This article discusses the construction of automation system operation and maintenance. Based on some documents on automation operation and maintenance, summarizes the functional modules of automation system operation and maintenance, and pave the way for the design of automated operation and maintenance system below. First, the overall architecture of the system is proposed. Figure, on the basis of the design of some small modules, and finally the design of the system in this paper is tested, compared with the manual operation and maintenance, the system operation and maintenance time comparison. The release time after the system continues to deliver the module application is 0.5 times that of manual operation and maintenance, which greatly improves the operation and maintenance efficiency and guarantees the stability of the system. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"P., Zuk, Pawel; B., Przybylski, Bartłomiej; K., Rza̧dca, Krzysztof","Call Scheduling to Reduce Response Time of a FaaS System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140896922&partnerID=40&md5=b14e0db77e926fd3e50a41b5d58d2f22","In an overloaded FaaS cluster, individual worker nodes strain under lengthening queues of requests. Although the cluster might be eventually horizontally-scaled, adding a new node takes dozens of seconds. As serving applications are tuned for tail serving latencies, and these greatly increase under heavier loads, the current workaround is resource over-provisioning. In fact, even though a service can withstand a steady load of, e.g., 70% CPU utilization, the autoscaler is triggered at, e.g., 30-40% (thus the service uses twice as many nodes as it would be needed). We propose an alternative: a worker-level method handling heavy load without increasing the number of nodes. FaaS executions are not interactive, compared to, e.g., text editors: end-users do not benefit from the CPU allocated to processes often, yet for short periods. Inspired by scheduling methods for High Performance Computing, we take a radical step of replacing the classic OS preemption by (1) queuing requests based on their historical characteristics; (2) once a request is being processed, setting its CPU limit to exactly one core (with no CPU oversubscription). We extend OpenWhisk and measure the efficiency of the proposed solutions using the SeBS benchmark. In a loaded system, our method decreases the average response time by a factor of 4. The improvement is even higher for shorter requests, as the average stretch is decreased by a factor of 18. This leads us to show that we can provide better response-time statistics with 3 machines compared to a 4-machine baseline. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"F., Minna, Francesco; F., Massacci, Fabio; K., Tuma, Katja","Towards a Security Stress-Test for Cloud Configurations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137604317&partnerID=40&md5=11fdb64ca8663e9a4e8df43aa615779e","Securing cloud configurations is an elusive task, which is left up to system administrators who have to base their decisions on ""trial and error""experimentations or by observing good practices (e.g., CIS Benchmarks). We propose a knowledge, AND/OR, graphs approach to model cloud deployment security objects and vulnerabilities. In this way, we can capture relationships between configurations, permissions (e.g., CAP_SYS_ADMIN), and security profiles (e.g., AppArmor and SecComp). Such an approach allows us to suggest alternative and safer configurations, support administrators in the study of what-if scenarios, and scale the analysis to large scale deployments. We present an initial validation and illustrate the approach with three real vulnerabilities from known sources. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"K., Alanezi, Khaled; S., Mishra, Shivakant","Utilizing Microservices Architecture for Enhanced Service Sharing in IoT Edge Environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137584436&partnerID=40&md5=46ef3a0db1567d51b5b8d68581974f0b","Latency sensitive IoT (Internet of Things) applications at the edge are designed using a microservice-based architecture. This architecture is comprised of a set of microservices, each implementing a simple functionality with clearly-defined interfaces, and applications are constructed by selecting and interconnecting appropriate microservices. To understand the performance implications of using a microservice-based architecture for constructing IoT applications at the edge, this paper provides a detailed evaluation based on an actual prototpye implementation and performance measurement. In our setup, an edge server fulfills dual roles of being an administrative controller of the IoT infrastructure and satisfying application's latency and privacy constraints. We demonstrate the utility of this architecture by isolated and independent implementation of different microservices, constructing an IoT application by interconnecting these microservices, and potential sharing of microservices between different IoT applications running simultaneously to enhance interoperability. Finally, we provide an extensive performance evaluation focusing on application latency as well as CPU and memory consumption. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"M., Amaral, Marcelo; T., Chiba, Tatsuhiro; S., Trent, Scott; T., Yoshimura, Takeshi; S., Choochotkaew, Sunyanan","MicroLens: A Performance Analysis Framework for Microservices Using Hidden Metrics With BPF","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137572673&partnerID=40&md5=bd0d0039737db0e7b2dfffb88615eb25","Determining the root cause of performance regression for microservices is challenging. The topological cascading performance implications among microservices hide the source of the problem. Additionally, the lack of knowledge about application phases can potentially lead to false-positive critical service detection. Service resource utilization is an imperfect proxy for application performance, potentially leading to false positives. Therefore, in this work, we propose a new performance testing framework that leverages hidden Berkeley Packet Filter (BPF) kernel metrics to locate root causes of performance regression. The framework applies a systematic multi-level approach to analyze microservice performance without intrusive code instrumentation. First, the framework constructs an attributed graph with microservice requests, scores the services to identify the critical paths, and ranks the low-level metrics to highlight the root cause of performance regression. Through judiciously designed experiments, we evaluated the metric collection overhead, showing less than 18% more latency when the application is running across hosts and 9% within the same host. In addition, depending on the application, no overhead is experienced, while the state-of-the-art approach presented up to 1060% more latency. The microservice benchmark evaluation shows that MicroLens can successfully identify the set of root causes and that the causes vary when the application is running in different infrastructures. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"S., Choochotkaew, Sunyanan; T., Chiba, Tatsuhiro; S., Trent, Scott; T., Yoshimura, Takeshi; M., Amaral, Marcelo","AutoDECK: Automated Declarative Performance Evaluation and Tuning Framework on Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137551925&partnerID=40&md5=c4a8e67b353c5d566f44fa4cc7d5529f","Containerization and application variety bring many challenges in automating evaluations for performance tuning and comparison among infrastructure choices. Due to the tightly-coupled design of benchmarks and evaluation tools, the present automated tools on Kubernetes are limited to trivial microbenchmarks and cannot be extended to complex cloudnative architectures such as microservices and serverless, which are usually managed by customized operators for setting up workload dependencies. In this paper, we propose AutoDECK, a performance evaluation framework with a fully declarative manner. The proposed framework automates configuring, deploying, evaluating, summarizing, and visualizing the benchmarking workload. It seamlessly integrates mature Kubernetes-native systems and extends multiple functionalities such as tracking the image-build pipeline, and auto-tuning. We present five use cases of evaluations and analysis through various kinds of bench-marks including microbenchmarks and HPC/AI benchmarks. The evaluation results can also differentiate characteristics such as resource usage behavior and parallelism effectiveness between different clusters. Furthermore, the results demonstrate the benefit of integrating an auto-tuning feature in the proposed framework, as shown by the 10% transferred memory bytes in the Sysbench benchmark. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"B., Xu, Beibei; Y., Zhao, Yanqing; V.O., Kuzminykh, Valeriy O.; S., Zhu, Shiwei; J., Yu, Junfeng; M., Zhang, Mingjun; S., Li, Sisi","Research on the Evaluation System of International S&T Cooperation Based on Microservice Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137060369&partnerID=40&md5=24425614d4366fd95598ee8ed33bec71","The development of the world has benefited from advances in science and technology, and the destiny of mankind has become closer due to scientific and technological cooperation. International scientific and technological innovation cooperation is one of the important indicators to measure the potential and technological innovation of a country or region. Scientific evaluation and performance evaluation of international scientific and technological cooperation have become important for effectively improving the management level of international scientific and technological cooperation projects and promoting scientific and technological output. Means, the construction of a scientific cooperation evaluation and performance evaluation system has become a realistic demand for promoting international scientific and technological cooperation and strengthening performance management of international scientific and technological cooperation in the new era. Based on the analysis of the data sources, data structure, index evaluation system and system functions of the international scientific and technological cooperation evaluation system, the article proposes the system logic and hierarchical structure under the microservice architecture, and designs and implements the international scientific and technological cooperation evaluation system based on the microservice architecture. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"X., Li, Xue; P., Kang, Peng; J., Molone, Jordan; W., Wang, Wei; P., Lama, Palden","KneeScale: Efficient Resource Scaling for Serverless Computing at the Edge","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135736319&partnerID=40&md5=1e9224722e12678a99e7a43df03a2577","Serverless computing is a promising paradigm for delivering services to the Internet of Things (IoT) applications at the edge of the network. Its event-triggered computation, as well as fine-grained and agile resource scaling, is well-suited for a resource-constrained edge computing environment. However, general-purpose auto-scalers that are predominant in the cloud settings perform poorly for serverless computing at the Edge. This is mainly due to the difficulty in quickly determining the optimal resource allocation under resource-budget constraints and dynamic workloads. In this paper, we present an adaptive auto-scaler, KneeScale, that dynamically adjusts the number of replicas for serverless functions to reach a point at which the relative cost to increase resource allocation is no longer worth the corresponding performance benefit. We have designed and implemented KneeScale as lightweight system software that utilizes Kubernetes for resource management. Experimental results with a function-as-a-service (FaaS) benchmark, FunetionBeneh, and an open-source serverless computing platform, OpenFaaS, demonstrate the superior performance and resource efficiency of KneeScale. It outperforms Kubernetes Horizontal Pod AutoScaler (HPA) and OpenFaaS built-in scheduler in terms of cumulative performance under a given resource budget by up to 32 % and 106 % respectively. KneeScale achieves higher cumulative throughput than both competing techniques, lower latencies than OpenFaaS built-in scheduler, and similar latencies compared to HPA for a variety of serverless functions. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"N., Bartelucci, Nicolo; P., Bellavista, Paolo; T.W., Pusztai, Thomas W.; A., Morichetta, Andrea; S., Dustdar, Schahram","High-Level Metrics for Service Level Objective-Aware Autoscaling in Polaris: A Performance Evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134054117&partnerID=40&md5=44c783938d758088807b231ac9f6e0a1","With the increasing complexity, requirements, and variability of cloud services, it is not always easy to find the right static/dynamic thresholds for the optimal configuration of low-level metrics for autoscaling resource management decisions. A Service Level Objective (SLO) is a high-level commitment to maintaining a specific state of a service in a given period, within a Service Level Agreement (SLA): The goal is to respect a given metric, like uptime or response time within given time or accuracy constraints. In this paper, we show the advantages and present the progress of an original SLO-Aware autoscaler for the Polaris framework. In addition, the paper contributes to the literature in the field by proposing novel experimental results comparing the Polaris autoscaling performance, based on highlevel latency SLO, and the performance of a low-level average CPU-based SLO, implemented by the Kubernetes Horizontal Pod Autoscaler. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"G.R., Gunnam, Ganesh Reddy; D., Inupakutika, Devasena; R., Mundlamuri, Rahul; S., Kaghyan, Sahak; D.A., Akopian, David A.","Chatbot Integrated with Machine Learning Deployed in the Cloud and Performance Evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132362344&partnerID=40&md5=4af1e40ac7f0c9594666f5e10b5841d1","Recently human-machine digital assistants gained popularity and commonly used in question-and-answer applications and similar consumer-supporting domains. A class of more sophisticated digital assistants employing longer dialogs follow the trend, and there are several commercial platforms supporting their prototyping such as Google DialogFlow, Manychat, Chatfuel, Amazon Lex, etc. This paper explores cloud deployment of chatbots systems and their performance assessment methodologies. The performance measures includes system response delays and natural language processing capabilities. A case study platform supporting so-called deep-logic chatbots with long cycling capabilities is implemented and used for the assessment. To enable human-like conversations with a chatbot, huge training data, complex natural language understanding models are required and need to be adjusted and trained continuously. We explore implementation formats supporting auto scaling, and uninterrupted availability. In particular, we employ an architecture consisting of separate dialog management, authentication, and Natural Language Understanding (NLU) services. Finally, we present a performance evaluation of such loosely coupled chatbot system. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"A., Adel, Ahmed; A.H., El-Mougy, Amr H.","Cloud Computing Predictive Resource Management Framework Using Hidden Markov Model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130472752&partnerID=40&md5=5810988e977bc56c68d93bfd7e2f5df3","Volunteer and cloud computing are heterogeneous environments that aggregate the capabilities of their resources to solve large scale computationally-intensive problems and provide various services to users. Due to the dynamic nature of these environments, performance states of resources rapidly change, making elasticity characteristic and task allocation very challenging aspects. In order to implement a scalable elastic mechanism while utilizing the resources efficiently and maintaining the overall balance of these systems, real-time performance data need to be collected periodically. However, data collection may significantly increase the communication overhead in the cloud and volunteer network and consume from the limited processing power, energy and bandwidth of resources. Accordingly, this paper proposes solutions for balancing the load while reducing the communication overhead. A reactive and proactive resource auto-scaling task allocation algorithms are proposed. The proactive auto-scaling algorithm is based on the Hidden Markov Model (HMM). Performance evaluation using computer simulations show that the proposed algorithm achieves high prediction accuracy, enhances the overall system utilization and significantly decreases the communication overhead. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"B., Lemoine, Benoit","Energy Efficiency of N:1 Protection Setups with Kubernetes Horizontal Pod Autoscaler","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129572202&partnerID=40&md5=68e216d31d2bdaba1bbaaaeb1e987fd0","Kubernetes Horizontal Pod Autoscaler (HPA) permits to enhance the Power Usage Effectiveness of Data Centers. However, critical workflows like the 5G User Plane elements require some Nominal:Backup Protection schemes that are not well adapted to Kubernetes HPA. We propose an Energy Efficiency framework for comparing two different ways to setup an Horizontal Pod Autoscaler for a N:1 protected Virtual Network Function. We provide an analytic formulation, decision curves for engineers, as well as experimental results. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"L.J.A.D., Costa, Leandro José Abreu Dias; A.N., Ribeiro, António Nestor","Performance Evaluation of Microservices Featuring Different Implementation Patterns","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127655427&partnerID=40&md5=a8e351dd076d60092ed014bc9dd471f8","The process of migrating from a monolithic to a microservices based architecture is currently described as a form of modernizing applications. The core principles of microservices, which mostly reside in achieving loose coupling between the services, highly depend on the implementation approaches used. Being microservices a complete change of paradigm that contrasts with the traditional way of developing software, the current lack of established principles often results in implementations that conflict with its alleged benefits. Given its distributed nature, performance is affected, but specific implementation patterns can further impact it. This paper aims to address the impact that microservices-based solutions, featuring different implementation patterns, have on performance and how it compares with monolithic applications. To do so, benchmarks are conducted over one application developed following a traditional monolithic approach, and two equivalent microservices-based implementations featuring distinct inter-service communication mechanisms and data management methodologies. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"G.J., Blinowski, Grzegorz J.; A., Ojdowska, Anna; A., Przybyłek, Adam","Monolithic vs. Microservice Architecture: A Performance and Scalability Evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125293717&partnerID=40&md5=b2b7d7c3dafdfd7eeee11c622d47355d","Context. Since its proclamation in 2012, microservices-based architecture has gained widespread popularity due to its advantages, such as improved availability, fault tolerance, and horizontal scalability, as well as greater software development agility. Motivation. Yet, refactoring a monolith to microservices by smaller businesses and expecting that the migration will bring benefits similar to those reported by top global companies, such as Netflix, Amazon, eBay, and Uber, might be an illusion. Indeed, for systems that do not have thousands of concurrent users and can be scaled vertically, the benefits of such migration have not been sufficiently investigated, while the existing evidence is inconsistent. Objective. The purpose of this paper is to compare the performance and scalability of monolithic and microservice architectures on a reference web application. Method. The application was implemented in four different versions, covering not only two different architectural styles (monolith vs. microservices) but also two different implementation technologies (Java vs. C#.NET). Next, we conducted a series of controlled experiments in three different deployment environments (local, Azure Spring Cloud, and Azure App Service). Findings. The key lessons learned are as follows: (1) on a single machine, a monolith performs better than its microservice-based counterpart; (2) The Java platform makes better use of powerful machines in case of computation-intensive services when compared to.NET; the technology platform effect is reversed when non-computationally intensive services are run on machines with low computational capacity; (3) vertical scaling is more cost-effective than horizontal scaling in the Azure cloud; (4) scaling out beyond a certain number of instances degrades the application performance; (5) implementation technology (either Java or C#.NET) does not have a noticeable impact on the scalability performance. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"A., Maamouri, Amine; L., Sfaxi, Lilia; R., Robbana, Riadh","Phi: A Generic Microservices-Based Big Data Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125228963&partnerID=40&md5=7871bfadb7dc3d10ea5e2ddfa8657385","We present in this paper Phi, a generic microservices-based Big Data architecture dedicated to complex multi-layered systems, that rallies multiple machine learning jobs, stream and batch processing. We show how to apply our architecture to an adaptive e-learning application that adjusts its recommendation to the emotions of the learner on the spot. We deploy our application on the cloud using AWS services, and perform some performance tests to show its feasibility in a realistic environment. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"P.K., Erdelt, Patrick K.","Orchestrating DBMS Benchmarking in the Cloud with Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124003947&partnerID=40&md5=bea63f2b6fa86667e5e313994d8b4973","Containerization has become a common practise in software provisioning. Kubernetes (K8s) is useful in deploying containers in clusters, in managing their lifecycle, in scheduling and resource allocation. The benchmarking process requires the interaction of various components. We propose a way to organize benchmarking in the Cloud by looking at typical components in the process and ask if they could be managed by K8s as containerized Microservices. We aim at scalability for the process, parallelized execution and minimized traffic I/O from and into the Cloud. This supports planning a series of experiments to investigate a high-dimensional parameter space and avoiding complex installations. This also provides a way for Cross-Cloud comparison. In this article we discuss 1. how objects of K8s can match components of a benchmarking process, 2. how to orchestrate the benchmarking workflow in K8s. We also present an implementation. We show this approach is feasible, relevant, portable and scalable by designing and inspecting a basic profiling benchmark on TPC-DS data handled by 13 DBMS at two private Clouds and five commercial Cloud providers. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"I., Kim, In-kee; W., Wang, Wei; Y., Qi, Yanjun; M.A., Humphrey, Marty A.","Forecasting Cloud Application Workloads With CloudInsight for Predictive Resource Management","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085753714&partnerID=40&md5=17ecf27940c6ab1a76e5e2e3c954724d","Predictive cloud resource management has been widely adopted to overcome the limitations of reactive cloud autoscaling. The predictive resource management is highly relying on workload predictors, which estimate short-/long-term fluctuations of cloud application workloads. These predictors tend to be pre-optimized for specific workload patterns. However, such predictors are still insufficient to handle real-world cloud workloads whose patterns may be unknown a priori, may dynamically change over time and may be irregular. As a result, these predictors often cause over-/under-provisioning of cloud resources. To address this problem, we have created CloudInsight, a novel cloud workload prediction framework, leveraging the combined power of multiple workload predictors. CloudInsight creates an ensemble model using multiple predictors to make accurate predictions for real workloads. The weights of the predictors in CloudInsight are determined at runtime with their accuracy for the current workload using multi-class regression. The ensemble model is periodically optimized to handle sudden changes in the workload. We evaluated CloudInsight with various real workload traces. The results show that CloudInsight has 13-27 percent higher accuracy than state-of-the-art predictors. Moreover, the results from trace-based simulations with a cloud resource manager show that CloudInsight has 15-20 percent less under-/over-provisioning periods, resulting in high cost-efficiency and low SLA violations. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"L., Ju, Li; P., Singh, Prashant; S.Z., Toor, Salman Zubair","Proactive autoscaling for edge computing systems with kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124796621&partnerID=40&md5=bf1046cf0322648351933d21ec533310","With the emergence of the Internet of Things and 5G technologies, the edge computing paradigm is playing increasingly important roles with better availability, latency-control and performance. However, existing autoscaling tools for edge computing applications do not utilize heterogeneous resources of edge systems efficiently, leaving scope for performance improvement. In this work, we propose a Proactive Pod Autoscaler (PPA) for edge computing applications on Kubernetes. The proposed PPA is able to forecast workloads in advance with multiple user-defined/customized metrics and to scale edge computing applications up and down correspondingly. The PPA is optimized and evaluated on an example CPU-intensive edge computing application further. It can be concluded that the proposed PPA outperforms the default pod autoscaler of Kubernetes on both efficiency of resource utilization and application performance. The article also highlights future possible improvements on the proposed PPA. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"S., Quinn, Sterling; R., Cordingly, Robert; W.J., Lloyd, Wes James","Implications of Alternative Serverless Application Control Flow Methods","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121471495&partnerID=40&md5=98a41d1333ba3cba1effa42add9be414","Function-as-a-Service or FaaS is a popular delivery model of serverless computing where developers upload code to be executed in the cloud as short running stateless functions. Using smaller functions to decompose processing of larger tasks or workflows introduces the question of how to instrument application control flow to orchestrate an overall task or workflow. In this paper, we examine implications of using different methods to orchestrate the control flow of a serverless data processing pipeline composed as a set of independent FaaS functions. We performed experiments on the AWS Lambda FaaS platform and compared how four different patterns of control flow impact the cost and performance of the pipeline. We investigate control flow using client orchestration, microservice controllers, event-based triggers, and state-machines. Overall, we found that asynchronous methods led to lower orchestration costs, and that event-based orchestration incurred a performance penalty. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"J., Dantas, Jaime; H., Khazaei, Hamzeh; M., Litoiu, M.","BIAS Autoscaler: Leveraging Burstable Instances for Cost-Effective Autoscaling on Cloud Systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121470927&partnerID=40&md5=822249e5a404a3240dcc8214c88dcdeb","Burstable instances have recently been introduced by cloud providers as a cost-efficient alternative to customers that do not require powerful machines for running their workloads. Unlike conventional instances, the CPU capacity of burstable instances is rate limited, but they can be boosted to their full capacity for small periods when needed. Currently, the majority of cloud providers offer this option as a cheaper solution for their clients. However, little research has been done on the practical usage of these CPU-limited instances. In this paper, we present a novel autoscaling solution that uses burstable instances along with regular instances to handle the queueing arising in traffic and flash crowds. We design BIAS Autoscaler, a state-of-the-art framework that leverages burstable and regular instances for cost-efficient autoscaling and evaluate it on the Google Cloud Platform. We apply our framework to a real-world microservice workload, and conduct extensive experimental evaluations using Google Compute Engines. Experimental results show that BIAS Autoscaler can reduce the overall cost up to 25% and increase resource efficiency by 42% while maintaining the same service quality observed when using conventional instances only. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"J., Park, Jinwoo; B., Choi, Byungkwon; C., Lee, Chunghan; D., Han, Dongsu","GRAF: A graph neural network based proactive resource allocation framework for SLO-oriented microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121606462&partnerID=40&md5=9c47b19331973d1425c6e81b6263ccca","Microservice is an architectural style that has been widely adopted in various latency-sensitive applications. Similar to the monolith, autoscaling has attracted the attention of operators for managing resource utilization of microservices. However, it is still challenging to optimize resources in terms of latency service-level-objective (SLO) without human intervention. In this paper, we present GRAF, a graph neural network-based proactive resource allocation framework for minimizing total CPU resources while satisfying latency SLO. GRAF leverages front-end workload, distributed tracing data, and machine learning approaches to (a) observe/estimate impact of traffic change (b) find optimal resource combinations (c) make proactive resource allocation. Experiments using various open-source benchmarks demonstrate that GRAF successfully targets latency SLO while saving up to 19% of total CPU resources compared to the fine-tuned autoscaler. Moreover, GRAF handles traffic surge with 36% fewer resources while achieving up to 2.6x faster tail latency convergence compared to the Kubernetes autoscaler. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"V., Mittal, Viyom; S., Qi, Shixiong; R., Bhattacharya, Ratnadeep; X., Lyu, Xiaosu; J., Li, Junfeng; S.G., Kulkarni, Sameer G.; D., Li, Dan; J., Hwang, Jinho; K.K., Ramakrishnan, Kadangode K.; T.W., Wood, Timothy W.","Mu: An efficient, fair and responsive serverless framework for resource-constrained edge clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119283941&partnerID=40&md5=a70b607d8e4631e8f7afdd3315d76a19","Serverless computing platforms simplify development, deployment, and automated management of modular software functions. However, existing serverless platforms typically assume an over-provisioned cloud, making them a poor fit for Edge Computing environments where resources are scarce. In this paper we propose a redesigned serverless platform that comprehensively tackles the key challenges for serverless functions in a resource constrained Edge Cloud. Our Mu platform cleanly integrates the core resource management components of a serverless platform: autoscaling, load balancing, and placement. Each worker node in Mu transparently propagates metrics such as service rate and queue length in response headers, feeding this information to the load balancing system so that it can better route requests, and to our autoscaler to anticipate workload fluctuations and proactively meet SLOs. Data from the Autoscaler is then used by the placement engine to account for heterogeneity and fairness across competing functions, ensuring overall resource efficiency, and minimizing resource fragmentation. We implement our design as a set of extensions to the Knative serverless platform and demonstrate its improvements in terms of resource efficiency, fairness, and response time. Evaluating Mu, shows that it improves fairness by more than 2x over the default Kubernetes placement engine, improves 99th percentile response times by 62% through better load balancing, reduces SLO violations and resource consumption by pro-active and precise autoscaling. Mu reduces the average number of pods required by more than ∼15% for a set of real Azure workloads. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"V., Safran, Valentino; D., Hari, Daniel; U., Arioz, Umut; I., Mlakar, Izidor","PERSIST sensing network: A multimodal sensing network architecture for collection of patient-generated health data in the clinical workflow","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119445354&partnerID=40&md5=f2ebf62eb9932011526a81115af24989","Patient gathered health data (PGHD), including self-reports and patient-reported outcomes (PROs) and data based on biometry collected from the wearables, represent an important source of context into patient's life and experiences. However, in light of recent technological boom and privacy concerns, the integration of sensitive data sources, with the knowledge extraction (data processing) and the health-care information systems (data sinks) represents a challenge. In this paper we highlight PERSIST Multimodal Sensing network (P-MSN) as a light-weight enabler for privacy-aware and patient-centric collection and integration of data from the edge. It is designed as an aggregator of data sources. It connects them with sinks and can deliver data extraction in form of multimodal sensing. The framework is designed to support 160 patients in a prospective multicentre clinical study of the PERSIST Project. The main building blocks consist of Apache Camel and ActiveMQ Artemis to deliver interconnectivity, and Kafka to deliver multimodal micro-service network. For evaluation of the system, we applied load tests and the results showed that the lightweight infrastructure can support well over 1000 simultaneous users. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"A.H., Detti, Andrea H.; H., Nakazato, Hidenori; J.A., Martínez, Juan Antonio; G., Tropea, Giuseppe; L., Funari, Ludovico; L., Petrucci, Luca; J.A.S., Segado, Juan Andrés Sánchez; K., Kanai, Kenji","Viriot: A cloud of things that offers iot infrastructures as a service","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116050874&partnerID=40&md5=be27e799de4231975bee35c8ab76e6a9","Many cloud providers offer IoT services that simplify the collection and processing of IoT information. However, the IoT infrastructure composed of sensors and actuators that produces this information remains outside the cloud; therefore, application developers must install, connect and manage the cloud. This requirement can be a market barrier, especially for small/medium software companies that cannot afford the infrastructural costs associated with it and would only prefer to focus on IoT application developments. Motivated by the wish to eliminate this barrier, this paper proposes a Cloud of Things platform, called VirIoT, which fully brings the Infrastructure as a service model typical of cloud computing to the world of Internet of Things. VirIoT provides users with virtual IoT infrastructures (Virtual Silos) composed of virtual things, with which users can interact through dedicated and standardized broker servers in which the technology can be chosen among those offered by the platform, such as oneM2M, NGSI and NGSI-LD. VirIoT allows developers to focus their efforts exclusively on IoT applications without worrying about infrastructure management and allows cloud providers to expand their IoT services portfolio. VirIoT uses external things and cloud/edge computing resources to deliver the IoT virtualization services. Its open-source architecture is microservice-based and runs on top of a distributed Kubernetes platform with nodes in central and edge data centers. The architecture is scalable, efficient and able to support the continuous integration of heterogeneous things and IoT standards, taking care of interoperability issues. Using a VirIoT deployment spanning data centers in Europe and Japan, we conducted a performance evaluation with a two-fold objective: showing the efficiency and scalability of the architecture; and leveraging VirIoT’s ability to integrate different IoT standards in order to make a fair comparison of some open-source IoT Broker implementations, namely Mobius for oneM2M, Orion for NGSIv2, Orion-LD and Scorpio for NGSI-LD. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"B., Choi, Byungkwon; J., Park, Jinwoo; C., Lee, Chunghan; D., Han, Dongsu","PHPA: A Proactive Autoscaling Framework for Microservice Chain","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124492052&partnerID=40&md5=66a643fe3214f6e3b1198d9f31066742","Microservice is an architectural style that breaks down monolithic applications into smaller microservices and has been widely adopted by a variety of enterprises. Like the monolith, autoscaling has attracted the attention of operators in scaling microservices. However, most existing approaches of autoscaling do not consider microservice chain and severely degrade the performance of microservices when traffic surges. In this paper, we present pHPA, an autoscaling framework for the microservice chain. pHPA proactively allocates resources to the microservice chains and effectively handles traffic surges. Our evaluation using various open-source benchmarks shows that pHPA reduces 99%-tile latency and resource usage by up to 70% and 58% respectively compared to the most widely used autoscaler when traffic surges. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"G., Budigiri, Gerald; C., Baumann, Christoph; J.T., Mühlberg, Jan Tobias; E., Truyen, Eddy; W., Joosen, Wouter","Network policies in kubernetes: Performance evaluation and security analysis","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112608931&partnerID=40&md5=d48957ac7820deac79b64827d405c8a8","5G applications with ultra-high reliability and low latency requirements necessitate the adoption of edge computing solutions in mobile networks. Container orchestration frameworks like Kubernetes (K8s) have further emerged as the preferred standard to dynamically deploy edge applications on demand of end-users and third-party companies. Unfortunately, complex networking and security concerns have been highlighted as challenges that impede the successful adoption of container technology by the industry. The security challenge is exacerbated by (mis-)conceptions that secure inter-container communication comes at the cost of performance, yet both requirements are vital for 5G edge-computing use cases. Pursuing low-overhead security solutions, this paper investigates network policies, the K8s concept for controlling network isolation between tenants. We evaluate performance overheads of eBPF -based solutions by Calico and Cilium, and analyze the security of network policies, highlighting security threats to network policies and outline corresponding state-of-the-art solutions. Our assessment shows that network policies are a suitable low-overhead security solution for low-latency inter-container communication. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"G., Shen, Gang; J., Dai, Jianhui; H., Moustafa, Hassnaa; L., Zhai, Lei","5G and Edge Computing Enabling Experience Delivery Network (XDN) for Immersive Media","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113847455&partnerID=40&md5=ab6e16203fb403b392c8a3e0985c18fd","With 5G and Edge Computing, media services observe a very big revolution nowadays. LTE led to media services evolution through enabling video streaming anytime, anywhere, and to any connected mobile device. CDN offered HTTP-based solutions for media streaming services, however real-time communication (RTC) and interactive traffic are not fully supported. 5G and Edge Computing are now enabling new experience through an Experience Delivery Network (XDN), where content creation is in real-time, on the fly and equally uses upstream and downstream links. Real-time Communication (RTC) with 5G and Edge Computing allows negligible latency between the communicating parties and is promising not only for immersive media, but also for Cloud Gaming and Tele-health applications. In this paper, we present a 360 Immersive Media solution using Intel-incubated Open WebRTC Toolkit (OWT) and edge computing platforms, while allowing media ingestion over 5G networks from multiple cameras, media control and 360 media distribution over 5G networks. Microservices architecture is used for media ingestion, control, and distribution functions, allowing the solution deployment across multiple edge computing clusters and sites. We implemented a prototype for our solution, and we present a performance evaluation showing the gain in terms of bandwidth and latency besides the modular deployment. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"H., Calderon-Gomez, Huriviades; L., Mendoza-Pittí, Luis; M., Vargas-Lombardo, Miguel; J.M.M., Gómez Pulido, José Manuel M.; D., Rodríguez–Puyol, Diego; G.L., Sención-Martínez, Gloria Lisette; M.L., Polo-Luque, Maria Luz","Evaluating service-oriented and microservice architecture patterns to deploy ehealth applications in cloud computing environment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106660998&partnerID=40&md5=28aaa99d4c4fbae8a00be21c7c05f7cf","This article proposes a new framework for a Cloud-based eHealth platform concept focused on Cloud computing environments, since current and emerging approaches using digital clinical history increasingly demonstrate their potential in maintaining the quality of the benefits in medical care services, especially in computer-assisted clinical diagnosis within the field of infectious diseases and due to the worsening of chronic pathologies. Our objective is to evaluate and contrast the performance of the architectural patterns most commonly used for developing eHealth applications (i.e., service-oriented architecture (SOA) and microservices architecture (MSA)), using as reference the quantitative values obtained from the various performance tests and their ability to adapt to the required software attribute (i.e., versatile high-performance). Therefore, it was necessary to modify our platform to fit two architectural variants. As a follow-up to this activity, corresponding tests were performed that showed that the MSA variant functions better in terms of performance and response time compared to the SOA variant; however, it consumed significantly more bandwidth than SOA, and scalability in SOA is generally not possible or requires significant effort to be achieved. We conclude that the implementation of SOA and MSA depends on the nature and needs of organizations (e.g., performance or interoperability). © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"N., Eskandani, Nafise; G., Salvaneschi, Guido","The wonderless dataset for serverless computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113686399&partnerID=40&md5=d38b0c6b2208110bc6133b12cdacc099","Function as a Service (FaaS) has grown in popularity in recent years, with an increasing number of applications following the Serverless computing model. Serverless computing supports out of the box autoscaling in a pay-as-you-go manner, letting developers focus on the application logic rather than worrying about resource management. With the increasing adoption of the this model, researchers have started studying a wide variety of aspects of Serverless computing, including communication, security, performance, and cost optimization. Yet, we still know very little of how Serverless computing is used in practice.In this paper, we introduce Wonderless, a novel dataset of open-source Serverless applications. Wonderless consists of 1, 877 real-world Serverless applications extracted from GitHub, and it can be used as a data source for further research in the Serverless ecosystem, such as performance evaluation and software mining. To the best of our knowledge, Wonderless is currently the most diverse and largest dataset for research on Serverless computing. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"N.M., Dang-Quang, Nhat Minh; M., Yoo, Myungsik","Deep learning-based autoscaling using bidirectional long short-term memory for kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105234124&partnerID=40&md5=0f5f91e60ef0ebcebde1969006caab39","Presently, the cloud computing environment attracts many application developers to deploy their web applications on cloud data centers. Kubernetes, a well-known container orchestration for deploying web applications on cloud systems, offers an automatic scaling feature to meet clients’ ever-changing demands with the reactive approach. This paper proposes a system architecture based on Kubernetes with a proactive custom autoscaler using a deep neural network model to handle the workload during run time dynamically. The proposed system architecture is designed based on the Monitor–Analyze–Plan–Execute (MAPE) loop. The main contribution of this paper is the proactive custom autoscaler, which focuses on the analysis and planning phases. In analysis phase, Bidirectional Long Short-term Memory (Bi-LSTM) is applied to predict the number of HTTP workloads in the future. In the planning phase, a cooling-down time period is implemented to mitigate the oscillation problem. In addition, a resource removal strategy is proposed to remove a part of the resources when the workload decreases, so that the autoscaler can handle it faster when the burst of workload happens. Through experiments with two different realistic workloads, the Bi-LSTM model achieves better accuracy not only than the Long Short-Term Memory model but also than the state-of-the-art statistical auto-regression integrated moving average model in terms of short-and long-term forecasting. Moreover, it offers 530 to 600 times faster prediction speed than ARIMA models with different workloads. Furthermore, as compared to the LSTM model, the Bi-LSTM model performs better in terms of resource provision accuracy and elastic speedup. Finally, it is shown that the proposed proactive custom autoscaler outperforms the default horizontal pod autoscaler (HPA) of the Kubernetes in terms of accuracy and speed when provisioning and de-provisioning resources. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"D.N., Jha, Devki Nandan; S.K., Garg, Saurabh Kumar; P.P., Prakash Jayaraman, Prem Prakash; R., Buyya, Rajkumar; Z., Li, Zheng; G., Morgan, Graham; R., Ranjan, Rajiv","A study on the evaluation of HPC microservices in containerized environment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065311246&partnerID=40&md5=e182cc8afb7a39151b92be8ad0f605e0","Containers are gaining popularity over virtual machines as they provide the advantages of virtualization with the performance of near bare metal. The uniformity of support provided by Docker containers across different cloud providers makes them a popular choice for developers. Evolution of microservice architecture allows complex applications to be structured into independent modular components making them easier to manage. High-performance computing (HPC) applications are one such application to be deployed as microservices, placing significant resource requirements on the container framework. However, there is a possibility of interference between different microservices hosted within the same container (intracontainer) and different containers (intercontainer) on the same physical host. In this paper, we describe an extensive experimental investigation to determine the performance evaluation of Docker containers executing heterogeneous HPC microservices. We are particularly concerned with how intracontainer and intercontainer interference influences the performance. Moreover, we investigate the performance variations in Docker containers when control groups (cgroups) are used for resource limitation. For ease of presentation and reproducibility, we use Cloud Evaluation Experiment Methodology (CEEM) to conduct our comprehensive set of experiments. We expect that the results of evaluation can be used in understanding the behavior of HPC microservices in the interfering containerized environment. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"F., Hussain Khoso, Fida; A.R., Lakhan, Abdullah Raza; A.A., Arain, Aijaz Ahmed; M.A., Soomro, Muhammad Ali; S.Z., Nizamani, Shah Zaman; K., Kanwar, Kelash","A Microservice-Based System for Industrial Internet of Things in Fog-Cloud Assisted Network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107731422&partnerID=40&md5=f1528fb5b538818249604620efbc5d18","Nowadays, the usage of the Industrial Internet of Things (IIoT) in practical applications has increased. The primary utilization is a fog cloud network, which offers different services, such as network and remote edges, at different places. Existing studies implemented the Service-Oriented Architecture (SOA) based on the fog-cloud network to run IIoT applications, such as e-healthcare, e-agriculture, renewable energy, etc. However, due to the applications' monolithic property, issues like failures, security, and cost factors occur, e.g. the failure of one service in SOA affects monolithic applications' performance in the system. With this motivation, this study suggests a microservice-based system to deal with the cost, security, and failure risks of IIoT applications in the fog-cloud system. The study improves the existing SOA systems for e-healthcare, eagriculture, and renewable energy and minimizes the applications' overall cost. The performance evaluation shows that the devised systems outperform the existing SOA system in terms of failure, cost, and the deadline for all applications. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"N., Gonçalves, Nuno; D., Faustino, Diogo; A., Rito Silva, António; M., Portela, Manuel","Monolith Modularization towards Microservices: Refactoring and Performance Trade-offs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106639316&partnerID=40&md5=b3cdb77d818eb2a271bd2fae5dbd92f4","The agility inherent to today's business promotes the definition of software architectures where the business entities are decoupled into modules and/or services. However, there are advantages in having a rich domain model, where domain entities are tightly connected, because it fosters reuse. On the other hand, the split of the business logic into modules and its encapsulation through well-defined interfaces introduces a cost in terms of performance. In this paper we analyze the impact of migrating a rich domain object into a modular architecture, both in terms of the development cost associated with the refactoring, and the performance cost associated with the execution. Current state of the art analyses the migration of monolith systems to a microservices architecture, but we observed that migration effort and performance issues are already relevant in the migration to a modular monolith. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"Y., Li, Ye; H., Zhang, Haitao; W., Tian, Wei; H., Ma, Huadong","Joint Optimization of Auto-Scaling and Adaptive Service Placement in Edge Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129791315&partnerID=40&md5=a5a4f1721ecfb3c3e6ba668e8857c1cc","In edge computing environment where network connections are often unstable and workload intensity changes frequently, the proper scaling mechanism and service placement strategy based on microservices are needed to ensure the edge services can be provided consistently. However, the common elastic scaling mechanism nowadays is threshold-based responsive scaling and has reaction time in the order of minutes, which is not suitable for delay-sensitive applications in the edge computing environment. Moreover, auto-scaling strategy and service replica placement are considered separately. If the scaled service replicas are misplaced on the edge nodes with limited resources or significant communication latency between upstream and downstream neighbours, the Quality of Service (QoS) cannot be guaranteed even with the auto-scaling mechanism. In this paper, we study the joint optimization of dynamic auto-scaling and adaptive service placement, and define it as a task delay minimization problem while satisfying resource and bandwidth constraints. Firstly, we design a multi-stage auto-scaling model based on workload prediction and performance evaluation of edge nodes to dynamically create an appropriate number of service replicas. Secondly, we propose a Dynamic Adaptive Service Placement (DASP) approach to iteratively place each service replica by using Adaptive Discrete Binary Particle Swarm Optimization (ADBPSO) algorithm. DASP can determine the current optimal placement strategy according to dynamic service replica scaling decision in a short time. The placement results of the current round will guide the optimization of the next cycle iteratively. The experimental evaluation shows that our approach significantly outperforms the existing methods in reducing the average task response time. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"J., Flora, José; P., Goncalves, Paulo; M., Teixeira, Miguel; N., Antunes, Nuno","My Services Got Old! Can Kubernetes Handle the Aging of Microservices?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126938427&partnerID=40&md5=cde4328dd8fbc46d8093c421e88b388b","The exploding popularity of microservice based applications is taking companies to adopt them along with cloud services to support them. Containers are the common deployment infrastructures that currently serve millions of customers daily, being managed using orchestration platforms that monitor, manage, and automate most of the work. However, there are multiple concerns with the claims put forward by the developers of such tools. In this paper, we study the effects of aging in microservices and the utilization of faults to accelerate aging effects while evaluating the capacity of Kubernetes to detect microservice aging. We consider three operation scenarios for a representative microservice-based system through the utilization of stress testing and fault injection as a manner to potentiate aging in the services composing the system to evaluate the capacity of Kubernetes mechanisms to detect it. The results demonstrate that even though some services tend to accumulate aging effects, with increasing resource consumption, Kubernetes does not detect them nor acts on them, which indicates that the probe mechanisms may be insufficient for aging scenarios. This factor may indicate the necessity for more effective mechanisms, capable of detecting aging early on and act on it in a more proactive manner without requiring the services to become unresponsive. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"M., Kazlauskas, Mantas; D., Navakauskas, Dalius","Case Study of a Multisensor Patient Network and Microservices Managed by Fog Computing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125183750&partnerID=40&md5=94616befd53ebc841667fc16fbe75307","Fog Computing is a new paradigm of Internet of Things shifting services from Cloud closer to the Edge. It allows real-time execution, low latency, effective energy consumption and data privacy. Fog Computing solutions could greatly benefit Health Care as it is one of the most expensive person-centered industries. This article discusses a case of multisensor patient health data collection network working with Fog Computing devices and the Cloud. Patient multisensor network simulations of Fog and Cloud Computing hardware and Microservices with iFogSim2 were performed to estimate the performance ensuring quality of service requirements. Multisensor application model and performance evaluation for different multisensor scenarios utilizing distributed Microservice placement is presented. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"M., Sliem, Mehdi; N., Salmi, Nabila; M., Boukala-Ioualalen, Malika","Performance Analysis of Self-adaptive Policies in Containerized Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124674281&partnerID=40&md5=26f0120b477a259b52def3fe96b39846","Nowadays applications tend to be designed following a microservice based architecture, deployed on containers and handled by orchestration platforms on the Cloud. This increasing attention to containerized microservices is due to the high modularity, flexibility and scalability of their architecture. Container technologies running on orchestration platforms are skyrocketing in popularity among developers and in deploying all kinds of microservices and applications such as smart vehicles, IoTs and fog/edge computing. Therefore, efficient management of Cloud resources at run time through self-adaptive clusters has become of prime importance in Cloud computing. This paper presents a Stochastic Petri Net modelling approach to analyze the performances of self-adaptive policies in containerized microservices. We illustrate the effectiveness of our approach through a set of experimental results. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"C., Boeira, Conrado; M.C., Neves, Miguel Cardoso; T.C., Ferreto, Tiago Coelho; I.T., Haque, Israat Tanzeena","Characterizing network performance of single-node large-scale container deployments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124565060&partnerID=40&md5=bfd693ddb516221030b455f8961fcc87","Cloud services have shifted from complex monolithic designs to hundreds of loosely coupled microservices over the last years. These microservices communicate via pre-defined APIs (e.g., RPC) and are usually implemented on top of containers. To make the microservices model profitable, cloud providers often co-locate them on a single (virtual) machine, thus achieving high server utilization. Despite being overlooked by previous work, the challenge of providing high-quality network connectivity to multiple containers running on the same host becomes crucial for the overall cloud service performance in this scenario. For that reason, this paper focuses on identifying the overheads and bottlenecks caused by the increasing number of concurrent containers running on a single node, particularly from a networking perspective. Through an extensive set of experiments, we show that the networking performance is mostly restricted by the CPU capacity (even for I/O intensive workloads), that containers can largely suffer from interference originated from packet processing, and that proper core scheduling policies can significantly improve connection throughput. Ultimately, our findings can help to pave the way towards more efficient large-scale microservice deployments. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Houmani, Zeina; D., Balouek-Thomert, Daniel; E., Caron, Eddy; M., Parashar, Manish","Enabling microservices management for Deep Learning applications across the Edge-Cloud Continuum","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124333656&partnerID=40&md5=8727d4774a69411b961f56572d67737a","Deep Learning has shifted the focus of traditional batch workflows to data-driven feature engineering on streaming data. In particular, the execution of Deep Learning workflows presents expectations of near-real-time results with user-defined acceptable accuracy. Meeting the objectives of such applications across heterogeneous resources located at the edge of the network, the core, and in-between requires managing trade-offs between the accuracy and the urgency of the results. However, current data analysis rarely manages the entire Deep Learning pipeline along the data path, making it complex for developers to implement strategies in realworld deployments. Driven by an object detection use case, this paper presents an architecture for time-critical Deep Learning workflows by providing a data-driven scheduling approach to distribute the pipeline across Edge to Cloud resources. Furthermore, it adopts a data management strategy that reduces the resolution of incoming data when potential trade-off optimizations are available. We illustrate the system's viability through a performance evaluation of the object detection use case on the Grid'5000 testbed. We demonstrate that in a multi-user scenario, with a standard frame rate of 25 frames per second, the system speed-up data analysis up to 54.4% compared to a Cloud-only-based scenario with an analysis accuracy higher than a fixed threshold. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"M., Simon, Mathieu; A., Spallina, Alessandro; L., Dubocquet, Loic; A., Araldo, Andrea","Parsimonious Edge Computing to Reduce Microservice Resource Usage","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124034547&partnerID=40&md5=8cae7ddb94033412225b34f2c5258381","Cloud Computing (CC) is the most prevalent paradigm under which services are provided over the Internet. The most relevant feature for its success is its capability to promptly scale service based on user demand. When scaling, the main objective is to maximize as much as possible service performance. Moreover, resources in the Cloud are usually so abundant, that they can be assumed infinite from the service point of view: an application provider can have as many servers it wills, as long it pays for it.This model has some limitations. First, energy efficiency is not among the first criteria for scaling decisions, which has raised concerns about the environmental effects of today's ""wild""computations in the Cloud. Moreover, it is not viable for Edge Computing (EC), a paradigm in which computational resources are distributed up to the very edge of the network, i.e., co-located with base stations or access points. In edge nodes, resources are limited, which imposes different parsimonious scaling strategies to be adopted.In this work, we design a scaling strategy aimed to instantiate, parsimoniously, a number of microservices sufficient to guarantee a certain Quality of Service (QoS) target. We implement such a strategy in a Kubernetes/Docker environment. The strategy is based on a simple Proportional-Integrative-Derivative (PID) controller. In this paper we describe the system design and a preliminary performance evaluation. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"L., Liu, Lei; Z., Tu, Zhiying; X., He, Xiang; X., Xu, Xiaofei; Z., Wang, Zhongjie","An Empirical Study on Underlying Correlations between Runtime Performance Deficiencies and 'Bad Smells' of Microservice Systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123214735&partnerID=40&md5=842efb00f69b44e917e3665fea203e53","Although many principles have been put forward to guide microservice design, such as Domain-Driven Design, Architectural Bad Smells (ABS) would be inevitably imported during microservice system design and development. There has been a consensus that the existence of ABSs would bring negative effects to microservice systems. Although some approaches use static analysis to detect ABSs in the design phase, we conjecture that the design-phase ABSs would result in some performance deficiencies at runtime. This paper conducts an empirical study on underlying correlations between runtime performance deficiencies and ABSs in microservice system design. An automated experimental stress testing framework called MRSTF is developed for automatic deployment of MSS and collecting/analyzing runtime performance data. A microservice system TrainTicket is used in this empirical study. We manually inject several typical ABSs into the system and use MRSTF to compare the runtime performances before and after the injection and check if the existence of ABSs has significant effects on the runtime performance. Experiment results show that ABSs have significant negative effects on some performance metrics of runtime MSS while doing not on others. This study provides solid evidence on the feasibility of optimizing MSS design by eliminating ABSs based on runtime performance data. © 2022 Elsevier B.V., All rights reserved.",No load test generation.
"H.T., Nguyen, Hai T.; T.V., van do, Tien V.; C.V., Rotter, Csaba V.","Scaling UPF Instances in 5G/6G Core with Deep Reinforcement Learning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121798087&partnerID=40&md5=dba34800f0c5ae80cfa56c5f45fe4987","In the 5G core and the upcoming 6G core, the User Plane Function (UPF) is responsible for the transportation of data from and to subscribers in Protocol Data Unit (PDU) sessions. The UPF is generally implemented in software and packed into either a virtual machine or container that can be launched as a UPF instance with a specific resource requirement in a cluster. To save resource consumption needed for UPF instances, the number of initiated UPF instances should depend on the number of PDU sessions required by customers, which is often controlled by a scaling algorithm. In this paper, we investigate the application of Deep Reinforcement Learning (DRL) for scaling UPF instances that are packed in the containers of the Kubernetes container-orchestration framework. We propose an approach with the formulation of a threshold-based reward function and adapt the proximal policy optimization (PPO) algorithm. Also, we apply a support vector machine (SVM) classifier to cope with a problem when the agent suggests an unwanted action due to the stochastic policy. Extensive numerical results show that our approach outperforms Kubernetes's built-in Horizontal Pod Autoscaler (HPA). DRL could save 2.7-3.8% of the average number of Pods, while SVM could achieve 0.7-4.5% saving compared to HPA. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"C., Jiang, Chunmao; P., Wu, Peng","A Fine-Grained Horizontal Scaling Method for Container-Based Cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120823047&partnerID=40&md5=7587d06ca0ce4334378551f239310852","The container scaling mechanism, or elastic scaling, means the cluster can be dynamically adjusted based on the workload. As a typical container orchestration tool in cloud computing, Horizontal Pod Autoscaler (HPA) automatically adjusts the number of pods in a replication controller, deployment, replication set, or stateful set based on observed CPU utilization. There are several concerns with the current HPA technology. The first concern is that it can easily lead to untimely scaling and insufficient scaling for burst traffic. The second is that the antijitter mechanism of HPA may cause an inadequate number of onetime scale-outs and, thus, the inability to satisfy subsequent service requests. The third concern is that the fixed data sampling time means that the time interval for data reporting is the same for average and high loads, leading to untimely and insufficient scaling at high load times. In this study, we propose a Double Threshold Horizontal Pod Autoscaler (DHPA) algorithm, which fine-grained divides the scale of events into three categories: scale-out, no scale, and scale-in. And then, on the scaling strength, we also employ two thresholds that are further subdivided into no scaling (antijitter), regular scaling, and fast scaling for each of the three cases. The DHPA algorithm determines the scaling strategy using the average of the growth rates of CPU utilization, and thus, different scheduling policies are adopted. We compare the DHPA with the HPA algorithm under different loads, including low, medium, and high. The experiments show that the DHPA algorithm has better antijitter and antiload characteristics in container increase and reduction while ensuring service and cluster security. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"D., Baid, Darshan; P.M., Goel, Pallavi Murghai; P., Bhardwaj, Pragya; A., Singh, Astha; V., Tyagi, Vishu","Comparative Analysis of Serverless Solutions from Public Cloud Providers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118112544&partnerID=40&md5=385b7dbc8546d79366e72494ed1644e8","Cloud Providers such as Google Cloud Platform (GCP), Microsoft Azure, Amazon Web Services (AWS) offer have started expanding their serverless platforms, promising higher availability and dynamic auto-scaling while at the same time diminishing operational and maintenance costs. One such is serverless processing, or function-as-a-service (FaaS). Serverless has emerged as a decent contestant for web applications, APIs, backend-services due to their unique event-driven architecture and pay-as-you-go model. In this paper, we discussed the current architecture of serverless functions & compared the strength of serverless offering of two major cloud providers - Google’s GCP & Microsoft Azure and conducted a load testing experiment against their node js environment to better understand their capabilities. We conducted 2 different scenarios of user-load testing via the open-source framework, k6. These scenarios included several phases of sudden and spontaneous load increment and decrement, mimicking how it happens in real-world situations. Finally, we observed how quickly and effectively they scale, their cold boot times, and their response times. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"H., Koziolek, Heiko; A., Burger, Andreas; P.P., Abdulla, P. P.; J., Rückert, Julius; S., Sonar, Shardul; P., Rodriguez, Pablo","Dynamic Updates of Virtual PLCs Deployed as Kubernetes Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115167397&partnerID=40&md5=fa2f6e17f44c7b30328d29a848a58b51","Industrial control systems (e.g. programmable logic controllers, PLC or distributed control systems, DCS) cyclically execute control algorithms to automated production processes. Nowadays, for many applications their deployment is moving from dedicated embedded controllers into more flexible container environments, thus becoming “Virtual PLCs”. It is difficult to update such containerized Virtual PLCs during runtime by switching to a newer instance, which requires transferring internal state. Former research has only proposed dynamic update approaches for single embedded controllers, while other work introduced special Kubernetes (K8s) state replication approaches, which did not support cyclic real-time applications. We propose a dynamic update mechanism for Virtual PLCs deployed as K8s microservices. This approach is based on a purpose-built K8s Operator and allows control application updates without halting the production processes. Our experimental validation shows that the approach can support the internal state transfer of large industrial control applications (100.000 state variables) within only 15% of the available cycle slack time. Therefore, the approach creates vast opportunities for updating applications on-the-fly and migrating them between nodes in a cloud-native fashion. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"D., Chowdary Attota, Dinesh; V., Mothukuri, Viraaji; R.M., Parizi, Reza Meimandi; S., Pouriyeh, Seyedamin","An Ensemble Multi-View Federated Learning Intrusion Detection for IoT","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113871134&partnerID=40&md5=1e5b253710e3dc92731afd4275e7b315","The rise in popularity of Internet of Things (IoT) devices has attracted hackers to develop IoT-specific attacks. The microservice architecture of IoT devices relies on the Internet to provide their intended services. An unguarded IoT network makes inter-connected devices vulnerable to attacks. It will be a tedious and ineffective process to manually detect the attacks in the network, as the attackers frequently upgrade their attack strategies. Machine learning (ML)-assisted approaches have been proposed to build intrusion detection for cybersecurity automation in IoT networks. However, most such approaches focus on training an ML model using a single view of the dataset, which often fails to build insightful knowledge and understand each feature’s impact on the ML model’s decision-making ability. As such, the model training with a single view may result in an incomplete understanding of patterns in large feature-set datasets. Moreover, the current approaches are mainly designed in a centralized manner in which the raw data is transferred from the edge devices to the central server for training. This, in turn, may expose the data to all kinds of attacks without adhering to the privacy-preserving of data security. Multi-view learning has gained popularity for its ability to learn from different data views and deliver efficient performance with more distinguished predictions. This paper proposes a federated learning-based intrusion detection approach, called MV-FLID, that trains on multiple views of IoT network data in a decentralized format to detect, classify, and defend against attacks. The multi-view ensemble learning aspect helps in maximizing the learning efficiency of different classes of attacks. The Federated Learning (FL) aspect, wherein the device’s data is not shared to the server, performs profile aggregation efficiently with the benefit of peer learning. Our evaluation results show that our proposed approach has higher accuracy compared to the traditional non-FL centralized approach. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"M., Whaiduzzaman, Md; M.J.N., Mahi, Md Julkar Nayeen; A.P., Barros, Alistair P.; M.I., Khalil, Md Ibrahim; C.J., Fidge, Colin J.; R., Buyya, Rajkumar","BFIM: Performance Measurement of a Blockchain Based Hierarchical Tree Layered Fog-IoT Microservice Architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112648003&partnerID=40&md5=b156c67edbb9094b6b5061a125deb7d3","Fog computing complements cloud computing by removing several limitations, such as delays and network bandwidth. It emerged to support Internet of Things (IoT) applications wherein its computations and tasks are carried out at the network's edge. Heterogeneous IoT devices interact with different users throughout a network. However, data security is a crucial concern for IoT, fog and cloud network ecosystems. Since the number of anonymous users increases and new identity disclosures occur within the IoTs, it is becoming challenging to grow mesh networks to deliver end to end communications, as the extended IoT networks resemble a mesh architecture. To reinforce data security over IoTs, we deploy a microservice-based blockchain mechanism for fogs, which works as a decentralized client-server network medium (i.e., secured end device-based communication). We implement a blockchain equipped security scheme to be used with a fog-IoT hierarchical tree-based overlay mesh architecture to address and develop the network performance issues. In this study, we consider encryption and decryption delays from IoT and fog-integrated parts to monitor data records and compare them through the developed security scheme. The blocks of a blockchain offer the desired execution results mainly in terms of the algorithmic efficiency, which correlates with the existing algorithms, namely the Advanced Encryption Standard (AES), the Rivest Shamir Adleman (RSA), and the Data Encryption Standard (DES). Our 'BFIM' scheme has an enhanced task scheduling capacity and a more efficient throughput than the AES, DES, RSA resource deliverables (i.e., tasks). Our comprehensive performance evaluation implies that the Blockchain-based Fog IoT Microservice (i.e., BFIM) architecture provides a task delivery efficiency of 78.79% (i.e., task deliverable) and a service delivery efficiency of 83.24% (i.e., task scheduling). The 'BFIM' also has an overall process delivery efficiency of 75% (i.e., time delay, throughput) in the fog layer, rather than a central cloud layer running the AES, DES, and RSA algorithms. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"V., Raj, Vinay; R., Sadam, Ravichandra","Performance and complexity comparison of service oriented architecture and microservices architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111433167&partnerID=40&md5=cbcbd15eceec8f0ff345f96d9adc559c","Microservices has emerged as a new architectural style of designing applications to overcome the challenges of service oriented architecture (SOA). With the evolution of microservices architecture, architects have started migrating legacy applications to microservices. However, some of the architects are in chaos whether to migrate the application from SOA to microservices or not. The need for empirical evaluation and comparison of both the SOA and microservices architecture is also on-demand. This work helps the software architects in better understanding of the technical differences between both styles. We, therefore, present a comparison of a web application that is designed using both SOA and microservices architectures. The comparison is presented with two different parameters: 1) complexity with architectural metrics; 2) performance with load testing. It is clear from the results that though the complexity of microservices architecture is high, the response time for processing the requests is very fast compared to SOA services. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"J.O., de Carvalho, Juliana Oliveira; F.A.M., Trinta, Fernando Antonio Mota; D., Vieira, Dario","A Multi-cloud Parallel Selection Approach for Unlinked Microservice Mapped to Budget’s Quota: The PUM2Q","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104759225&partnerID=40&md5=7fbc2979a6ba72067feedd8afd7b8ab7","The world is facing a complicated moment in which social isolation is necessary. Therefore, to minimize the problems of companies, remote work is being widely adopted, which is only possible because of existing technologies, including cloud computing. Choosing the providers to host the business applications is a complex task, as there are many providers and most of them offer various services with the same functionality and different capabilities. Thus, in this paper, we propose an approach, called PUM2Q, for selecting providers to host a distributed application based on microservices that have little communication between them. PUM2Q is a provider selection approach based on multi-criteria, and it copes with the needs of microservices individually and in parallel. The proposed approach extends our previous one, UM2Q, and should be incorporated by PacificClouds. Besides, we carry out a performance evaluation by varying the number of requirements, microservices, and providers. We also compare PUM2Q and UM2Q. The results presented by PUM2Q are better than those given by UM2Q, showing not only its viability but also expanding the number of approaches adopted by PacificClouds. As a result, PUM2Q making the tasks of the software architect, who is the user of PacificClouds, more flexible. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"A.A., Slamaa, Amany A.; H.A., El-Ghareeb, Haitham A.; A.A., Saleh, Ahmed Aboelfetouh","A Roadmap for Migration System-Architecture Decision by Neutrosophic-ANP and Benchmark for Enterprise Resource Planning Systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103261852&partnerID=40&md5=479622dc4aa0674f7f3a21f2f4e1fa13","The selection of the system-architecture is critical step in software-engineering lifecycle. Unfortunately, the selecting a suitable architecture is a hard mission. The difficulty of the adopting architecture increases in inconsistence case as in SOA and MSA. The paper answers 'when architecture transform', 'to which architecture migrate' and 'how'. The paper addresses the lack of evaluation ERPs in post-implementation phase. The paper provides static and dynamic analysis for monolith, SOA and MSA by numerical-comparison and Benchmark. This paper proposes a roadmap for migration-architecture. The roadmap consists of flowchart of migration and migration-decision model that is supported with criteria of evaluation form for assessing system's baseline and making a consistence decision by Neutrosophic-ANP. Because of the dependency between factors of information system evolution, ANP is used. Furthermore, Neutrosophic set is selected because its accuracy in inconsistence. Benchmark assesses OpenBravoERP, Odoo12 and Metasfresh5.144 to represent monolith, SOA and MSA respectively. Benchmark assesses performance, time to the market, stability, future proof, network issue and cost. They are tested based on scenario tests which grantee that transactions are executed within more than one module of ERPS, simulates real business's procedures, huge number of records and concurrent load by virtual users. The proposed model and benchmark help 'Hitac' enterprise in two decisions for its expansion-plan in COVID-19 crisis. The numerical steps of this case-study are provided. Most published papers and literatures about Microservices are not empirical. The main paper's result is filling this gap by a one of the empirical aspects with an academic study support. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"M.A., Tamiru, Mulugeta Ayalew; J., Tordsson, Johan; E., Elmroth, Erik; G., Pierre, Guillaume","An Experimental Evaluation of the Kubernetes Cluster Autoscaler in the Cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105813997&partnerID=40&md5=600063555c55af3b864a3109e616ddb8","Despite the abundant research in cloud autoscaling, autoscaling in Kubernetes, arguably the most popular cloud platform today, is largely unexplored. Kubernetes' Cluster Autoscaler can be configured to select nodes either from a single node pool (CA) or from multiple node pools (CA-NAP). We evaluate and compare these configurations using two representative applications and workloads on Google Kubernetes Engine (GKE). We report our results using monetary cost and standard autoscaling performance metrics (under-and over-provisioning accuracy, under-and over-provisioning timeshare, instability of elasticity and deviation from the theoretical optimal autoscaler) endorsed by the SPEC Cloud Group. We show that, overall, CA-NAP outperforms CA and that autoscaling performance depends mainly on the composition of the workload. We compare our results with those of the related work and point out further configuration tuning opportunities to improve performance and cost-saving. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"M.S., Aslanpour, Mohammad Sadegh; A.N., Toosi, Adel N.; R., Gaire, Raj; M.A., Cheema, Muhammad Aamir","Auto-scaling of web applications in clouds: A tail latency evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099586695&partnerID=40&md5=4aa314d8f31c35ed46984ce2597335ac","Mechanisms for dynamically adding and removing Virtual Machines (VMs) to reduce cost while minimizing the latency are called auto-scaling. Latency improvements are mainly fulfilled through minimizing the ""average""response times while unpredictabilities and fluctuations of the Web applications, aka flash crowds, can result in very high latencies for users' requests. Requests influenced by flash crowd suffer from long latencies, known as outliers. Such outliers are inevitable to a large extent as auto-scaling solutions continue to improve the average, not the ""tail""of latencies. In this paper, we study possible sources of tail latency in auto-scaling mechanisms for Web applications. Based on our extensive evaluations in a real cloud platform, we discovered sources of a tail latency as 1) large requests, i.e. those data-intensive; 2) long-term scaling intervals; 3) instant analysis of scaling parameters; 4) conservative, i.e. tight, threshold tuning; 5) load-unaware surplus VM selection policies used for executing a scale-down decision; 6) cooldown feature, although cost-effective; and 7) VM start-up delay. We also discovered that after improving the average latency by auto-scaling mechanisms, the tail may behave differently, demanding dedicated tail-aware solutions for auto-scaling mechanisms. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"F., Dewanta, Favian","Secure Microservices Deployment for Fog Computing Services in a Remote Office","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100883376&partnerID=40&md5=c706222bdae2fb35022d6f178f468191","Microservices deployment remains insecure because it relies on the knowledge of microservices'users against security aspects in some particular fog computing networks. As a consequence, the users need to carefully assess the vulnerability of the micorservices' deployment. In addition, the users have to ensure that transactions between microservices and fog computing server should be verified and protected against any potential attacks. This paper proposes secure microservices deployment for environment of fog computing services by establishing trusted and authenticated communicationchannel prior to engaging any transactions among all entities. The proposed method is lightweight due to employing one-way hash function and XOR operation. Eventually, performance evaluation shows that the method is secure against replay attack, offline guessing attack, impersonation attack, and ephemeral secret leakage attack. Moreover, the proposed scheme is more lightweight in terms of communication and computational cost with respect to the J-PAKE algorithm. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"M., Nekovee, Maziar; S.K., Sharma, Sachin K.; N., Uniyal, Navdeep; A., Nag, Avishek; R., Nejabati, Reza; D.E., Dimitra Simeonidou, Dimitra E.","Towards AI-enabled Microservice Architecture for Network Function Virtualization","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097599131&partnerID=40&md5=39290826e8ba62d440063cd3df4f3374","Network Function Virtualization (NFV) enables operators to flexibly deploy network services on commodity servers in an on-demand and agile manner. This has recently attracted significant attention from industry and academia. However, there are important challenges for NFV deployment, including performance bottlenecks, degraded fault tolerance, upgrading complexities, and security threats. To overcome these challenges, the microservice approach, which has been applied successfully in cloud computing, has motivated the research communities to apply its principles in NFV domains. In this paper, we first compare the challenges of employing the microservices approach in both the cloud computing and the NFV domains, and then discuss the need for AI-enabled microservices architecture for NFV. Performance evaluation of the microservices approach in NFV is performed on bare-metal machine setups. The results compare the pros and cons of the microservice approach and show the need for AI to handle the complex decisions associated with decomposing network functions into microservices or vice versa. We also propose an AI-enabled microservice architecture and present its potential use cases for personalized live streaming, smart public safety, and enterprise VPN (Virtual Private Network). Open questions and future work are also presented. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"X., Zuo, Xianyu; Y., Su, Yuehan; Q., Wang, Qianqian; Y., Xie, Yi","An API gateway design strategy optimized for persistence and coupling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087401223&partnerID=40&md5=4f27dcbae107995aea87cfc0028ee81c","Microservices play a more and more important role in software development nowadays. Almost every programming language has its own microservices development framework. The characteristics of microservices make microservices have cross-platform compatibility issues and inconsistent call standards issues in the process of development and call microservices. The birth of API Gateway alleviates these problems to some extent. For small and medium-sized enterprises using today's popular API Gateways, it is difficult for them to get a balance between cost, performance and maintainability. This paper proposes a scheme to optimize the API Gateway. Firstly, the framework of API Gateway is optimized. Next, the coupling degree of API Gateway is optimized by reducing the coupling degree of core services and extended functions. In this way, the optimized Gateway can adapt to the plug-in mode, improve the user experience and reduce development costs. Then, the persistent design of the configuration information of the API Gateway is carried out, and the read-write optimization is carried out so that the optimized API Gateway not only has advantages in the configuration persistence, but also has further improved the I/O performance. Based on the optimized design, this paper implements a cross platform compatible API Gateway. Then it compares the performance of two popular API Gateway schemes through performance testing. The test results show that the optimized design of API Gateway achieves a new balance among cost, performance and maintainability. At the end of this paper, the work of this paper is summarized, and the next work is projected. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"C., Zheng, Chao; N., Kremer-Herman, Nathaniel; T.R., Shaffer, Tim R.; D.L., Thain, Douglas L.","Autoscaling High-Throughput Workloads on Container Orchestrators","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096204578&partnerID=40&md5=eee47c44c33493ea4a952ae38ea4f23a","High-throughput computing (HTC) workloads seek to complete as many jobs as possible over a long period of time. Such workloads require efficient execution of many parallel jobs and can occupy a large number of resources for a long time. As a result, full utilization is the normal state of an HTC facility. The widespread use of container orchestrators eases the deployment of HTC frameworks across different platforms, which also provides an opportunity to scale up HTC workloads with almost infinite resources on the public cloud. However, the autoscaling mechanisms of container orchestrators are primarily designed to support latency-sensitive microservices, and result in unexpected behavior when presented with HTC workloads. In this paper, we design a feedback autoscaler, High Throughput Autoscaler (HTA), that leverages the unique characteristics of the HTC workload to autoscales the resource pools used by HTC workloads on container orchestrators. HTA takes into account a reference input, the real-time status of the jobs' queue, as well as two feedback inputs, resource consumption of jobs, and the resource initialization time of the container orchestrator. We implement HTA using the Makeflow workload manager, Work Queue job scheduler, and the Kubernetes cluster manager. We evaluate its performance on both CPU-bound and IO-bound workloads. The evaluation results show that, by using HTA, we improve resource utilization by 5.6 × with a slight increase in execution time (about 15%) for a CPU-bound workload, and shorten the workload execution time by up to 3.65× for an IO-bound workload. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"R., Cordingly, Robert; W., Shu, Wen; W.J., Lloyd, Wes James","Predicting Performance and Cost of Serverless Computing Functions with SAAF","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097651725&partnerID=40&md5=087d50ad10c26d2edd2fc2cd7ca869a8","Next generation software built for the cloud recently has embraced serverless computing platforms that use temporary infrastructure to host microservices offering building blocks for resilient, loosely coupled systems that are scalable, easy to manage, and extend. Serverless architectures enable decomposing software into independent components packaged and run using isolated containers or microVMs. This decomposition approach enables application hosting using very fine-grained cloud infrastructure enabling cost savings as deployments are billed granularly for resource use. Adoption of serverless platforms promise reduced hosting costs while achieving high availability, fault tolerance, and dynamic elasticity. These benefits are offset by pricing obfuscation, as performance variance from CPU heterogeneity, multitenancy, and provisioning variation obscure the true cost of hosting applications with serverless platforms. Where determining hosting costs for traditional VM-based application deployments simply involves accounting for the number of VMs and their uptime, predicting hosting costs for serverless applications can be far more complex. To address these challenges, we introduce the Serverless Application Analytics Framework (SAAF), a tool that allows profiling FaaS workload performance, resource utilization, and infrastructure to enable accurate performance predictions. We apply Linux CPU time accounting principles and multiple regression to estimate FaaS function runtime. We predict runtime using a series of increasingly variant compute bound workloads that execute across heterogeneous CPUs, different memory settings, and to alternate FaaS platforms evaluating our approach for 77 different scenarios. We found that the mean absolute percentage error of our runtime predictions for these scenarios was just ~3.49% resulting in an average cost error of 6.46 for 1-million FaaS function workloads averaging 150.45 in price. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"F., Rossi, Fabiana; V., Cardellini, Valeria; F., Lo Presti, Francesco","Hierarchical scaling of microservices in Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092696119&partnerID=40&md5=e62e9eae9bf8b02f3d54999ce034e8a3","In the last years, we have seen the increasing adoption of the microservice architectural style where applications satisfy user requests by invoking a set of independently deployable services. Software containers and orchestration tools, such as Kubernetes, have simplified the development and management of microservices. To manage containers' horizontal elasticity, Kubernetes uses a decentralized threshold-based policy that requires to set thresholds on system-oriented metrics (i.e., CPU utilization). This might not be well-suited to scale latency-sensitive applications, which need to express requirements in terms of response time. Moreover, being a fully decentralized solution, it may lead to frequent and uncoordinated application reconfigurations.In this paper, we present me-kube (Multi-level Elastic Kubernetes), a Kubernetes extension that introduces a hierarchical architecture for controlling the elasticity of microservice-based applications. At higher level, a centralized per-application component coordinates the run-time adaptation of subordinated distributed components, which, in turn, locally control the adaptation of each microservice. Then, we propose novel proactive and reactive hierarchical control policies, based on queuing theory. To show that me-kube provides general mechanisms, we also integrate reinforcement learning-based scaling policies. Using me-kube, we perform a large set of experiments, aimed to show the advantages of a hierarchical control over the default Kubernetes autoscaler. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"D., Khatri, Deepak; S.K., Khatri, Sunil Kumar; D., Mishra, Deepti","Potential Bottleneck and Measuring Performance of Serverless Computing: A Literature Study","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093103522&partnerID=40&md5=f8eb6ace5978e1c901891fb5f1837f99","Trending form of cloud computing is Serverless computing, where developer just needs to focus on his code rather than worrying about server management. In serverless computing, application is nothing but collection of one or more functions, written for specific business functionality, which triggers on an event. There are various cloud service providers, i.e. Amazon, Microsoft, Google, IBM, etc. who provide serverless services, on pay as you use and auto scalable solution to execute the application code as a function. The developer just needs to upload the code for execution. The performance of the serverless computing may vary due to dynamic configuration of the solution, technologies and different technology used by the service provider.This paper reviews various past and recent work in the serverless computing to identify possible bottlenecks and the scope of measuring performance of serverless computing. It will also put some light to leverage machine learning in various possible ways to do performance engineering for future research. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"M.G., Gokan Khan, Michel Gokan; J., Taheri, Javid; M.A., Khoshkholghi, Mohammad Ali; A.J., Kassler, Andreas Jürgen; C., Cartwright, Carolyn; M., Darula, Marian; S., Deng, Shuiguang","A performance modelling approach for SLA-Aware resource recommendation in cloud native network functions","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091994999&partnerID=40&md5=8b62ccd4c1d5ff148e02a6e37af9d452","Network Function Virtualization (NFV) becomes the primary driver for the evolution of 5G networks, and in recent years, Network Function Cloudification (NFC) proved to be an inevitable part of this evolution. Microservice architecture also becomes the de facto choice for designing a modern Cloud Native Network Function (CNF) due to its ability to decouple components of each CNF into multiple independently manageable microservices. Even though taking advantage of microservice architecture in designing CNFs solves specific problems, this additional granularity makes estimating resource requirements for a Production Environment (PE) a complex task and sometimes leads to an over-provisioned PE. Traditionally, performance engineers dimension each CNF within a Service Function Chain (SFC) in a smaller Performance Testing Environment (PTE) through a series of performance benchmarks. Then, considering the Quality of Service (QoS) constraints of a Service Provider (SP) that are guaranteed in the Service Level Agreement (SLA), they estimate the required resources to set up the PE. In this paper, we used a machine learning approach to model the impact of each microservice's resource configuration (i.e., CPU and memory) on the QoS metrics (i.e. serving throughput and latency) of each SFC in a PTE. Then, considering an SP's Service Level Objectives (SLO), we proposed an algorithm to predict each microservice's resource capacities in a PE. We evaluated the accuracy of our prediction on a prototype of a cloud native 5G Home Subscriber Server (HSS). Our model showed 95%-78% accuracy in a PE that has 2-5 times more computing resources than the PTE. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"S., Oyucu, Saadin; H., Polat, Huseyin; H., Sever, Hayri","Web Service-Based Turkish Automatic Speech Recognition Platform","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089680231&partnerID=40&md5=2a001436596cd7e1b49c3a1a7312439b","In response to the similar challenges in building large-scale distributed applications and platforms on the Web, microservice architecture has emerged and gained a lot of popularity in recent years. Therefore, both for the use of microservices and for the provided of the necessary interface for Automatic Speech Recognition (ASR), a web-based platform has been developed. Within firstly the scope of the study, a Turkish ASR system was developed. A web service structure was created to facilitate access to the ASR system. The access of methods and data in the web service structure was provided through Representational State Transfer (REST) web services and service layer. An interface was developed to enable interaction with the web service. The platform was developed using a combination of different technologies such as ASR, web services, microservices, and interface technologies. The developed platform can be used via a standard web browser or an Application Programming Interface (API). In this study, Docker packages were used to improve system performance instead of using different virtual machines on a single server. In the experiments performed, it was shown that the Turkish ASR system had a word error rate of 24.70%. In web service performance tests, it was shown that the platform responded in an average of 9.6 seconds for a 59-second speech recording. The developed user interface was tested in both mobile and desktop web browsers and was shown to function properly. Applications and other services were given access to the platform without the need to use an interface via API support provided by the platform. As a result, a web service-based Turkish ASR platform working seamlessly on the ever-increasing number of mobile devices, the Internet of Things ecosystem, or other access devices was developed. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"K., Castro, Klayton; L.E.M.C., Martins, Lucas E.M.C.; P., Wercelens, Polyane; R., Padilha, Rafael; I., Gervasio, Italo; F.E., Deus, Flavio E.; W.F., Giozza, William F.; R.T., De Sousa Júnior, Rafael Timóteo","Performance Evaluation of the Virtualization Environment of a Microservices-Based Payroll System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089023478&partnerID=40&md5=8fb0ab36bf19577bcc97b81f784cfe5c","For better usage of idle resources in a symmetric multiprocessing environment, cloud computing providers often exploit the boundaries of parallelism by imposing high CPU subscription rates over their virtualization systems. Moreover, unsuitable resource allocation can significantly impair the performance during intensive workloads and increase infrastructure expenditures unnecessarily. It becomes an increasing challenge when dealing with microservices architecture and container encapsulated applications, for these are approaches that add more intricacy layers to the workload scheduling over the hardware. This paper presents a case study on a public agency private cloud, which aims to evaluate the performance of hypervisors and virtual machines, that are serving a Kubernetes container orchestration cluster and running a microservices-based Payroll System during CPU-bound behavior tasks. We also propose a model for sizing the virtual environment according to the best choices for the hardware characteristics and the workload needs. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"M., Wang, Mingming; D., Zhang, Dongmei; B., Wu, Bin","A Cluster Autoscaler Based on Multiple Node Types in Kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086222331&partnerID=40&md5=94a8bbf2dd4b8678e6509cf9ddc0f684","Kubernetes, as a production-grade container orchestration system, has been used in varieties of fields on a large scale. However, as used more and more widely, the appropriate selection of auto scaling schemes in kubernetes also faces considerable challenges in their respective application fields. Based on multiple node types with image pre-loaded, this paper proposes an improved automatic scaling scheme that combines the advantages of different types of nodes in the scaling process. The results show that, compared with the default auto scaler, it can improve the performance of the system under rapid load pressure and reduce the instability within the running clusters. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"A.A., Avritzer, Alberto A.","Automated scalability assessment in devops environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086017481&partnerID=40&md5=1eefb9122f6a52c7a451245ff4d444e2","In this extended abstract, we provide an outline of the presentation planned for WOSP-C 2020. The goal of the presentation is to provide an overview of the challenges and approaches for automated scalability assessment in the context of DevOps and microservices. The focus of this presentation is on approaches that employ automated identification of performance problems because these approaches can leverage performance anti-pattern[5] detection technology. In addition, we envision extending the approach to recommend component refactoring. In our previous work[1,2] we have designed a methodology and associated tool support for the automated scalability assessment of micro-service architectures, which included the automation of all the steps required for scalability assessment. The presentation starts with an introduction to dependability, operational Profile Data, and DevOps. Specifically, we provide an overview of the state of the art in continuous performance monitoring technologies[4] that are used for obtaining operational profile data using APM tools. We then present an overview of selected approaches for production and performance testing based on the application monitoring tool (PPTAM) as introduced in [1,2]. The presentation concludes by outlining a vision for automated performance anti-pattern[5] detection. Specifically, we present the approach introduced for automated anti-pattern detection based on load testing results and profiling introduced in[6] and provide recommendations for future research. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"S., Trilles, Sergio; A., González-Pérez, Alberto; J., Huerta, Joaquín","An IoT platform based on microservices and serverless paradigms for smart farming purposes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084170463&partnerID=40&md5=67d67583f5f253f093cbfe4170452582","Nowadays, the concept of “Everything is connected to Everything” has spread to reach increasingly diverse scenarios, due to the benefits of constantly being able to know, in real-time, the status of your factory, your city, your health or your smallholding. This wide variety of scenarios creates different challenges such as the heterogeneity of IoT devices, support for large numbers of connected devices, reliable and safe systems, energy efficiency and the possibility of using this system by third-parties in other scenarios. A transversal middleware in all IoT solutions is called an IoT platform. the IoT platform is a piece of software that works like a kind of “glue” to combine platforms and orchestrate capabilities that connect devices, users and applications/services in a “cyber-physical” world. In this way, the IoT platform can help solve the challenges listed above. This paper proposes an IoT agnostic architecture, highlighting the role of the IoT platform, within a broader ecosystem of interconnected tools, aiming at increasing scalability, stability, interoperability and reusability. For that purpose, different paradigms of computing will be used, such as microservices architecture and serverless computing. Additionally, a technological proposal of the architecture, called SEnviro Connect, is presented. This proposal is validated in the IoT scenario of smart farming, where five IoT devices (SEnviro nodes) have been deployed to improve wine production. A comprehensive performance evaluation is carried out to guarantee a scalable and stable platform. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"B., Shafabakhsh, Benyamin; R., Lagerström, Robert; S., Hacks, Simon","Evaluating the impact of inter process communication in microservice architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097902929&partnerID=40&md5=40528477cb885c692ec5242fa822a608","With the substantial growth of cloud computing over the past decade, microservice architectures have gained significant popularity and have become a prevalent choice for designing cloud-based applications. Microservices based applications are distributed and each service can run on a different machine. Due to its distributed nature, one of the key challenges when designing applications is the mechanism by which services communicate with each other. There are several approaches for implementing inter process communication (IPC) in microservices; each comes with different advantages and trade-offs. While theoretical and informal comparisons exist between them, this paper has taken an experimental approach to compare and contrast the popular forms of IPC communications. Several load test scenarios have been executed to obtain quantitative data related to performance efficiency, and availability of each method. The evaluation of the experiment indicates that, although there is no universal IPC solution that can be applied in all cases, the asynchronous pattern offers various advantages over its synchronous rival. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"J.L., Berral Garcia, Josep Lluís; C., Wang, Chen; A.S., Youssef, Alaa S.","AI4DL: Mining behaviors of deep learning workloads for resource management","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091936992&partnerID=40&md5=547246052132dd244da3be55db9f4921","The more we know about the resource usage patterns of workloads, the better we can allocate resources. Here we present a methodology to discover resource usage behaviors of containers training Deep Learning (DL) models. From monitoring, we can observe repeating patterns and similitude of resource usage among containers training different DL models. The repeating patterns observed can be leveraged by the scheduler or the resource autoscaler to reduce resource fragmentation and overall resource utilization in a dedicated DL cluster. Specifically, our approach combines Conditional Restricted Boltzmann Machines (CRBMs) and clustering techniques to discover common sequences of behaviors (phases) of containers running the DL training workloads in clusters providing IBM Deep Learning Services. By studying the resource usage pattern at each phase and the typical sequences of phases among different containers, we discover a reduced set of prototypical executions representing the majority of executions. We use statistical information from each phase to refine resource provisioning by dynamically tuning the amount of resource each container requires at each phase. Evaluation of our method shows that by leveraging typical resource usage patterns, we can auto-scale containers to reduce CPU and Memory allocation by 30% compared to statistics based reactive policies, which is close to having a-priori knowledge of resource usage while fulfilling resource demand over 95% of the time. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"M., Jayasinghe, Malith; J., Chathurangani, Jayathma; G., Kuruppu, Gayal; P., Tennage, Pasindu; S., Perera, Srinath","An analysis of throughput and latency behaviours under microservice decomposition","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087036181&partnerID=40&md5=7999a4ac9002f5c9e15ff1f82a894471","Microservice architecture is a widely used architectural style which allows you to design your application using a set of loosely coupled services which can be developed, deployed, and scaled independently. The service decomposition is the act of decomposing (breaking) a coarse-grained service into a set of fine-grained services that collectively perform the functionality of the original service. The service decomposition introduces additional overhead due to inter-service communication of services which impacts the performance. In this paper, we study the effect of service decomposition on the throughput and average latency. We perform an extensive performance analysis using a set of standard microservice benchmarks with different workload characteristics. Our results indicate that when we decompose a service into a set of micro-services the performance of the new application can improve or degrade. The factors which impact the performance behaviours are the number of service calls, the service demand, concurrency (i.e. number of concurrent users) and the decomposition strategy. In addition to the experimental performance evaluation, we analyze the performance impact of service decomposition using queueing theoretic models. We compare the analytical results with experimental results and notice that analytical results match well with the experimental results. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"A.M., Potdar, Amit M.; D.G., Narayan, D. G.; S., Kengond, Shivaraj; M.M., Mulla, Mohammed Moin","Performance Evaluation of Docker Container and Virtual Machine","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086629212&partnerID=40&md5=21bd001494de81e3c0bace9764a39c6f","Server virtualization is a technological innovation broadly used in IT enterprises. Virtualization provides a platform to run different services of operating systems on the cloud. It facilitates to build multiple virtual machines on a single basic physical machine either in the form of hypervisors or containers. To host many microservice applications, the emergent technology has introduced a model which consists of different operations performed by smaller individual deployed services. Thus, the demand for low-overhead virtualization technique is rapidly developing. There are many lightweight virtualization technologies; docker is one among them, which is an open-source platform. This technology allows developers and system admins to build, create, and run applications using docker engine. This paper provides the performance evaluation of Docker containers and virtual machines using standard benchmark tools such as Sysbench, Phoronix, and Apache benchmark, which include CPU performance, Memory throughput, Storage read/write performance, load test, and operation speed measurement. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"N.D., Keni, Nishant Deepak; A., Kak, Ahan","Adaptive Containerization for Microservices in Distributed Cloud Systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085527910&partnerID=40&md5=2923cd278cbfc180efaa6fe0b0396d48","The traditional monolithic on-premises model of application deployment is fast being replaced by a cloud-based microservices paradigm, driven in part by the rise of numerous cloud infrastructure providers providing seamless access to a variety of computing hardware, and the need for applications to serve an ever-increasing audience necessitating scalability. While container-based virtualization has been the preferred method of microservice deployment, Cloud Consumers have not had much opportunity for cost and resource optimization thus far. To this end, this paper introduces a resource allocation framework for the containerized deployment of microservices, called Adaptive Containerization for Microservices in Distributed Cloud Systems, which helps reduce operating costs while ensuring a minimum guaranteed level of service. Further, a variety of performance evaluation metrics have been provided to reinforce the validity of the proposed framework. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"A.S., Mattoo, Asif Showkat; D., Upadhyay, Divya; A.K., Dubey, Ashwani Kumar; M.K., Shukla, Manoj Kumar","An approach to analyse and protect data on untrusted cloud network","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083994764&partnerID=40&md5=8bb21cc0247a198bf7eaa6c04789d7f4","The outsourcing of data has gradually increased with the development of cloud computing environment to provide features like pay as you go and auto scaling. This helps small and medium size vendors to expand their business by migrating to cloud. This expansion needs to address important security concern for its customers including data privacy and vulnerability towards the cyber-attacks. One such attack: insider attack continues to worsen the situation of security affecting the confidentiality, integrity and availability of data. This paper reviews different security frameworks designed to detect and prevent abnormalities in the cloud infrastructure. It analyzes two important security hypervisors: TrustVisor and Flicker. Both hypervisors are responsible for providing the information isolation and integrity using piece of logic (PAL) mechanism on the devices where operating system, BIOS and dynamic memory allocation are not recommended reliable. The analysis was done for the execution environment using TPM Remote Attestation, Secure VM launch, Memory Seal Storage based on Storage Root Key (SRK). In the end this paper presents a comparative performance evaluation metrics, for TrustVisor and Flicker hypervisors using eucalyptus cloud software for private cloud. Through this research it was observed that TrustVisor provides high information isolation and integrity as compared to Flicker hypervisor. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"J., Park, Joonseok; D., Kim, Daeho; K., Yeom, Keunhyuk","An Approach for Reconstructing Applications to Develop Container-Based Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079294972&partnerID=40&md5=f568557f77101c28ee36f1a3c98501b9","Microservices are small-scale services that can operate independently. An application consisting of microservice units can be developed independently as a service unit, and it can handle individual logic without being affected by other services. In addition, it is possible to rapidly distribute the configured microservices by a container, and a container orchestration technology that manages the distributed multiple containers can be realized; thus, it is possible to update and distribute the microservices separately. Therefore, many companies are moving away from existing monolithic structures and attempting to switch to microservices. In this paper, we present a method for reconstructing a monolithic application into a container-based microservice unit. The microservices of data units are derived through the collection and analysis of monolithic design data. Furthermore, we propose a method to generate a template script based on deployment design data so that the derived microservice and support distribution can be implemented in a container environment. The results of a case study conducted verified that the container-based microservices deployed in this study work properly. In addition, for the development of monolithic applications and the development of container-based microservices presented in this paper, we confirmed that developing on the basis of microservices is efficient by conducting execution time performance evaluation for API calls at various iterations. Finally, we show that microservices constructed using the proposed method have higher reusability than those constructed using existing methods. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"R.B., Ross, Robert B.; G., Amvrosiadis, George; P.H., Carns, Philip H.; C.D., Cranor, Charles D.; M., Dorier, Matthieu; K., Harms, Kevin; G.R., Ganger, Gregory R.; G.A., Gibson, Garth A.; S.K., Gutierrez, Samuel K.; R., Latham, Rob","Mochi: Composing Data Services for High-Performance Computing Environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078477229&partnerID=40&md5=c969a5ab44cc5e5a7f68abb63d271297","Technology enhancements and the growing breadth of application workflows running on high-performance computing (HPC) platforms drive the development of new data services that provide high performance on these new platforms, provide capable and productive interfaces and abstractions for a variety of applications, and are readily adapted when new technologies are deployed. The Mochi framework enables composition of specialized distributed data services from a collection of connectable modules and subservices. Rather than forcing all applications to use a one-size-fits-all data staging and I/O software configuration, Mochi allows each application to use a data service specialized to its needs and access patterns. This paper introduces the Mochi framework and methodology. The Mochi core components and microservices are described. Examples of the application of the Mochi methodology to the development of four specialized services are detailed. Finally, a performance evaluation of a Mochi core component, a Mochi microservice, and a composed service providing an object model is performed. The paper concludes by positioning Mochi relative to related work in the HPC space and indicating directions for future work. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"S., Verreydt, Stef; E., Heydari Beni, Emad; E., Truyen, Eddy; B., Lagaisse, Bert; W., Joosen, Wouter","Leveraging Kubernetes for adaptive and cost-efficient resource management","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078873938&partnerID=40&md5=fe674dbb1ed7dedcbc5831a8e1e0d839","Software providers face the challenge of minimizing the amount of resources used while still meeting their customer’s requirements. Several frameworks to manage resources and applications in a distributed environment are available, but their development is still ongoing and the state of the art is rapidly evolving, making it a challenge to use such frameworks and their features effectively in practice. The goal of this paper is to research how applications can be enhanced with adaptive performance management by relying on the capabilities of Kubernetes, a popular framework for container orchestration. In particular, horizontal as well as vertical scaling concepts of Kubernetes may prove useful to support adaptive resource allocation. Moreover, concepts for oversubscription as a way to simulate vertical scaling without having to reschedule applications, are evaluated. Through a series of experiments involving multiple applications and workloads, the effects of different configurations and combinations of horizontal and vertical scaling in Kubernetes are explored. Both the resource utilization of the nodes and the applications’ performance are taken into account. In brief, the resource management concepts of Kubernetes allow to simulate vertical scaling without a negative effect on performance. The effectiveness of the default horizontal autoscaler, however, depends on the type of application and the user workload at hand. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"K., Dumkasem, Kanlayanee; P., Srisingchai, Padchaya; P., Rattanatamrong, Prapaporn","EyeMath: Increasing Accessibility of Mathematics to Visually Impaired Readers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081656218&partnerID=40&md5=5d3973cb9495d0418030a012dc2f4ee2","Mathematics education for visually impaired students is challenging because their learning materials are generally limited to braille books, and audiobooks. In order to increase the chance of learning mathematical content for people with visual impairment, this paper presents the design and development of a cloud-based mobile application called EyeMath, using serverless microservices in Amazon AWS. Users can provide images of page snippets for the application to process and read their content to the users. EyeMath segments an input image into smaller pieces and separates pieces that have only plain text from pieces with mathematical symbols. The mathematical-related pieces are further processed into an Abstract Syntax Tree (AST) and then parsed into Thai sentences. For plain text pieces, EyeMath relies on Tesseract OCR to convert them into text. Finally, results for all pieces are combined together systematically for the device's screen reader program to read aloud. The performance evaluation of the application shows high correctness in reading math content within test images and our usability testing confirms the potential usefulness of the application to visually impaired readers. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"A.A., Janes, Andrea A.; B., Russo, Barbara","Automatic performance monitoring and regression testing during the transition from monolith to microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080961994&partnerID=40&md5=d68a5b2b9988e1d43da7fc294e80c6ad","The transition from monolith to microservices poses several challenges, like how to redistribute the features of system over different microservices. During the transition, developers may also redesign or rethink system services significantly, which can have a strong impact on various quality aspects of the resulting system. Thus, the new system may be more or less performing depending on the ability of the developers to design microservices and the capability of the microservice architecture to represent the system. Overall, a transition to microservices may or may not end up with the same or a better performing system. One way to control the migration to microservices is to continuously monitor a system by continuously collecting performance data and feeding the resulting data analysis back in the transition process. In DevOps, such continuous feedback can be exploited to re-Tune the development and deployment of system's builds. In this paper, we present PPTAM+, a tool to continuously assess the degradation of a system during a transition to microservices. In an in-production system, the tool can continuously monitor each microservice and provide indications of lost performance and overall degradation. The system is designed to be integrated in a DevOps process. The tool automates the whole process from collecting data for building the reference operational profile to streamline performance data and automatically adapt and regress performance tests on each build based the analysis' feedback obtained from tests of the previous build. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"T., Tournaire, Thomas; H., Castel-Taleb, Hind; E., Hyon, Emmanuel; T., Hoché, Toussaint","Generating optimal thresholds in a hysteresis queue: Application to a cloud model","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077796321&partnerID=40&md5=06712532dea10dd8cef272c4624578e4","Reducing the energy consumption of a cloud system while guaranteeing a given quality of service level is a crucial problem encountered today by cloud providers. We consider an auto-scaling model where virtual machines are turned on and off depending on the queue's occupation (or thresholds). This model represents the variability of allocated resources (Virtual Machines or VMs) according to user demands. It can be studied using an hysteresis queuing model, which is represented by a multidimensional Markov chain, whose calculation of the stationary distribution becomes complex when the number of VMs grows. We adopt a cost-aware approach and define a mean cost computed as a reward function on the stationary distribution. This cost takes into account both the performance (for Service Level Agreement: SLA) and the use of the resources (for Energy). We propose efficient optimisation methods to find threshold values minimising the global cost. Because this mean cost is a non-convex function, the research of the optimal value is complex. We propose different optimisation methods: the first one, based on heuristics, coupled with aggregation of the Markov Chain to reduce the execution time and the second one which is a meta heuristic: the Simulated Annealing. Finally, we present a real case of a cloud system that we model and set parameter values to test our optimisation algorithms and show their relevance. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"T.F., da Silva Pinheiro, Thiago Felipe; F.A., Silva, Francisco Airton; I.D.S., Fe, Iure De Sousa; D.M., Oliveira, Danilo Mendonça; P.R.M., MacIel, Paulo Romero Martins","Performance and resource consumption analysis of elastic systems on public clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076751047&partnerID=40&md5=b3b123cdc91c4bb0124424db2ce49ac9","Service providers may build elastic systems on public clouds. The public cloud may offer economies of scale, but there are some considerations to take into account. Infrastructure-as-a-Service (IaaS) providers charge their customers by the use of virtual machines (VMs), and wrong deployment decisions may lead to financial losses. This paper proposes an approach for estimating systems' performance, use of VM instances and its related costs on a public cloud. This work proposes a Stochastic Petri Net (SPN)-based formal modeling strategy to represent elastic systems deployed on a public cloud and a cost model to predict the use of VM instances. The approach enables designers to plan and tune elastic architectures based on Mean Response Time (MRT) estimation. Using our strategy it is possible to estimate the impact of each deployment configuration on the evaluated metrics. Our modeling strategy considers reactive scaling policies. The model represents a remote infrastructure for supporting external users requests. By combining different instance types, simultaneous jobs per VM instance, stepsizes and scaling thresholds, it is possible to offer different response times for each deployment configuration. One case study was performed to evaluate the approach. Our approach has proven to be feasible and it highlights the most effective scenarios to minimize MRT and reduce costs. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"E., Casalicchio, Emiliano","A study on performance measures for auto-scaling CPU-intensive containerized applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059669161&partnerID=40&md5=46908eb4c08f031a1f98783d4ad8e2df","Autoscaling of containers can leverage performance measures from the different layers of the computational stack. This paper investigate the problem of selecting the most appropriate performance measure to activate auto-scaling actions aiming at guaranteeing QoS constraints. First, the correlation between absolute and relative usage measures and how a resource allocation decision can be influenced by them is analyzed in different workload scenarios. Absolute and relative measures could assume quite different values. The former account for the actual utilization of resources in the host system, while the latter account for the share that each container has of the resources used. Then, the performance of a variant of Kubernetes’ auto-scaling algorithm, that transparently uses the absolute usage measures to scale-in/out containers, is evaluated through a wide set of experiments. Finally, a detailed analysis of the state-of-the-art is presented. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"S., Sussi, Sussi; R.M., Negara, Ridha Muldina; Z., Triartono, Zehan","Implementation of role-based access control on OAuth 2.0 as authentication and authorization system","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079746450&partnerID=40&md5=22c7f4a662646981340f16a13a85adcd","As today’s technology transition from monolithic towards microservices architecture, the authentication and authorization system also becomes a new concern because of the difference between monolithic and microservices pattern. Monolithic mostly uses role-based access control while microservices uses scope with OAuth 2.0. With this in mind, there is a need for a model that can integrate OAuth 2.0 with role-based access control. With role-based access control implemented on OAuth 2.0, we expect a simpler authorization process and a more secure authentication and authorization system for microservices backend architecture. This paper proposes a model to implement role-based access control on OAuth 2.0 using Laravel framework, we also test the performance of the system following by response time, data transferred and throughput. From the performance test, this approach has a good performance and can handle certain requests with simulated users even with limited resources. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"G., Rattihalli, Gourav; M., Govindaraju, Madhusudhan; H., Lu, Hui; D., Tiwari, Devesh","Exploring potential for non-disruptive vertical auto scaling and resource estimation in kubernetes","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072324127&partnerID=40&md5=629506d5a33d0ef0fe336247da666662","Cloud platforms typically require users to provide resource requirements for applications so that resource managers can schedule containers with adequate allocations. However, the requirements for container resources often depend on numerous factors such as application input parameters, optimization flags, input files, and attributes that are specified for each run. So, it is complex for users to estimate the resource requirements for a given container accurately, leading to resource over-estimation that negatively affects overall utilization. We have designed a Resource Utilization Based Autoscaling System (RUBAS) that can dynamically adjust the allocation of containers running in a Kubernetes cluster. RUBAS improves upon the Kubernetes Vertical Pod Autoscaler (VPA) system non-disruptively by incorporating container migration. Our experiments use multiple scientific benchmarks. We analyze the allocation pattern of RUBAS with Kubernetes VPA. We compare the performance of container migration for in-place and remote node migration and we evaluate the overhead in RUBAS. Our results show that compared to Kubernetes VPA, RUBAS improves the CPU and memory utilization of the cluster by 10% and reduces the runtime by 15% with an overhead for each application ranging from 5% to 20%. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"E., Markoska, Elena; S., Lazarova-Molnar, Sanja","LEAF: Live building performance evaluation framework","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071680833&partnerID=40&md5=841f4b5f0ecaae55424c8f7a61298600","Buildings contribute to approx. 32% of the world's energy consumption, and as such, are one of the prime producers of CO<inf>2</inf> emissions. Given documented discrepancies between the designed and operational behaviour of buildings, concepts such as continuous commissioning and performance testing have emerged. Performance testing utilises metadata schemas and generic libraries of tests to increase applicability and customizability of real-time performance monitoring. In this paper we present LEAF: A Live building performance EvAluation Framework. LEAF has been developed as a result of an extensive survey with building management professionals, who gave insight into the preferences for management of various buildings. We present LEAF's microservice-style architecture, to increase reusability of its processes by other building intelligence applications, and use it to develop a performance monitoring application for a case study building. Furthermore, the live streaming processing of building operational data has posed significant challenges to the application of LEAF. We discuss these challenges and offer directions for their solutions. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"G., Cornetta, Gianluca; F.J., Mateos, Francisco Javier; A., Touhafi, Abdellah; G.M., Muntean, Gabriel Miro","Modelling and simulation of a cloud platform for sharing distributed digital fabrication resources","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069913836&partnerID=40&md5=700789eb13598c26116809ecac0c5464","Fabrication as a Service (FaaS) is a new concept developed within the framework of the NEWTON Horizon 2020 project. It is aimed at empowering digital fabrication laboratories (Fab Labs) by providing hardware and software wrappers to expose numerically-controlled expensive fabrication equipment as web services. More specifically, FaaS leverages cloud and IoT technologies to enable a wide learning community to have remote access to these labs’ computer-controlled tools and equipment over the Internet. In such context, the fabrication machines can be seen as networked resources distributed over a wide geographical area. These resources can communicate through machine-to-machine protocols and a centralized cloud infrastructure and can be digitally monitored and controlled through programmatic interfaces relying on REST APIs. This paper introduces FaaS in the context of Fab Lab challenges and describes FaaS deployment within NEWTON Fab Labs, part of the NEWTON European Horizon 2020 project on technology enhanced learning. The NEWTON Fab Labs architecture is described in detail targeting software, hardware and network architecture. The system has been extensively load-tested simulating real use-case scenarios and it is presently in production. In particular, this paper shows how the measured data has been used to build a simulation model to estimate system performance and identify possible bottlenecks. The measurements performed show that the platform delays exhibit a tail distribution with Pareto-like behaviour; this finding has been used to build a simple mathematical model and a simulator on top of CloudSim to estimate the latencies of the critical paths of the NEWTON Fab Lab platform under several load conditions. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"V., Podolskiy, Vladimir; A., Jindal, Anshul; M., Gerndt, Michael","Multilayered Autoscaling Performance Evaluation: Can Virtual Machines and Containers Co-Scale?","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069664045&partnerID=40&md5=996fd3c6152c28a9ff68d8914ecfe26d","The wide adoption of cloud computing by businesses is due to several reasons, among which the elasticity of the cloud virtual infrastructure is the definite leader. Container technology allows increasing the flexibility of an application by adding another layer of virtualization. The containers can be dynamically created and terminated, and also moved from one host to another. A company can achieve a significant cost reduction and increase the manageability of its applications by allowing the running of containerized microservice applications in the cloud. Scaling for such solutions is conducted on both the virtual infrastructure layer and the container layer. Scaling on both layers needs to be synchronized so that, for example, the virtual machine is not terminated with containers still running on it. The synchronization between layers is enabled by multilayered cooperative scaling, implying that the autoscaling solution of the virtual infrastructure layers is aware of the decisions of the autoscaling solution on the container layer and vice versa. In this paper, we introduce the notion of cooperative multilayered scaling and the performance of multilayered autoscaling solutions evaluated using the approach implemented in ScaleX (previously known as Autoscaling Performance Measurement Tool, APMT). We provide the results of the experimental evaluation of multilayered autoscaling performance for the combination of virtual infrastructure autoscaling of AWS, Microsoft Azure and Google Compute Engine with pods horizontal autoscaling of Kubernetes by using ScaleX with four distinct load patterns. We also discuss the effect of the Docker container image size and its pulling policy on the scaling performance. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"F., Willnecker, Felix; H.A.O., Krcmar, Helmut A.O.","Model-based prediction of automatic memory management and garbage collection behavior","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054085138&partnerID=40&md5=45a763f3e6e380396b0280c510accc91","Performance models focus on resource consumption and the effects of CPU, network, or hard-disk utilization. These resources usually have the largest effect on the response times and throughput of an application. However, deficient memory management can have severe effects on an application and its runtime, such as overlong response times or even crashes. As memory management has been disregarded in performance simulations, we address this gap with an approach based on memory measurements and derived metrics to predict the behavior of this resource and the effects on the CPU. Although numerous works exist that analyze memory management and especially garbage collections, accurate prediction models are rare. We demonstrate the automatic extraction of memory behavior using a performance model generator. Furthermore, the approach is evaluated using the SPECjEnterprise2010 and the SPECjEnterpriseNEXT industry benchmark, using different resource environments, garbage collection algorithms, and workloads. This work demonstrates that a certain set of probabilities allows one to create a memory profile for an architecture and predict the behavior of the memory management. The results of such predictions can be used for better capacity planning (on-premise), cost-prediction (cloud), architecture evaluation and optimization, or memory profiling. This approach allows for a continuous model-based evaluation of an enterprise architecture regarding its memory footprint. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"T., Kiss, Tamás; P., Kacsuk, Peter; J., Kovacs, Jozsef; B., Rakoczi, Botond; Á., Hajnal, Ákos; A., Farkas, Attila; G., Gesmier, Gregoire; G.Z., Terstyánszky, Gábor Z.","MiCADO—Microservice-based Cloud Application-level Dynamic Orchestrator","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030791545&partnerID=40&md5=f33359094cf8f18d3d3470aff6c08db9","Various scientific and commercial applications require automated scalability and orchestration on cloud computing resources. However, extending applications with such automated scalability on an individual basis is not feasible. This paper investigates how such automated orchestration can be added to cloud applications without major reengineering of the application code. We suggest a generic architecture for an application level cloud orchestration framework, called MiCADO that supports various application scenarios on multiple heterogeneous federated clouds. Besides the generic architecture description, the paper also presents the first MiCADO reference implementation, and explains how the scalability of the Data Avenue service that is applied for data transfer in WS-PGRADE/gUSE based science gateways, can be improved. Performance evaluation of the implemented scalability based on up and downscaling experiments is presented. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"D., Mendes, Diogo; D., Jorge, Dário; G., Pires, Gabriel; R., Panda, Renato; R., Antonio, Ricardo; P.M.A., Dias, Pedro Miguel Aparício; L.M., Oliveira, Luís M.L.","VITASENIOR-MT: A distributed and scalable cloud-based telehealth solution","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073907165&partnerID=40&md5=386927d5618f255a57a154d0dad66a60","VITASENIOR-MT is a telehealth platform that allows to remotely monitor biometric and environmental data in a domestic environment, designed specifically to the elderly population. This paper proposes a highly scalable and efficient architecture to transport, process, store and visualize the data collected by devices of an Internet of Things (IoT) scenario. The cloud infrastructure follows a microservices architecture to provide computational scalability, better fault isolation, easy integration and automatic deployment. This solution is complemented with a pre-processing and validation of the collected data at the edge of the Internet by using the Fog Computing concept, allowing a better computing distribution. The presented approach provides personal data security and a simplified way to collect and present the data to the different actors, allowing a dynamic and intuitive management of patients and equipment to caregivers. The presented load tests proved that this solution is more efficient than a monolithic approach, promoting better access and control in the data flowing from heterogeneous equipment. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Triartono, Zehan; R., Muldina, Ridha; S., Sussi, Sussi","Implementation of role-based access control on oauth 2.0 as authentication and authorization system","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086781950&partnerID=40&md5=0ff0f8e37f47144364ac6796d46eef6d","As today’s technology transition from monolithic towards microservices architecture, the authentication and authorization system also becomes a new concern because of the difference between monolithic and microservices pattern. Monolithic mostly uses role-based access control while microservices uses scope with OAuth 2.0. With this in mind, there is a need for a model that can integrate OAuth 2.0 with role-based access control. With rolebased access control implemented on OAuth 2.0, we expect a simpler authorization process and a more secure authentication and authorization system for microservices backend architecture. This paper proposes a model to implement role-based access control on OAuth 2.0 using Laravel framework, we also test the performance of the system following by response time, data transferred and throughput. From the performance test, this approach has a good performance and can handle certain requests with simulated users even with limited resources. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"N.A., Sani, Nisfu Asrul; W.A., Fillah, Wildan Azka; A., Tjahyanto, Aris; H., Suryotrisongko, Hatma","Development of microservice based application e-inkubator: Incubation and investment service provider for SMEs","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078943384&partnerID=40&md5=e822c40e3d68c5f7304cfbde7ad92fa5","Information Technology had significant influenced in business without exception for small and medium enterprises (SMEs). Many SMEs has used software or system. This research aims to study one of SME's need in increasing their growth through business incubator. Previous research implementation produces E-Inkubator, an incubation and investment service provider platform. This application still used monolithic architecture, by the time users were increased cause response time of that application decrease. This application also cannot be integrated with other existing system in governments or enterprises as an investor in order to make easier investment process. Increasing number of users, response time, and integrity issues can be solved by implement microservice architecture, in this paper we apply. Microservice concept as loosely coupled solution. It makes easier third party to access service from this application. By separating services this framework also makes request per second from application faster toward many requests. This paper will cover design, development, and evaluate E-Inkubator application that based on microservice architecture. By comparing monolithic against microservice architecture, we show the results of performance tests executed on both applications and describe that microservice is cheaper and make easier third party's system to access services. In addition, the result shows that microservice architecture make request per second 2.5 higher than monolithic architecture. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"O., Abu Oun, Osama; T., Kiss, Tamás","Job-queuing and auto-scaling in container-based cloud environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065540263&partnerID=40&md5=abac4dd289823214cbce5ab18847ac96","Many applications process large quantities of data that takes significant time and requires big amount of computational resources. Optimising the execution of such applications in a cloud computing environment by keeping costs at minimum but still completing the task by a set deadline has paramount importance. As container-based technologies are becoming more widespread, support for job-queuing and auto-scaling in such environments is becoming important. Current container technologies, such as Docker or Kubernetes provide limited support in this area. This paper presents JQueuer and CAutoScaler, a couple of cloud-independent solutions that offer job-queuing and automated scalability at the level of containers. Applying these solutions leads to more cloud-aware applications providing transparent auto-scaling for end-users and optimising execution time and costs. Business and science gateways will benefit from using an orchestrator combined with JQueuer and CAutoScaler since it will provide the layers needed to auto-scale the containers and to batch/sweep the jobs from a queue depending on a userdefined policy. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"J., Navarro-Ortiz, Jorge; J.J., Ramos-Munoz, Juan J.; J.M., Lopez-Soler, Juan M.; C., Cervelló-Pastor, Cristina; M., Catalan, M.","A LoRaWAN Testbed Design for Supporting Critical Situations: Prototype and Evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062612373&partnerID=40&md5=f3b08ef39a2f5085d7cd4a5c7640f52b","The Internet of Things is one of the hottest topics in communications today, with current revenues of $151B, around 7 billion connected devices, and an unprecedented growth expected for next years. A massive number of sensors and actuators are expected to emerge, requiring new wireless technologies that can extend their battery life and can cover large areas. LoRaWAN is one of the most outstanding technologies which fulfill these demands, attracting the attention of both academia and industry. In this paper, the design of a LoRaWAN testbed to support critical situations, such as emergency scenarios or natural disasters, is proposed. This self-healing LoRaWAN network architecture will provide resilience when part of the equipment in the core network may become faulty. This resilience is achieved by virtualizing and properly orchestrating the different network entities. Different options have been designed and implemented as real prototypes. Based on our performance evaluation, we claim that the usage of microservice orchestration with several replicas of the LoRaWAN network entities and a load balancer produces an almost seamless recovery which makes it a proper solution to recover after a system crash caused by any catastrophic event. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"N.B., Shah, Namra Bhadreshkumar; T.C., Thakkar, Tirth Chetankumar; S.M., Raval, Shrey Manish; H., Trivedi, Harshal","Adaptive live task migration in cloud environment for significant disaster prevention and cost reduction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059694801&partnerID=40&md5=1325d7a679d23e9ee0732a853234f2d5","The “Cloud” in IT terms is straightforwardly a storage of data. Cloud computing is one of the most emerging technologies in IT industries as of late and it means to store and manage data persistently over the cloud (the Internet) at a very low cost. Migration to and among cloud servers helps IT professionals to protect their data, prevent them from any disasters, and provide their resources efficiently without any delay or problems. Auto-scaling provides agility in managing virtual machines, whether to increase or decrease them. Any successful prevention of disaster will necessarily depend on the migration of certain tasks from one virtual machine to another. Most of the data recovery approaches suffer from high recovery time, balancing load and to cut cost. In this work, we incorporated an adaptive live task migration technique to prevent as many disasters as possible and to significantly reduce cost which is presented in the form of a graph later in the performance evaluation section. The experimental outcome shows that the proposed algorithm outperforms other approaches by 15–25% in terms of reducing cost, and balancing the load among available nodes. It also diminishes any prospect of disaster. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"A., Saransig, Alexis; F.M.L., Tapia, Freddy M.L.","Performance analysis of monolithic and micro service architectures – containers technology","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054791089&partnerID=40&md5=7a4827c643c416f9d958ca1af3d784c1","Comparative analysis of the performance of hardware resources, between Monolithic Architecture and Micro services Architecture, using virtualization technology based on development and production environments. Today, the new trend is the development and/or deployment of applications in the Cloud, in this aspect, monolithic applications have flexibility, scalability, maintainability and performance limitations. On the other hand, the focus of Microservices adapts to new trends and solves these limitations. Meanwhile, virtualization with virtual machines is currently not efficient enough with hardware resources. With the appearance of containers, this problem is solved due to its functioning characteristics as independent processes and resources optimization. Now, two scenarios are presented, the first consisting of a Web application based on a Monolithic Architecture that is executed in a Kernel based Virtual Machine - KVM and the second scenario shows the same Web application, this time, based on a Micro services Architecture and running in containers. Each scenario is subjected to the same stress tests; the generated data are recorded in “log” files for further analysis. The hardware resources are the same for both scenarios. The comparison of these scenarios helps to identify the efficiency of the Application and the hardware resources, as well as the development and/or deployment of Applications. This can be improved with the use of Microservices and Containers. In addition, the reduction of costs that would imply the optimization in the resources. For greater reliability in the interpretation of the data, two analysis tools were used: JMeter and NewRelic. Finally, the two resulting cases from the analysis are shown, each case being considered due to the feasibility of the same depending on the needs and availability of resources. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"K., Lillaney, Kunal; D.M., Kleissas, Dean Mark; A., Eusman, Alexander; E.A., Perlman, Eric A.; W.R., Gray-Roncal, William Roberts; J.T., Vogelstein, Joshua T.; R.C., Burns, Randal C.","Building NDStore through hierarchical storage management and microservice processing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061373633&partnerID=40&md5=5278e0cd811977d74da0255ba16e8cf2","We describe NDStore, a scalable multi-hierarchical data storage deployment for spatial analysis of neuroscience data on the AWS cloud. The system design is inspired by the requirement to maintain high I/O throughput for workloads that build neural connectivity maps of the brain from peta-scale imaging data using computer vision algorithms. We store all our data on the AWS object store S3 to limit our deployment costs. S3 serves as our base-tier of storage. Redis, an in-memory key-value engine, is used as our caching tier. The data is dynamically moved between the different storage tiers based on user access. All programming interfaces to this system are RESTful web-services. We include a performance evaluation that shows that our production system provides good performance for a variety of workloads by combining the assets of multiple cloud services. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"T., Goethals, Tom; M., Sebrechts, Merlijn; A., Atrey, Ankita; B., Volckaert, Bruno; F., de Turck, Filip","Unikernels vs containers: An in-depth benchmarking study in the context of microservice applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060246102&partnerID=40&md5=46d69a37be731035572bc529df916606","Unikernels are a relatively recent way to create and quickly deploy extremely small virtual machines that do not require as much functional and operational software overhead as containers or virtual machines by leaving out unnecessary parts. This paradigm aims to replace bulky virtual machines on one hand, and to open up new classes of hardware for virtualization and networking applications on the other. In recent years, the tool chains used to create unikernels have grown from proof of concept to platforms that can run both new and existing software written in various programming languages. This paper studies the performance (both execution time and memory footprint) of unikernels versus Docker containers in the context of REST services and heavy processing workloads, written in Java, Go, and Python. With the results of the performance evaluations, predictions can be made about which cases could benefit from the use of unikernels over containers. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"O., Al-Debagy, Omar; P., Martinek, Péter","A Comparative Review of Microservices and Monolithic Architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077783518&partnerID=40&md5=500ecb54ad9c017fdc93b25eaba45e40","Microservices' architecture is getting attention in the academic community and the industry, and mostly is compared with monolithic architecture. Plenty of the results of these research papers contradict each other regarding the performance of these architectures. Therefore, these two architectures are compared in this paper, and some specific configurations of microservices' applications are evaluated as well in the term of service discovery. Monolithic architecture in concurrency testing showed better performance in throughput by 6% when compared to microservices architecture. The load testing scenario did not present significant difference between the two architectures. Furthermore, a third test comparing microservices applications built with different service discovery technologies such as Consul and Eureka showed that applications with Consul presented better results in terms of throughput. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"D., Gedia, Dewang; L., Perigo, Levi","Performance Evaluation of SDN-VNF in Virtual Machine and Container","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067581972&partnerID=40&md5=40081b6e05ca7ef7f86c3016e61e16a0","Discrete non-virtualized network elements are characterized by large costs, limited functions, vendor lock-in, and limited orchestration. Network Function Virtualization (NFV) changes the way of creating, deploying, and operating networks by decomposing hardware elements into software components that run on virtualized servers. Two virtualization technologies - Virtual Machines (VM) and containers - have the capability to host Virtual Network Functions (VNFs). These virtualized solutions offer varying results when compared with VNF provisioning time, runtime performance, throughput, and portability depending on the VNF application. Although prior research has identified these results, they lack evaluation of a VNF serving as a Software Defined Network (SDN) controller. In SDN, these parameters serve as a vital criterion for selecting an optimum virtualized solution for hosting an SDN controller, which serves as the control plane for the underlying infrastructure layer. This research paper aims to evaluate an ONOS SDN controller application in a Docker container environment versus a VM environment per the NFV Research Group (NFVRG) testing guidelines. This will help an operator identify an optimum platform to host SDN-VNF microservice. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"F., Klinaku, Floriment; V., Ferme, Vincenzo","Towards generating elastic microservices: A declarative specification for consistent elasticity configurations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057204483&partnerID=40&md5=2c647cb08ac308a0dd21f866a8f63c32","The adoption of Microservice architectures deployed using containers has increased over the last years. This deployment and operation stack brings new challenges in exploiting Cloud elasticity. Currently, the generation and maintenance of configuration rules that specify when and how to scale containers are human-driven - thus error-prone - and wholly decoupled from continuous delivery pipelines. Considering frequent changes, the deployed Microservice might diverge from its elasticity configuration rules which either inquires unforeseen costs or degrades its performance. We tackle this problem by leveraging declarative performance approaches to keep elasticity configuration artefacts aligned with the deployment and their requirements through the entire software development life-cycle. In this paper we propose an extension of a state of the art approach for declarative performance test definition, to enable practitioners to continuously and automatically obtain feedback on Microservices elasticity, to be used to update scalability rules for the given workload and context. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"W., Chiang, Weikuo; J.W., Wen, Juin Wei","Design and Experiment of NFV-Based Virtualized IP Multimedia Subsystem","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054821764&partnerID=40&md5=dc99ec1959b0206d12d4b6f2ffe3f289","IP multimedia subsystem (IMS) has been recognized as the core control platform for the next generation network (NGN). In this paper, we focus on virtualizing IMS by enabling network function virtualization (NFV). NFV virtualizes network functions into software applications. The virtualized network functions can be decomposed into smaller functional blocks which will enable a high level of elasticity. This paper proposes an NFV-based virtualized IMS (vIMS) architecture that decomposes the S-CSCF into two key functions: One handles the UE registration (registration; R) and the other processes the call control (call control; C). Then the original I-CSCF is combined with the registration function (R), denoted by vI-CSCF(R). The S-CSCF reserves the call control function (C) only, denoted by vS-CSCF(C). We also design the registration procedure and call setup procedure in the vIMS. The vI-CSCF(R) is responsible for the original I-CSCF function and UE registration; thus, the vS-CSCF(C) is offloaded without handling any UE registration message. In addition, we analyze the queuing delay time of registration and call setup in the NFV-based vIMS architecture. Compared with the straightforward solution for IMS virtualization, our proposed vIMS has shorter delay time of registration and call setup by improving the S-CSCF's performance. In order to verify the feasibility of our proposal, we developed and deployed our proposed vIMS and the Clearwater-vIMS with OpenStack and Clearwater software, and run the stress test on them respectively. Compared to the original Clearwater-vIMS, our proposed vIMS have higher registration success rate and call setup success rate. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"V., Podolskiy, Vladimir; A., Jindal, Anshul; M., Gerndt, Michael","IaaS Reactive Autoscaling Performance Challenges","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057466813&partnerID=40&md5=1433fa72d4f5cf4784add32c0ba374f4","The main feature of a cloud application is its scalability. Major IaaS cloud services providers (CSP) employ autoscaling on the level of virtual machines (VM). Other virtualization solutions (e.g. containers, pods) can also scale. An application scales in response to change in observed metrics, e.g. in CPU utilization. Occasionally, cloud applications exhibit the inability to meet the Quality of Service (QoS) requirements during the scaling caused by the reactivity of autoscaling solutions. This paper provides the results of the autoscaling performance evaluation for two-layered virtualization (VMs and Kubernetes pods) conducted in the public clouds of AWS, Microsoft and Google using the approach and the Autoscaling Performance Measurement Tool developed by the authors. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"D.N., Jha, Devki Nandan; S.K., Garg, Saurabh Kumar; P.P., Prakash Jayaraman, Prem Prakash; R., Buyya, Rajkumar; Z., Li, Zheng; R., Ranjan, Rajiv","A holistic evaluation of docker containers for interfering microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054031154&partnerID=40&md5=5495fe07bc0630cfd989605a463faa1f","Advancement of container technology (e.g. Docker, LXC, etc.) transformed the virtualization concept by providing a lightweight alternative to hypervisors. Docker has emerged as the most popular container management tool. Recent research regarding the comparison of container with hypervisor and bare-metal demonstrates that the container can accomplish bare-metal performance in almost all case. However, the current literature lacks an in-depth study on the experimental evaluation for understanding the performance interference between microservices that are hosted within a single or across multiple containers. In this paper, we have presented the experimental study on the performance evaluation of Docker containers running heterogeneous set of microservices concurrently. We have conducted a comprehensive set of experiments following CEEM (Cloud Evaluation Experiment Methodology) to measure the interference between containers running either competing or independent microservices. We have also considered the effects of constraining the resources of a container by explicitly specifying the cgroups. We have evaluated the performance of containers in terms of inter-container (caused by two concurrent executing containers) and intra-container (caused between two microservices executing inside a container) interference which is almost neglected in the current literature. The evaluation results can be utilized to model the interference effect for smart resource provisioning of microservices in the containerized environment. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"A.J., Fernández-García, Antonio Jesús; L., Iribarne, L.; A.L., Corral, Antonio Leopoldo; J., Criado, Javier; J.Z., Wang, James Z.","A flexible data acquisition system for storing the interactions on mashup user interfaces","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041943115&partnerID=40&md5=7515db5409e0d1b99d54aba5bf70cdd8","Nowadays, mashups applications are growing in popularity. They are accessible by cross-device applications, supporting multiple forms of interaction in cloud environments. In general, mashups manage a huge amount of heterogeneous data from different sources and handle different kinds of users. In this respect, mashup User Interfaces are becoming one of the most important pieces in many kinds of current management systems, such as for certain geographic or environmental information systems working on the Internet. In this type of systems, the user interface plays a particular role due to the huge variety of components or apps that the users need to manage at the same time. However, currently, there has been scant attention paid to the management of the user's interaction with mashups interfaces. This goal involves the need of having important, well-constructed tools and methods conducting the data acquisition process for managing properly: (a) the interaction over the mashup user interfaces, at the front-end side; (b) the storage of the interaction in relational databases; and (c) well-supported microservices structures handled in the cloud. The fact of having valuable and flexible data acquisition processes encourages the deployment of others important issues of the interaction management, i.e., data searching, data mining, marketing, security, accessibility, usability or traceability of interaction data, among others. In this article, we present a flexible Data Acquisition System capable of capturing the human-computer interactions performed by users over mashup (User) Interfaces with the aim of storing them in a relational database. Firstly, the morphology of traditional mashup applications, their specifications and the relevant information that surrounds an interaction have been studied. Thereupon, a data acquisition system that stores user interaction on mashup based on such specifications was constructed. To achieve that purpose, an architecture of microservices was also designed in the cloud to detect, acquire, and collect the interactions performed over this kind of interfaces. The whole process is ready for acquiring internal data of the information system as well as context information and location awareness. To validate the data acquisition system, some tests on empirical case studies have been developed. Efficiency and effectiveness have also been determined by evaluating the performance of the acquisition system during different load tests. Finally, in order to ensure the software quality, a continuous integration strategy for software development and an easy management of the code have been used, facilitating the software maintenance alongside the microservice architecture, where functionalities are well encapsulated. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"P., Saha, Pankaj; P.W., Umiński, Piotr W.; A.M., Beltre, Angel M.; M., Govindaraju, Madhusudhan","Evaluation of docker containers for scientific workloads in the cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051434236&partnerID=40&md5=fa9381313ce52e72d647c586c69f1330","The HPC community is actively researching and evaluating tools to support execution of scientific applications in cloud-based environments. Among the various technologies, containers have recently gained importance as they have significantly better performance compared to full-scale virtualization, support for microservices and DevOps, and work seamlessly with workflow and orchestration tools. Docker is currently the leader in containerization technology because it offers low overhead, flexibility, portability of applications, and reproducibility. Singularity is another container solution that is of interest as it is designed specifically for scientific applications. It is important to conduct performance and feature analysis of the container technologies to understand their applicability for each application and target execution environment. This paper presents a (1) performance evaluation of Docker and Singularity on bare metal nodes in the Chameleon cloud (2) mechanism by which Docker containers can be mapped with InfiniBand hardware with RDMA communication and (3) analysis of mapping elements of parallel workloads to the containers for optimal resource management with container-ready orchestration tools. Our experiments are targeted toward application developers so that they can make informed decisions on choosing the container technologies and approaches that are suitable for their HPC workloads on cloud infrastructure. Our performance analysis shows that scientific workloads for both Docker and Singularity based containers can achieve near-native performance. Singularity is designed specifically for HPC workloads. However, Docker still has advantages over Singularity for use in clouds as it provides overlay networking and an intuitive way to run MPI applications with one container per rank for fine-grained resources allocation. Both Docker and Singularity make it possible to directly use the underlying network fabric from the containers for coarse-grained resource allocation. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"J.J., Merelo-Guervós, Juan Julián; M., Garcia-Valdez, Mario","Mapping evolutionary algorithms to a reactive, stateless architecture using a modern concurrent language","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051456321&partnerID=40&md5=757af4db6ac0af0377775acea93d4662","Microservices, distributed computing, event-based systems, Kappa architecture, stateless algorithms, algorithm implementation, performance evaluation, distributed computing, pool-based systems, heterogeneous distributed systems, serverless computing, functions as a service. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"D., Son, Dongyeong; J., Park, Jaeho; E., Huh, Euinam","Dynamic SAR for Efficient Container Auto-Scaling Based on Network Traffic Prediction","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062593929&partnerID=40&md5=22b58935465c0cb12794ab80c8ffb3fd","One of the important characteristics provided by cloud computing is dynamic scalability. For scaling cloud resources, a criteria is needed, such as a threshold. It may not be easy to setup the optimal threshold that minimizes cost and achieves SLA (Service Level Agreement), especially in the case of the sudden network traffic fluctuation. To cope with this issue, a fast, accurate method to predict network traffic is required. In this paper, EMA (Exponential Moving Average) and Parabolic SAR (Stop and Reverse) are mainly considered for predicting the volume and trend of network traffic. These two complement each other and would provide better performance for network traffic prediction. It is called Dynamic SAR in this paper. By adopting Dynamic SAR, an algorithm is proposed for efficient container auto-scaling based on network traffic prediction (ACNP). And also, through performance evaluation, the proposed algorithm shows that it can contribute fast, accurate container scaling on the case of sudden network traffic fluctuation. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"S., Prasadi, Sanduni; V.G., Mallawaarachchi, Vijini G.; A.S., Wickramarachchi, Anuradha S.; I., Perera, Indika; D.A., Meedeniya, Dulani A.","Efficient scheduling for scalable bioinformatics analysis platform with microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062074828&partnerID=40&md5=09989004aca093f586716f949894b819","With the advancement of biology and computer science, amount of bioinformatics data has grown at a rapid rate. Due to this increasing demand for performance and testing of new algorithms, bioinformaticians are required to maintain efficient technological infrastructures. Hence, adoption of such novel technologies is necessary to cater the increasing demand of the industry. Furthermore, it is imperative to increase the productivity of the existing systems and at the same time execute large jobs associated with the domain. Various scheduling techniques ranging from classic First Come First Serve to the latest cloud technologies such as MapReduce can be used to execute these jobs in parallel. The work presented in this paper demonstrates an optimized platform to support the execution of various bioinformatics computations that deal with massively large datasets. This platform comprises of a MapReduce model that adopt multilevel feedback queue algorithm in scheduling such large-scale, time-consuming jobs parallel in a multicore processor. A broad comparison of existing common scheduling algorithms is conducted, to identify the most suitable scheduling algorithm. The paper also presents the performance evaluation results of the proposed solution with a range of biological sequences and algorithms as inputs. The time efficiency of the proposed solution has a x18 improvement over general First Come First Serve algorithm, for processing 1000 sequences while it gives 10x improvement at 10000 sequences, dropping again to 3x at 50000. Multilevel sequence alignment tools that are not optimized for GPU parallelism are benefited mostly from our solution. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"A.S., Gaur, Amit Singh; J., Budakoti, Jyoti; C.H., Lung, Chung Horng","Design and performance evaluation of containerized microservices on edge gateway in mobile iot","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067869718&partnerID=40&md5=0ced84148eca4653454d94f1e268fab6","Recently, Internet of Things (IoT) has drawn a great deal of attention and is envisioned in various sectors in the near future due to its promising benefits. However, the constant and rapid growth in IoT devices also brings new challenges due to constrained power and resources associated to them. One of the challenges is to provide seamless connectivity in mobile IoT. Secondly, IoT devices may stream enormous amount of data; hence, providing a solution that can effectively reduce service cost of data transfer. Finally, there are challenges in management and deployment of services running at mobile IoT Edge Gateway. In this context, containerized virtualization solution could play a key role in support of efficient management and deployment of microservices to provide seamless connectivity. This paper proposes a lightweight container-based virtualization technology for IoT, which employs Docker container-based microservices architecture for effectively deploying applications in a virtualized ecosystem. We evaluated the performance of the proposed solution on real IoT testbed using Raspberry Pi 3 as a mobile IoT Edge Gateway for network handover decision making among various alternatives, such as Wi-Fi, Radio, and Satellite. The results demonstrated better performance compared with the native environment, i.e., the one without introduction of a virtualization layer. The results also showed that the Docker container produces negligible resource overhead and can be used on resource constrained mobile IoT Edge Gateway devices like Raspberry Pi 3 for efficiently managing IoT application and services. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"B., Liu, Bingfeng; R., Buyya, Rajkumar; A.N., Toosi, Adel N.","A fuzzy-based auto-scaler for web applications in cloud computing environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056837125&partnerID=40&md5=c7e2b82897d565659bf1625c55cac446","Cloud computing provided the elasticity for its users allowing them to add or remove virtual machines depending on the load of their web applications. However, there is still no ideal auto-scaler which is both easy to use and sufficiently accurate to make web applications resilient under the dynamic load. The threshold-based auto-scaling approaches are among the most popular reactive auto-scaling strategies due to their high learnability and usability. However, the static threshold would become undesirable once the workload becomes highly dynamic and unpredictable. In this paper, we propose a novel fuzzy logic based approach that automatically and adaptively adjusts thresholds and cluster size for a web application. The proposed auto-scaler aims at reducing resource consumption without violation of Service Level Agreement (SLA). The performance evaluation is conducted with the real-life Wikipedia traces in the Amazon Web Services cloud platform. Experimental results demonstrate that our reactive auto-scaler efficiently reduces cloud resources usage and minimizes the SLA violations. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"J.J., Merelo-Guervós, Juan Julián; M., Garcia-Valdez, Mario","Introducing an event-based architecture for concurrent and distributed evolutionary algorithms","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053624183&partnerID=40&md5=29ce9cdb4db0829e829e64a984560a4b","Cloud-native applications add a layer of abstraction to the underlying distributed computing system, defining a high-level, self-scaling and self-managed architecture of different microservices linked by a messaging bus. Creating new algorithms that tap these architectural patterns and at the same time employ distributed resources efficiently is a challenge we will be taking up in this paper. We introduce KafkEO, a cloud-native evolutionary algorithms framework that is prepared to work with different implementations of evolutionary algorithms and other population-based metaheuristics by using micro-populations and stateless services as the main building blocks; KafkEO is an attempt to map the traditional evolutionary algorithm to this new cloud-native format. As far as we know, this is the first architecture of this kind that has been published and tested, and is free software and vendor-independent, based on OpenWhisk and Kafka. This paper presents a proof of concept, examines its cost, and tests the impact on the algorithm of the design around cloud-native and asynchronous system by comparing it on the well known BBOB benchmarks with other pool-based architectures, with which it has a remarkable functional resemblance. KafkEO results are quite competitive with similar architectures. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"A., Gujarati, Arpan; S., Elnikety, Sameh; Y., He, Yuxiong; K.S., McKinley, Kathryn S.; B.B., Brandenburg, Björn B.","Swayam: Distributed autoscaling to meet slas of machine learning inference services with resource efficiency","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041194932&partnerID=40&md5=7a82e3dcb116d267d97ecce195fd34c1","Developers use Machine Learning (ML) platforms to train ML models and then deploy these ML models as web services for inference (prediction). A key challenge for platform providers is to guarantee response-time Service Level Agreements (SLAs) for inference workloads while maximizing resource eficiency. Swayam is a fully distributed autoscaling framework that exploits characteristics of production ML inference workloads to deliver on the dual challenge of resource eficiency and SLA compliance. Our key contributions are (1) model-based autoscaling that takes into account SLAs and ML inference workload characteristics, (2) a distributed protocol that uses partial load information and prediction at frontends to provision new service instances, and (3) a backend self-decommissioning protocol for service instances. We evaluate Swayam on 15 popular services that were hosted on a production ML-as-a-service platform, for the following service-specific SLAs: for each service, at least 99% of requests must complete within the response-time threshold. Compared to a clairvoyant autoscaler that always satisfies the SLAs (i.e., even if there is a burst in the request rates), Swayam decreases resource utilization by up to 27%, while meeting the service-specific SLAs over 96% of the time during a three hour window. Microsoft Azure's Swayam-based framework was deployed in 2016 and has hosted over 100,000 services. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"C., Guerrero, Carlos; I., Lera, Isaac; C., Juiz, C.","Genetic algorithm for multi-objective optimization of container allocation in cloud architecture","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046676450&partnerID=40&md5=dfcddbd40737dabfa118ddd049169410","The use of containers in cloud architectures has become widespread, owing to advantages such as limited overheads, easier and faster deployment, and higher portability. Moreover, they present a suitable architectural solution for the deployment of applications created using a microservice development pattern. Despite the large number of solutions and implementations, there remain open issues that have not been completely addressed in container automation and management. Container resource allocation influences system performance and resource consumption, and so it is a key factor for cloud providers. We propose a genetic algorithm approach, using the Non-dominated Sorting Genetic Algorithm- II (NSGA-II), to optimize container allocation and elasticity management, motivated by the good results obtained with this algorithm in other resource management optimization problems in cloud architectures. Our optimization algorithm enhances system provisioning, system performance, system failure, and network overhead. A model for cloud clusters, containers, microservices, and four optimization objectives is presented. Experimental results demonstrate that our approach is a suitable solution for addressing the problem of container allocation and elasticity, and it obtains better objective values than the container management policies implemented in Kubernetes. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"J.M., Valenzuela Posadas, Jorge M.","Application of mixed distributed software architectures for social-productive projects management in peru","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039993869&partnerID=40&md5=c7486bc9364ae9396525c771ffaefef4","FONCODES is the public body in charge of financing and managing the execution of social-productive projects throughout the Peruvian territory. The objective of social-productive projects within the framework of the National Strategy of the Ministry of Development and Social Inclusion is to generate economic opportunities for rural households living in poverty and extreme poverty. The key processes to achieve these national strategic objectives are related to the management of portfolios and projects at all stages. The key information of these processes is that related to budget, expenditure authorizations, accountability, liquidation and financial records since it allows evaluating the efficiency of the obtained management results. There are serious limitations during the transmission of information in real time generating bottlenecks in the respective processes. This paper presents the development and application of mixed distributed software architectures to improve the flow and exchange of key information of social-productive projects in the Peruvian territory. A comparison is made of the developed mixed architectures, the monolithic architecture with SOA services and the microservices architecture as a REST API on Cloud. The mixed distributed software architecture satisfies the need to interoperate legacy systems with a monolithic web application and SOA services however also is required satisfies the need to support high peaks of requests in certain processes where microservices deployed on the Cloud are the best choice. The performance tests results obtained on both architectures are presented, allowing a performance comparison of monolithic architecture and microservice architecture. The paper allow to evaluate at high technical level the benefits and challenges that agencies government can face when implement monolithic web applications and microservices on cloud related to execution perfomance and development team management. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"E., Casalicchio, Emiliano; V., Perciballi, Vanessa","Auto-Scaling of Containers: The Impact of Relative and Absolute Metrics","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035196017&partnerID=40&md5=e5862851c62073c98638b055999d4f21","Today, The cloud industry is adopting the container technology both for internal usage and as commercial offering. The use of containers as base technology for large-scale systems opens many challenges in the area of resource management at run-Time. This paper addresses the problem of selecting the more appropriate performance metrics to activate auto-scaling actions. Specifically, we investigate the use of relative and absolute metrics. Results demonstrate that, for CPU intense workload, the use of absolute metrics enables more accurate scaling decisions. We propose and evaluate the performance of a new autoscaling algorithm that could reduce the response time of a factor between 0.66 and 0.5 compared to the actual Kubernetes' horizontal auto-scaling algorithm. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"G., Mazlami, Genc; J., Cito, Jürgen; P., Leitner, Philipp","Extraction of Microservices from Monolithic Software Architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032372980&partnerID=40&md5=af65b7a2969741c199b8be8f626a3272","Driven by developments such as mobile computing, cloud computing infrastructure, DevOps and elastic computing, the microservice architectural style has emerged as a new alternative to the monolithic style for designing large software systems. Monolithic legacy applications in industry undergo a migration to microservice-oriented architectures. A key challenge in this context is the extraction of microservices from existing monolithic code bases. While informal migration patterns and techniques exist, there is a lack of formal models and automated support tools in that area. This paper tackles that challenge by presenting a formal microservice extraction model to allow algorithmic recommendation of microservice candidates in a refactoring and migration scenario. The formal model is implemented in a web-based prototype. A performance evaluation demonstrates that the presented approach provides adequate performance. The recommendation quality is evaluated quantitatively by custom microservice-specific metrics. The results show that the produced microservice candidates lower the average development team size down to half of the original size or lower. Furthermore, the size of recommended microservice conforms with microservice sizing reported by empirical surveys and the domain-specific redundancy among different microservices is kept at a low rate. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"R., Morabito, Roberto; N.S., Beijar, Nicklas S.","A framework based on SDN and containers for dynamic service chains on IoT gateways","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030770088&partnerID=40&md5=5a99e5538ecfa65b4872215ac3a86e23","In this paper, we describe a new approach for managing service function chains in scenarios where data from Internet of Things (IoT) devices is partially processed at the network edge. Our framework is enabled by two emerging technologies, Software-Defined Networking (SDN) and container based virtualization, which ensure several benefits in terms of flexibility, easy programmability, and versatility. These features are well suitable with the increasingly stringent requirements of IoT applications, and allow a dynamic and automated network service chaining. An extensive performance evaluation, which has been carried out by means of a testbed, seeks to understand how our proposed framework performs in terms of computational overhead, network bandwidth, and energy consumption. By accounting for the constraints of typical IoT gateways, our evaluation tries to shed light on the actual deployability of the framework on low-power nodes. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"R.K., Kombi, Roland Kotto; N., Lumineau, Nicolas; P., Lamarre, Philippe","A Preventive Auto-Parallelization Approach for Elastic Stream Processing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027277046&partnerID=40&md5=8ac7711529d13aa98344f4c0b3378606","Nowadays, more and more sources (connected devices, social networks, etc.) emit real-time data with fluctuating rates over time. Existing distributed stream processing engines (SPE) have to resolve a difficult problem: deliver results satisfying end-users in terms of quality and latency without over-consuming resources. This paper focuses on parallelization of operators to adapt their throughput to their input rate. We suggest an approach which prevents operator congestion in order to limit degradation of results quality. This approach relies on an automatic and dynamic adaptation of resource consumption for each continuous query. This solution takes advantage of i) a metric estimating the activity level of operators in the near future ii) the AUTOSCALE approach which evaluates the need to modify parallelism degrees at local and global scope iii) an integration into the Apache Storm solution. We show performance tests comparing our approach to the native solution of this SPE. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"M.E.H.B.M., Frikha, Mohamed El Hedi Boussada Mounir; J.M., Garcia, Jean Marie","A fluid approach for evaluating the performance of TCP traffic in the presence of real time traffic","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050638217&partnerID=40&md5=0bc680ac35bdf7034b19276d06efb3a3","Today, the Internet traffic is mostly dominated by an elastic data transfer. However, with the progressive development of the real time applications, it is anticipated that the streaming traffic will contribute a significant amount of traffic in the near future. By combining priority queuing with Class Based Weighted Fair Queueing (CBWFQ), The Low Latency Queueing (LLQ) is a very important router discipline that aims to provide the needed quality of service (QoS) for each traffic category. Priority queueing is used to guarantee delay constraints for real-time traffic, whereas CBWFQ is used to ensure acceptable throughput for traffic classes that are less sensitive to delay. In this paper, we focus on developing a fluid model to capture the relation between elastic traffic and the real time traffic in a LLQ system under a quasi-stationary assumption. Our analysis of the CBWFQ system is based on some numerical observations and relies on the conservation of the average total number of elastic flows carried by the elastic system. Detailed packet level simulations of TCP flows show the accuracy of our analysis. The results presented in this paper allows a rapid performance evaluation of TCP traffic circulating in the actual IP networks. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"U., Sakthivel, Usha; N., Singhal, Neha; C.P., Raj, C. Pethuru","RESTful web services composition & performance evaluation with different databases","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046531602&partnerID=40&md5=d8ea0eaa65d59c1f3d9a9da96c2f4b72","Service composition is a popular mechanism to orchestrate different and distributed services to produce composite services. Composed services are typically businesscentric, and process-aware. There are a variety of use cases mandating such kinds of service compositions. With services emerging as the most appropriate building block and the unit of deployment, finding, composing, binding and leveraging services turns out to be an important job for software architects and developers. However, service composition is not an easy affair. There are several methods proposed by various researchers and scholars across the globe. Several parameters and considerations are being made in order to simplify and streamline service composition. Predominantly there are SOAP and RESTful services. There are markup languages to describe and define the distinct capabilities of participating services. In the recent past, a new architectural style (Microservices architecture (MSA)) is emerging and evolving fast. Microservices are lightweight, autonomous, self-defined, horizontally scalable, interoperable, and composable. In this paper, we have leveraged different databases for different services and evaluated the service performance individually and collectively. That is, multiple services are used and each service uses a database instance. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"T.F., Düllmann, Thomas F.; A.V., Van Hoorn, André V.","Model-driven generation of microservice architectures for benchmarking performance & resilience engineering approaches","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019479213&partnerID=40&md5=bd8bc416fb3ba3e141970452159ec84e","Microservice architectures are steadily gaining adoption in industrial practice. At the same time, performance and resilience are important properties that need to be ensured. Even though approaches for performance and resilience have been developed (e.g., for anomaly detection and fault tolerance), there are no benchmarking environments for their evaluation under controlled conditions. In this paper, we propose a generative platform for benchmarking performance and resilience engineering approaches in microservice architectures, comprising an underlying metamodel, a generation platform, and supporting services for workload generation, problem injection, and monitoring. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"E., Casalicchio, Emiliano; V., Perciballi, Vanessa","Measuring Docker performance: What a mess!!!","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019477222&partnerID=40&md5=c5fc1d065447eca2fc78e6bd1413920a","Today, a new technology is going to change the way platforms for the internet of services are designed and managed. This technology is called container (e.g. Docker and LXC). The internet of service industry is adopting the container technology both for internal usage and as commercial offering. The use of container as base technology for largescale systems opens many challenges in the area of resource management at run-Time, for example: Autoscaling, optimal deployment and monitoring. Specifically, monitoring of container based systems is at the ground of any resource management solution, and it is the focus of this work. This paper explores the tools available to measure the performance of Docker from the perspective of the host operating system and of the virtualization environment, and it provides a characterization of the CPU and disk I/O overhead introduced by containers. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"T., Salah, Tasneem; M.J., Zemerly, M. Jamal; C.Y., Yeun, Chan Yeob; M.A., Al-Qutayri, Mahmoud A.; Y., Al-Hammadi, Yousof","Performance comparison between container-based and VM-based services","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018882094&partnerID=40&md5=bb4284793910f9a44167b10a9ccea593","These days, microservice architecture is widely used in the design and development of many real-time, critical, and large-scale online services. These services are typically deployed using Docker containers on cloud platforms. Container technology supports the deployment of these services with high portability, scalability, and performance, when compared to deploying them using virtual machines (i.e. VM-based services). It is widely known fact that container-based services give better performance than VM-based services. However, we show in this paper that services deployed using Amazon AWS ECS (EC2 Container Service) surprisingly perform significantly worse when compared with services deployed using Amazon EC2 VMs. We study and quantify the performance difference in terms of throughput, response time and CPU utilization considering different deployment scenarios. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"K., Vandikas, Konstantinos; V., Tsiatsis, Vlasios","Microservices in IoT clouds","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017216389&partnerID=40&md5=e5d689281dec8385c44cf0c125f9540a","The current Internet of Things technology landscape is admittedly plagued with fragmentation. Fragmentation in IoT seems to be abundant ranging from the device hardware, operating system and software to device-to-device and device-to-cloud-based infrastructure protocols, to the actual cloud-based infrastructure and tools used for developing and operating software that runs on these two opposite ends of the system. As such it is challenging for a developer to decide the development, deployment and operational technologies for a complete end-to-end IoT solution. In this paper we focus on the performance evaluation and technology selection of the cloud end of the system. Given that a modern IoT cloud infrastructure is based on a microservices platform, we tackle the challenge of the selection of a Microservice Application Server (MAS) among several available in the developer community for the task of data collection from IoT devices. This paper provides two key contributions. First of all we present an empirical evaluation between different JVM-based MASs and corresponding client/frameworks taking into consideration throughput (req/s), memory footprint and binary footprint. Secondly we provide an open-source testbed that can be used for reproducing the current evaluation and for extending the evaluation towards additional MASs that have been implemented in different runtimes/programming languages. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"W., Kim, Won-yong; J., Lee, Jinseop; E., Huh, Euinam","Study on proactive auto scaling for instance through the prediction of network traffic on the container environment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015243559&partnerID=40&md5=56af80186931bfc1dd3417a866b5cefd","In this paper, we propose container traffic analyzer (COTA) structure to improve accommodating more network traffic to VMs and to reduce the scale-out time. COTA consists of two functions. The one is reporting the amount of network traffic on real-time. The other function is managing server balance on user requests and scale-out VM by using aggregated network traffic profile. Based on these network traffic information, we propose Least Traffic Load Balancing(LTLB) algorithm to solve network traffic imbalance problem. LTLB algorithm establishes new connection to VM which has the least traffic in real-time. We test performance comparison evaluation with existing well-known dynamic load balancing algorithms. And we apply the algorithm to the Docker based Container environment that has light-weight and occupy low storage capacity to provide fast and elasticity scale-out as well as scaling policy including traffic threshold. Then, performance evaluation is done between VM over hypervisor and Docket based Container. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"S., Wang, Shenlong; L., Luo, Linjie; N., Zhang, Ning; J., Li, Jia","AutoScaler: Scale-attention networks for visual correspondence","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086689504&partnerID=40&md5=aad7c14c6adaa4ef2a751825ba048abf","Finding visual correspondence between local features is key to many computer vision problems. While defining features with larger contextual scales usually implies greater discriminativeness, it could also lead to less spatial accuracy of the features. We propose AutoScaler, a scale-attention network to explicitly optimize this trade-off in visual correspondence tasks. Our architecture consists of a weight-sharing feature network to compute multi-scale feature maps and an attention network to combine them optimally in the scale space. This allows our network to have adaptive sizes of equivalent receptive field over different scales of the input. The entire network can be trained end-to-end in a Siamese framework for visual correspondence tasks. Using the latest off-the-shelf architecture for the feature network, our method achieves competitive results compared to state-of-the-art methods on challenging optical flow and semantic matching benchmarks, including Sintel, KITTI and CUB-2011. We also show that our attention network alone can be applied to existing hand-crafted feature descriptors (e.g Daisy) and improve their performance on visual correspondence tasks. Finally, we illustrate how the scale-attention maps generated from the attention network are visually interpretable. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"M.R., HoseinyFarahabady, M. Reza; Y., Lee, Youngchoon; A.Y.H., Zomaya, Albert Y.H.; Z., Tari, Zahir","A QoS-aware resource allocation controller for function as a service (FaaS) platform","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034097445&partnerID=40&md5=4404a95793a74f3e18ec6f5a54f45a84","Function as a Service (FaaS) is a recent event-driven serverless paradigm that allows enterprises to build their applications in a fault tolerant distributed manner. Having been considered as an attractive replacement of traditional Service Oriented Architecture (SOA), the FaaS platform leverages the management of massive data sets or the handling of event streams. However, the realization of such leverage is largely dependent on the effective exploitation of FaaS elasticity/scalability. In this paper, we present a closed-loop resource allocation controller to dynamically scale resources by predicting the future rate of incoming events and by considering the Quality of Service (QoS) enforcements requested by end-users. The performance evaluation is carried out by comparing the proposed controller with some well-known heuristics such as round robin and best-effort strategies. Experimental results confirm that the proposed controller increases the overall resource utilization by 21% on average, while reducing QoS violations by a factor of almost 3. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"H., Kim, Hyun-woo; J., Han, Jaekyung; J.H.(.J., Park, Jong Hyuk (James J.).; Y., Jeong, Youngsik","DIaaS: Resource management system for the intra-cloud with on-premise desktops","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011656489&partnerID=40&md5=df63bf3d4ede854f7c163cb2978ecb77","Infrastructure as a service with desktops (DIaaS) based on the extensible mark-up language (XML) is herein proposed to utilize surplus resources. DIaaS is a traditional surplus-resource integrated management technology. It is designed to provide fast work distribution and computing services based on user service requests as well as storage services through desktop-based distributed computing and storage resource integration. DIaaS includes a nondisruptive resource service and an auto-scalable scheme to enhance the availability and scalability of intra-cloud computing resources. A performance evaluation of the proposed scheme measured the clustering performance time for surplus resource utilization. The results showed improvement in computing and storage services in a connection of at least two computers compared to the traditional method for high-availability measurement of nondisruptive services. Furthermore, an artificial server error environment was used to create a clustering delay for computing and storage services and for nondisruptive services. It was compared to the Hadoop distributed file system (HDFS). © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"P., Ranft, Philipp; J.S., Neurath, Jan Steffen; J., Ehlers, Jens","Evaluating the Performance of AppScale","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010453382&partnerID=40&md5=cb2eb602cbceaa46451f65b302de66a6","AppScale provides an easy way to distribute applications using the Google App Engine SDK on different infrastructure platforms, e.g. in a private cloud. In this paper, we provide a performance evaluation comparing a benchmark application hosted at the original Google App Engine (GAE) and by means of AppScale at Amazon EC2, at Google Compute Engine (GCE) and on a private on-premise cluster. The benchmark results show that GAE scales best for our use case, closely followed by AppScale cloud deployments. Thus, AppScale provides a dependable alternative for auto-scaling applications built according to the GAE SDK without the need to host the application at Google's infrastructure. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"A., Iosup, Alexandru; S., Kounev, Samuel; K., Sachs, Kai","SPEC research group's cloud working group: [RG cloud group]","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020218733&partnerID=40&md5=4d2b2c1dc42c362816d496cf125eca8b","The 7th ACM/SPEC International Conference on Performance Engineering (ICPE 2016) takes place in Delft in The Netherlands in March 2016. The conference grew out of the ACM Workshop on Software Performance (WOSP since 1998) and the SPEC International Performance Engineering Workshop (SIPEW since 2008), with the goal of integrating theory and practice in the field of performance engineering. It is a great pleasure for us to offer an outstanding technical program this year, which we believe will allow researchers and practitioners to present their visions and latest innovation, and to exchange ideas within the community. Overall, we received 89 high quality submissions across all three tracks. The main Research Track attracted 57 submissions with 19 accepted (33% acceptance rate) for presentation at the conference. Among them were 16 full papers and three short papers. Each paper received at least three reviews from experienced program committee members. In the Work-In-Progress and Vision Track, six out of 15 contributions were selected. The Industry and Experience Track received 17 submissions, of which seven were selected for inclusion in the program. The accepted papers were organized into five research track sessions, two industry track sessions, and one WiP and vision track session. Three best paper candidates were also selected: two research papers and one industry paper. We are proud to have three excellent keynote speakers as part of our technical program: Bianca Schroeder from University of Toronto, Canada, presenting ""Case studies from the real world: The importance of measurement and analysis in building better systems"" Wilhelm Hasselbring from Kiel University, Germany, discussing ""Microservices for Scalability"" Angelo Corsaro, Chief Technology Officer at PrismTech, talking about ""Cloudy, Foggy and Misty Internet of Things"" In addition, the program includes four tutorials, a doctoral symposium, a poster and demo track, the SPEC Distinguished Dissertation Award, and three interesting workshops, including the International Workshop on Large-Scale Testing (LT), the 2nd International Workshop on Performance Analysis of Big data Systems (PABS), and the 2nd Workshop on Challenges in Performance Methods for Software Development (WOSPC). The program covers traditional ICPE topics such as software and systems performance modeling and prediction, analysis and optimization, characterization and profiling, as well as application of performance engineering theory and techniques to several practical fields, including distributed systems, cloud computing, storage, energy, big data, virtualized systems and containers. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"Z., Li, Zheng; L., O'Brien, Liam; M., Kihl, Maria","DoKnowMe: Towards a domain knowledge-driven methodology for performance evaluation","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009754937&partnerID=40&md5=2689691e1485630b439cb6d769d5c840","Software engineering considers performance evaluation to be one of the key portions of software quality assurance. Unfortunately, there seems to be a lack of standard methodologies for performance evaluation even in the scope of experimental computer science. Inspired by the concept of ""instantiation"" in object-oriented programming, we distinguish the generic performance evaluation logic from the distributed and ad-hoc relevant studies, and develop an abstract evaluation methodology (by analogy of ""class"") we name Domain Knowledge-driven Methodology (DoKnowMe). By replacing five predefined domain-specific knowledge artefacts, DoKnowMe could be instantiated into specific methodologies (by analogy of ""object"") to guide evaluators in performance evaluation of different software and even computing systems. We also propose a generic validation framework with four indicators (i.e. usefulness, feasibility, effectiveness and repeatability), and use it to validate DoKnowMe in the Cloud services evaluation domain. Given the positive and promising validation result, we plan to integrate more common evaluation strategies to improve DoKnowMe and further focus on the performance evaluation of Cloud autoscaler systems. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"E., Campos, Eliomar; R.D.S., Matos, Rubens De S.; P.R.M., MacIel, Paulo Romero Martins; A., Pereira, Airton; F., Souza, Francisco","Stochastic Modeling of Auto Scaling Mechanism in Private Clouds for Supporting Performance Tuning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964510264&partnerID=40&md5=927bd5c321dc4d71b3b37f17174bb2bf","Elasticity mechanisms allow private cloud platforms to dynamically increase or decrease resources according to policies related to workload. Auto scaling and virtual machines (VMs) instantiation are processes which affect the performance of such mechanisms. This paper evaluates the performance of the auto scaling process on a private cloud platform considering some relevant factors which are not addressed in many related research studies. Besides the experimental study, this paper presents a Markov chain model for parametric sensitivity analysis, useful to prioritize efforts in certain process parameters. The evaluation approach and results from this study can help system administrators to properly configure the auto scaling mechanism in private clouds frameworks. © 2016 Elsevier B.V., All rights reserved.",No load test generation.
"M., Amaral, Marcelo; J., Polo, Jordà; D., Carrera, David; I.I., Mohomed, Iqbal I.; M., Unuvar, Merve; M., Steinder, Małgorzata","Performance evaluation of microservices architectures using containers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963600252&partnerID=40&md5=a96d8fff6e43a5ae2d4727de23ab9480","Micro services architecture has started a new trend for application development for a number of reasons: (1) to reduce complexity by using tiny services, (2) to scale, remove and deploy parts of the system easily, (3) to improve flexibility to use different frameworks and tools, (4) to increase the overall scalability, and (5) to improve the resilience of the system. Containers have empowered the usage of micro services architectures by being lightweight, providing fast start-up times, and having a low overhead. Containers can be used to develop applications based on monolithic architectures where the whole system runs inside a single container or inside a micro services architecture where one or few processes run inside the containers. Two models can be used to implement a micro services architecture using containers: master-slave, or nested-container. The goal of this work is to compare the performance of CPU and network running benchmarks in the two aforementioned models of micro services architecture hence provide a benchmark analysis guidance for system designers. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"K., Hwang, Kai; X., Bai, Xiaoying; Y., Shi, Yue; M., Li, Muyang; W., Chen, Wenguang; Y., Wu, Yongwei","Cloud Performance Modeling with Benchmark Evaluation of Elastic Scaling Strategies","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961757531&partnerID=40&md5=4e7da5b85ae356d279be8e92065f897e","In this paper, we present generic cloud performance models for evaluating Iaas, PaaS, SaaS, and mashup or hybrid clouds. We test clouds with real-life benchmark programs and propose some new performance metrics. Our benchmark experiments are conducted mainly on IaaS cloud platforms over scale-out and scale-up workloads. Cloud benchmarking results are analyzed with the efficiency, elasticity, QoS, productivity, and scalability of cloud performance. Five cloud benchmarks were tested on Amazon IaaS EC2 cloud: namely YCSB, CloudSuite, HiBench, BenchClouds, and TPC-W. To satisfy production services, the choice of scale-up or scale-out solutions should be made primarily by the workload patterns and resources utilization rates required. Scaling-out machine instances have much lower overhead than those experienced in scale-up experiments. However, scaling up is found more cost-effective in sustaining heavier workload. The cloud productivity is greatly attributed to system elasticity, efficiency, QoS and scalability. We find that auto-scaling is easy to implement but tends to over provision the resources. Lower resource utilization rate may result from auto-scaling, compared with using scale-out or scale-up strategies. We also demonstrate that the proposed cloud performance models are applicable to evaluate PaaS, SaaS and hybrid clouds as well. © 2016 Elsevier B.V., All rights reserved.",No load test generation.
"M.J., Villamizar, Mario J.; O., Garcés, Oscar; H.E., Castro, Harold E.; M.V., Verano Merino, Mauricio Verano; L., Salamanca, Lorena; R.O., Casallas, Rubby Osear; S., Gil, Santiago","Evaluating the monolithic and the microservice architecture pattern to deploy web applications in the cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963831128&partnerID=40&md5=a50feb0ba95dc3c13493ba79ac2f3033","Cloud computing provides new opportunities to deploy scalable application in an efficient way, allowing enterprise applications to dynamically adjust their computing resources on demand. In this paper we analyze and test the microservice architecture pattern, used during the last years by large Internet companies like Amazon, Netflix and LinkedIn to deploy large applications in the cloud as a set of small services that can be developed, tested, deployed, scaled, operated and upgraded independently, allowing these companies to gain agility, reduce complexity and scale their applications in the cloud in a more efficient way. We present a case study where an enterprise application was developed and deployed in the cloud using a monolithic approach and a microservice architecture using the Play web framework. We show the results of performance tests executed on both applications, and we describe the benefits and challenges that existing enterprises can get and face when they implement microservices in their applications. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"E.L.C., Mamani, Edwin Luis Choquehuanca; L.A., Pereira Junior, Lourenco Alves; M.J., Santana, Marcos José; R.H.C., Santana, Regina Helena Carlucci; P.N., Nobile, Pedro Northon; F.J., Monaco, Francisco José","Transient performance evaluation of cloud computing applications and dynamic resource control in large-scale distributed systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948441441&partnerID=40&md5=f2360a8ed87df2490aecc80337c1f706","This paper discusses on non-stationary performance evaluation and dynamic modeling of cloud computing environments. In computer systems, dynamic effects results from the filling of buffers, event-handling delays, non-deterministic I/O response times, network latency, among other factors. While computer systems performance evaluation under stationary workloads have met the needs of many engineering problems, new challenges arise as the deployment of increasingly complex and large-scale distributed systems becomes commonplace. One key aspect of this discussion is that transient analysis models how the system reacts to changes in the workload and may reveal that the resources necessary to support a high steady-state workload may not be sufficient to handle a small, but sudden, workload change, even of intensity far smaller than that supported by the system's stationary capacity. This article elaborates on these issues under a control-theoretical approach. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"E., Campos, Eliomar; R.D.S., Matos, Rubens De S.; P.R.M., MacIel, Paulo Romero Martins; I.O., Costa, Igor Oliveira; F.A., Silva, Francisco Airton; F., Souza, Francisco","Performance Evaluation of Virtual Machines Instantiation in a Private Cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973380293&partnerID=40&md5=abce167203e37da91ef2dd7767907cc9","Elasticity is an outstanding concept of cloud computing, usually deployed through mechanisms such as auto scaling and load balancing. Cloud-based applications are able to adapt themselves dynamically to the workload behavior due to such mechanisms. The efficient instantiation of Virtual Machines (VMs) is one requirement for the elastic behavior of cloud-based applications. This study characterizes the performance of VM instantiation in a private cloud platform, considering distinct factors such as VM type, VM image size, and VM caching. We employed a full factorial design of experiments (DoE) to compute the effect and relevance of the factors as well as their interactions. Our experimental results show that the cache factor has an impact of 45.07 % on the total instantiation time, whereas the machine image (MI) has 26.45 % and the VM type only 1.05 %. The results of these experiments are also used as input parameters in a Markov chain model for sensitivity analysis. The model evaluation showed that for 6 GB and 8 GB MI, the probability of finding the MI on cache must be at least 40 % and 60 % respectively, to achieve an average instantiation time of 300 seconds. For MI with size 2 GB, such time is not exceeded even with the cache disabled. This analysis allows checking the impact of every parameter on the system response time and pointing out effective ways for improvement of performance. Such conclusions may be used as decision support for systems which often instantiate new VMs, including those using elasticity features, such as auto scaling. © 2017 Elsevier B.V., All rights reserved.",No load test generation.
"Y., Guo, Yang; A.L., Stolyar, Alexander L.; A.I., Walid, Anwar I.","Online algorithms for joint application-VM-physical-machine auto-scaling in a cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955563360&partnerID=40&md5=cada55929c81857eba50058472be39ad","We develop shadow routing based online algorithms for the joint problem of application-to-VM and VM-to-PM assignments in a cloud environment. The asymptotic optimality of the shadow algorithm is proved and the performance is evaluated by simulations. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"I., Hartung, Istvan; B., Goldschmidt, Balázs","Performance analysis of windows Azure data storage options","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904101015&partnerID=40&md5=ba24f1842fdd82346d0fa5a81512342e","Windows Azure provides an IaaS cloud service with virtual machines, web and worker roles and practically unlimited, pay-as-you-go storage options which can be used for applications requiring big data or parallel computing which is important in many fields including biology, astronomy, nuclear physics and economics. When moving an application or computation task to the cloud it is very important to perform proof of concept performance testing and to carefully choose the proper building blocks for the given tasks. Windows Azure provides multiple data management options with a relational SQL database for transactional data access, Azure Tables for auto scalable storage of unstructured data, and a blob storage for storing large amounts of binary data which is easily mountable to a given virtual machine. In this paper we present a general performance analysis of the Windows Azure cloud with focus on cloud storage options. We present an environment to perform automated testing of the major features of Azure storage and we also present the preliminary results and suggestions regarding the usage of the different services. © 2014 Springer-Verlag. © 2014 Elsevier B.V., All rights reserved.",No load test generation.
"B., Liu, Bo; R.K., Madduri, Ravi K.; B., Sotomayor, Borja; K., Chard, Kyle; Ł., Łaciński, Łukasz; U.J., Dave, Utpal J.; J., Li, Jianqiang; C., Liu, Chunchen; I.T., Foster, Ian T.","Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902553652&partnerID=40&md5=a0fb78ab4bbeefc93b2e1fb932443b62","Due to the upcoming data deluge of genome data, the need for storing and processing large-scale genome data, easy access to biomedical analyses tools, efficient data sharing and retrieval has presented significant challenges. The variability in data volume results in variable computing and storage requirements, therefore biomedical researchers are pursuing more reliable, dynamic and convenient methods for conducting sequencing analyses. This paper proposes a Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses, which enables reliable and highly scalable execution of sequencing analyses workflows in a fully automated manner. Our platform extends the existing Galaxy workflow system by adding data management capabilities for transferring large quantities of data efficiently and reliably (via Globus Transfer), domain-specific analyses tools preconfigured for immediate use by researchers (via user-specific tools integration), automatic deployment on Cloud for on-demand resource allocation and pay-as-you-go pricing (via Globus Provision), a Cloud provisioning tool for auto-scaling (via HTCondor scheduler), and the support for validating the correctness of workflows (via semantic verification tools). Two bioinformatics workflow use cases as well as performance evaluation are presented to validate the feasibility of the proposed approach. © 2014 Elsevier Inc. © 2014 Elsevier B.V., All rights reserved.",No load test generation.
"J., Namjoshi, Jyoti; Y., Wadia, Yohan; R., Gaonkar, Rohini","Portable autoscaler for managing multi-cloud elasticity","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893649478&partnerID=40&md5=a898e554715e8a6a1f77427757d62116","Ability to scale resources up or down dynamically as per changes in workload conditions is one of the key features of clouds. We present here a framework for elastic scaling of cloud resources that is portable across clouds from a wide range of private and public cloud providers and that can be easily integrated with other frameworks. © 2013 IEEE. © 2014 Elsevier B.V., All rights reserved.",No load test generation.
"C., Bunch, Chris; V., Arora, Vaibhav; N., Chohan, Navraj; C.J., Krintz, Chandra J.; S., Hegde, Shashank; A., Srivastava, Ankit","A pluggable autoscaling service for open cloud PaaS systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874228400&partnerID=40&md5=c7650e207de41a9264bf6cad8c6b6ff0","In this paper we present the design, implementation, and evaluation of a pluggable autoscaler within an open cloud platform-as-a-service (PaaS). We redefine high availability (HA) as the dynamic use of virtual machines to keep services available to users, making it a subset of elasticity (the dynamic use of virtual machines). This makes it possible to investigate autoscalers that simultaneously address HA and elasticity. We present and evaluate autoscalers within this pluggable system that are HA-aware and Quality-of-Service (QoS)-aware for web applications written in different programming languages. Hot spares can also be utilized to provide both HA and improve QoS to web users. Within the open source AppScale PaaS, hot spares can increase the amount of web traffic that the QoS-aware autoscaler serves to users by up to 32%. As this autoscaling system operates at the PaaS layer, it is able to control virtual machines and be cost-aware when addressing HA and QoS. This cost awareness uses Spot Instances within Amazon EC2 to reduce the cost of machines acquired by 91%, in exchange for increased startup time. This pluggable autoscaling system facilitates the investigation of new autoscaling algorithms by others that can take advantage of metrics provided by different levels of the cloud stack. © 2012 IEEE. © 2013 Elsevier B.V., All rights reserved.",No load test generation.
"D., Didona, Diego; P., Romano, Paolo; S., Peluso, Sebastiano; F., Quaglia, Francesco","Transactional auto scaler: Elastic scaling of in-memory transactional data grids","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867730562&partnerID=40&md5=a8027d97aabfaaee9f1f1eb2f39b662d","In this paper we introduce TAS (Transactional Auto Scaler), a system for automating elastic-scaling of in-memory transactional data grids, such as NoSQL data stores or Distributed Transactional Memories. Applications of TAS range from on-line self-optimization of in-production applications to automatic generation of QoS/cost driven elastic scaling policies, and support for what-if analysis on the scalability of transactional applications. The key innovation at the core of TAS is a novel performance forecasting methodology that relies on the joint usage of analytical modeling and machine-learning. By exploiting these two, classically competing, methodologies in a synergic fashion, TAS achieves the best of the two worlds, namely high extrapolation power and good accuracy even when faced with complex workloads deployed over public cloud infrastructures. We demonstrate the accuracy and feasibility of TAS via an extensive experimental study based on a fully fledged prototype implementation, integrated with a popular open-source transactional in-memory data store (Red Hat's Infinispan), and industry-standard benchmarks generating a breadth of heterogeneous workloads. Copyright 2012 ACM. © 2012 Elsevier B.V., All rights reserved.",No load test generation.
"H., Ghanbari, Hamoun; B., Simmons, Bradley; M., Litoiu, M.; C., Barna, Cornel; G., Iszlai, Gabriel","Optimal autoscaling in a IaaS cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867723626&partnerID=40&md5=40d4314afdadc9ae2ceb696c1340dbe9","An application provider leases resources (i.e., virtual ma- chine instances) of variable configurations from a IaaS provider over some lease duration (typically one hour). The application provider (i.e., consumer) would like to minimize their cost while meeting all service level obligations (SLOs). The mechanism of adding and removing resources at runtime is referred to as autoscaling. The process of autoscaling is automated through the use of a management component referred to as an autoscaler. This paper introduces a novel autoscaling approach in which both cloud and application dynamics are modeled in the context of a stochastic, model predictive control problem. The approach exploits trade- off between satisfying performance related objectives for the consumer's application while minimizing their cost. Simulation results are presented demonstrating the efficacy of this new approach. Copyright 2012 ACM. © 2012 Elsevier B.V., All rights reserved.",No load test generation.
"M., Vasar, Martti; S.N., Srirama, Satish Narayana; M., Dumas, Marlon","Framework for monitoring and testing web application scalability on the cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866879899&partnerID=40&md5=db5a9ef3b7c795ddbcfd3575ff436cdc","By allowing resources to be acquired on-demand and in variable amounts, cloud computing provides an appealing environment for deploying pilot projects and for performance testing of Web applications and services. However, setting up cloud environments for performance testing still requires a significant amount of manual effort. To aid performance engineers in this task, we developed a framework that integrates several common benchmarking and monitoring tools. The framework helps performance engineers to test applications under various configurations and loads. Furthermore, the framework supports dynamic server allocation based on incoming load using a response-time-aware heuristics. We validated the framework by deploying and stress-testing the MediaWiki application. An experimental evaluation was conducted aimed at comparing the response-time-aware heuristics against Amazon Auto-Scale. Copyright 2012 ACM. © 2012 Elsevier B.V., All rights reserved.",No load test generation.
"K.I.I., Kum, Ki Il I.; J., Kang, Jiyang; W., Sung, Wonyong","AUTOSCALER for C: An optimizing floating-point to integer C program converter for fixed-point digital signal processors","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034270457&partnerID=40&md5=91934fcf7b8dfd19f95a53f5b05a0d4f","A translator which converts C-based floating-point digital signal processing programs to optimized integer C versions is developed for convenient programming and efficient use of fixed-point digital signal processors (DSPs). It not only converts data types and supports automatic scaling, but also conducts shift optimization to enhance execution speed. Since the input and output of this translator are ANSI C compliant programs, it can be used for any fixed-point DSP that supports ANSI C compiler. The number of shift operations that are required for scaling in the converted integer programs is reduced by equalizing the integer word-lengths of relevant variables and constants. For an optimal reduction, a cost function that represents the overhead of scaling is formulated by considering the data-path of a target processor, program parsing, and profiling results. This cost function is then minimized by using either integer linear programming or simulated annealing algorithms. The translated integer C codes are 5-400 times faster than the floating-point versions when applied to TMS320C50, TMS320C60 and Motorola 56000 DSPs. © 2007 Elsevier B.V., All rights reserved.",No load test generation.
"Y.A., Khulief, Yehia A.","Pieced-interval modal response of elastic systems with high-frequency excitations","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101020792&partnerID=40&md5=4024f27b43ce3dc08a41a80b9a59373d","The current technology manifests a great demand for high precision and high positioning accuracy in many engineering applications that range from robot manipulators and high-speed flexible mechanisms to supercritical rotors and the space deployable structures. A reliable dynamic model is crucial to the performance evaluation and precisian control of such elastic mechanical systems. The modeling problem becomes even more difficult when higher frequencies of such large scale systems are excited. A pieced-interval modal analysis scheme is introduced to alleviate the problem of handling a system of widespread eignspectrum. A set of energy indices is defined to monitor the energy content in higher modes. The numerical scheme invokes a modal transformation with changeable basis that accounts for higher modes only when they are excited, and picked up more proportional values of the systems total energy. Numerical examples are presented to demonstrate the applicability of the developed method and to display its potential in performing efficient modeling of the dynamic behavior of such systems. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"M., Dalgitsis, Michail; G., Kibalya, Godfrey; M.A., Serrano, Maria A.; J., Serra, Jordi; A., Antonopoulos, Angelos","Exploiting 6G RAN and Core Network Information for Intelligent Edge-Cloud Service Orchestration","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010647644&partnerID=40&md5=f50a0d064d0875fec8416a5f775623d5","The advent of 5G networks and the imminent evolution to 6 G have driven significant changes in mobile network architecture, emphasizing edge-cloud integration, network softwarization, and Application Programmable Interface (API)-driven service delivery. To optimize resource allocation, service scaling, and service migration, mobile network operators must leverage both Radio Access Network (RAN) and Core Network (CN) information in orchestration frameworks. This paper introduces a novel cloud-native orchestration framework designed for 5G/6G networks, incorporating RAN and CN data to enable dynamic and efficient management of edge services. The framework includes three key orchestration strategies: (i) Horizontal Service Instance Autoscaler (HSIA), which adjusts service availability based on the number of active users derived from core network session data; (ii) Horizontal Service Resource Autoscaler (HSRA), which scales resources based on aggregated radio traffic from base stations; and (iii) Horizontal Service EdgeCloud Migration (HSECM), which dynamically migrates services to the cloud when edge resources are insufficient. Experimental results demonstrate the effectiveness of these strategies in reducing energy consumption and minimizing users affected by service migration (UASM) while maintaining service quality. The findings highlight the transformative potential of mobile network-aware orchestration for enabling more sustainable and adaptive edge service deployments in next-generation networks. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"B., Fodor, Balàzs; B., Sonkoly, Balazs","Improved Performance Control of Cloud-Native Microservices in the Edge with Proactive Autoscaling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012180015&partnerID=40&md5=996fc6df302d3a3f696de61dd3e4606e","Shifting from cloud to edge computing offers the advantage of being closer to the user, which improves latency and helps meet performance-related Service Level Agreement (SLA) requirements. However, the limited resources at the edge necessitate efficient resource scaling to handle fluctuating user demand. To maximize resource utilization, microservices architecture is favored over traditional monolithic approaches, allowing independent scaling of components so that only those needing extra resources are adjusted. Yet, standard reactive scaling methods may struggle to cope with unpredictable user traffic, leading to potential SLA violations. This underscores the need for proactive scaling solutions, where machine learning can play a key role in meeting diverse SLA requirements. In this work, we address these challenges by introducing a machine learning (ML) based proactive scaling framework for microservices in the edge. Our contribution is threefold, first we analyze several ML algorithms, identifying those that can be effectively applied in scaling. Second, we design and implement a scaling system that is capable of collecting metrics at multiple levels and making scaling decisions using ML models to ensure that the application meets the requirements specified in the SLA. Third, the system's efficiency is analyzed by measurements executed in a real environment, where we scale our test microservices-based application. Results show that the system can outperform the Kubernetes' Horizontal Pod Autoscaler in terms of SLA awareness without significant additional resource allocation, making it suitable for the edge. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"T., Theodoropoulos, Theodoros; Y.S., Singh Patel, Yashwant Singh; U., Zdun, Uwe; P., Townend, Paul; I., Korontanis, Ioannis; A., Makris, Antonios; K., Tserpes, Konstantinos","GraphOpticon: A Global proactive horizontal autoscaler for improved service performance & resource consumption","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007654758&partnerID=40&md5=c88d5d04e31c041940c6a51cc2347cab","The increasing complexity of distributed computing environments necessitates efficient resource management strategies to optimize performance and minimize resource consumption. Although proactive horizontal autoscaling dynamically adjusts computational resources based on workload predictions, existing approaches primarily focus on improving workload resource consumption, often neglecting the overhead introduced by the autoscaling system itself. This could have dire ramifications on resource efficiency, since many prior solutions rely on multiple forecasting models per compute node or group of pods, leading to significant resource consumption associated with the autoscaling system. To address this, we propose GraphOpticon, a novel proactive horizontal autoscaling framework that leverages a singular global forecasting model based on Spatio-temporal Graph Neural Networks. The experimental results demonstrate that GraphOpticon is capable of providing improved service performance, and resource consumption (caused by the workloads involved and the autoscaling system itself). As a matter of fact, GraphOpticon manages to consistently outperform other contemporary horizontal autoscaling solutions, such as Kubernetes’ Horizontal Pod Autoscaler, with improvements of 6.62% in median execution time, 7.62% in tail latency, and 6.77% in resource consumption, among others. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"Y., Jiang, Yi; J., Xue, Jin; K., Hu, Kun; T., Chen, Tianxiang; T., Wu, Tong","Saver: a proactive microservice resource scheduling strategy based on STGCN","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197635832&partnerID=40&md5=f7c6a0914df42eab9112087a5f2df795","As container technology and microservices mature, applications increasingly shift to microservices and cloud deployment. Growing microservices scale complicates resource scheduling. Traditional methods, based on fixed thresholds, are simple but lead to resource waste and poor adaptability to traffic spikes. To address this problem, we design a new resource scheduling strategy Saver based on the container cloud platform, which combines a microservice request prediction model with a microservice performance evaluation model that predicts SLO (Service Level Objective) violations and a heuristic algorithm to solve the optimal resource scheduling for the cluster. We deploy the microservices open-source project sock-shop in a Kubernetes cluster to evaluate Saver. Experimental results show that Saver saves 7.9% of CPU resources, 13% of the instances, and reduces the SLO violation rate by 31.2% compared to K8s autoscaler. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"W., Ding, Wenjie; Z., Liu, Zhihao; X., Lu, Xuhui; X., Du, Xiaoting; G., Xiao, Guanping","KPAMA: A Kubernetes based tool for Mitigating ML system Aging","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219075919&partnerID=40&md5=32c997ffeaa15251008fae72745d90af","As machine learning (ML) systems continue to evolve and be applied, their user base and system size also expand. This expansion is particularly evident with the widespread adoption of large language models. Currently, the infrastructure supporting ML systems, such as cloud services and computing hardware, which are increasingly becoming foundational to the ML system environment, is increasingly adopted to support continuous training and inference services. Nevertheless, it has been shown that the increased data volume, complexity of computations, and extended run times challenge the stability of ML systems, efficiency, and availability, precipitating system aging. To address this issue, we develop a novel solution, KPAMA, leveraging Kubernetes, the leading container orchestration platform, to enhance the autoscaling of computing workflows and resources, effectively mitigating system aging. KPAMA employs a hybrid model to predict key aging metrics and uses decision and anti-oscillation algorithms to achieve system resource autoscaling. Our experiments indicate that KPAMA markedly mitigates system aging and enhances task reliability compared to the standard Horizontal Pod Autoscaler and systems without scaling capabilities. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"X., Li, Xiaolong; R., Deng, Ruiting; J., Wei, Jianhao; X., Wu, Xin; J., Chen, Jiayuan; C., Yi, Changyan; J., Cai, Jun; D.(., Niyato, Dusit (Tao); X.S., Shen, Xuemin Sherman","AIGC-Driven Real-Time Interactive 4D Traffic Scene Generation in Vehicular Networks","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218874094&partnerID=40&md5=74e80acf91d64495e0e115c866872c82","Real-time, interactive 4D traffic scene generation enables rapid digital twinning of traffic scenarios, improving management and decision-making in intelligent transportation systems. However, current text-to-video models, such as Sora, struggle to maintain the temporal coherence of traffic elements and interact with dynamic environments and users when generating 4D scenes. This article introduces a novel cloud-edge-terminal collaborative framework that leverages Artificial Intelligence-Generated Content (AIGC) in vehicular networks to tackle these challenges, ensuring long-term coherence and improved interactivity. The framework presents a comprehensive architecture for real-time interactive 4D scene generation, encompassing data collection, management, model pre-training, fine-tuning, and inference. We examine key design requirements and challenges, demonstrating that our microservice-based framework enables the system to generate and update 4D traffic scenes in real time, effectively responding to traffic data and user inputs. To the best of our knowledge, this is the first successful implementation of real-time, interactive 4D traffic scene generation. Performance evaluations show the superiority of our framework, powered by microservice-based code fine-tuning, over traditional frameworks. Finally, we discuss future research directions to enhance AIGC-driven 4D traffic scene generation. © 2025 Elsevier B.V., All rights reserved.",No load test generation.
"S., Smith, Sheldon; E., Robinson, Ethan; T., Frederiksen, Timmy; T., Stevens, Trae; T., Cerny, Tomas; M., Bures, Miroslav; D., Taibi, Davide","Benchmarks for End-to-End Microservices Testing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174966908&partnerID=40&md5=828eed7822a976223a82f379523e7706","Testing microservice systems involves a large amount of planning and problem-solving. The difficulty of testing microservice systems increases as the size and structure of such systems become more complex. To help the microservice community and simplify experiments with testing and traffic simulation, we created a test benchmark containing full functional testing coverage for two well-established open-source microservice systems. Through our benchmark design, we aimed to demonstrate ways to overcome certain challenges and find effective strategies when testing microservices. In addition, to demonstrate our benchmark use, we conducted a case study to identify the best approaches to take to validate a full coverage of tests using service-dependency graph discovery and business process discovery using tracing. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"M., Abdullah, Muhammad; W., Iqbal, Waheed; A., Erradi, Abdelkarim; F., Bukhari, Faisal","Learning predictive autoscaling policies for cloud-hosted microservices using trace-driven modeling","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079072877&partnerID=40&md5=852132aff8b32754fa91d3d7f321ac2f","Autoscaling methods are important to ensure response time guarantees for cloud-hosted microservices. Most of the existing state-of-the-art autoscaling methods use rule-based reactive policies with static thresholds defined either on monitored resource consumption metrics such as CPU and memory utilization or application-level metrics such as the response time. However, it is challenging to determine the most appropriate threshold values to minimize resource consumption and performance violations. Whereas, predictive autoscaling methods can help to address these challenges. These methods require considerable time to collect sufficient performance traces representing different resource provisioning possibilities for a target infrastructure to train a useful predictive autoscaling model. In this paper, we tackle this problem by proposing a system that models the response time of microservices through stress testing and then uses a trace-driven simulation to learn a predictive autoscaling model for satisfying response time requirements automatically. The proposed solution reduces the need for collecting performance traces to learn a predictive autoscaling model. Our experimental evaluation on AWS cloud using a microservice under realistic dynamic workloads validates the proposed solution. The validation results show excellent performance to satisfy the response time requirement with only 4.5% extra cost for using the proposed autoscaling method compared to the reactive autoscaling method. © 2020 Elsevier B.V., All rights reserved.",No load test generation. However interesting study on common workload traces.
"I.M., Ramadan, Ibrahim Mohamed; C., Centofanti, Carlo; A., Marotta, Andrea; F., Graziosi, Fabio","Evaluating Kubernetes Distributions: Insights from Stress Testing Scenarios","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001662774&partnerID=40&md5=15310893e7dc6330dfa680ff18d1262d","In the modern era, Kubernetes has gained widespread adoption for its capability to manage containerized applications through automation. With a variety of Kubernetes distributions aimed at enhancing and extending core functionalities, there's a pressing need for a thorough evaluation to identify the most efficient solutions for orchestrating workloads. This study aims to assess a variety of Kubernetes distributions including k3s, k3d, Kind, MicroK8s, Minikube, and standard streamlined Kubernetes, focusing on pod orchestration efficiency under stress conditions. Utilizing the open-source tool kube-burner for stress testing, we propose a testing architecture and methodology to measure pod initiation, scheduling, and overall readiness times. The results show that streamlined Kubernetes and k3d outperform the others overall, with Minikube excelling in initiation time, while Kind's performance suggests it might not be suitable for production environments. This analysis highlights the trade-offs and performance impacts of different Kubernetes distributions, guiding users in selecting the most efficient option for their needs. © 2025 Elsevier B.V., All rights reserved.",Performance evaluation of different Kubernetes distributions using an open-source tool. Statically defined load tests.
"M., Niswar, Muhammad; R.A., Safruddin, Reza Arisandy; A., Bustamin, Anugrayani; I., Aswad, Iqra","Performance evaluation of microservices communication with REST, GraphQL, and gRPC","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198037286&partnerID=40&md5=73a049caec78fe62bf23eee1f5f833d7","Microservice architecture has become the design paradigm for creating scalable and maintainable software systems. Selecting the proper communication protocol in microservices is critical to achieving optimal system performance. This study compares the performance of three commonly used API protocols: REST, GraphQL, and gRPC, in microservices architecture. In this study, we established three microservices implemented in three containers and each microservice contained a Redis and MySQL database. We evaluated the performance of these API protocols using two key performance metrics: response time and CPU Utilization. This study performs two distinct data retrieval: fetching flat data and fetching nested data, with a number of requests ranging from 100 to 500 requests. The experimental results indicate that gRPC has a faster response time, followed by REST and GraphQL. Moreover, GraphQL shows higher CPU Utilization compared to gRPC and REST. The experimental results provide insight for developers and architects seeking to optimize their microservices communication protocols for specific use cases and workloads. © 2024 Elsevier B.V., All rights reserved.",Performance evaluation of microservice communication protocols. Fixed load tests.
"T., Zeng, Tao; Z., Xu, Zichen; D., Wu, Dan; X., Li, Xiaoling; B., Liu, Biyong; H., Hu, Haichuan; S., Tan, Shuang; Y., Tan, Yusong; C., Xu, Chenren; C.C., Stewart, Christopher C.","Exploring nonintrusive measurements of spatio-temporal portrait of microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163591426&partnerID=40&md5=5cc01462fce48c70647076544f3fa098","As cloud native technology advances, the scale and complexity of applications built on microservice architecture continue to expand, leading to increasingly intricate differences between software within the same application. Microservice applications, offering high flexibility, are deployed in data centers as black boxes from the users' perspective, leaving them with no insight into the orchestration of cloud service providers. Consequently, users face challenges in promptly recognizing performance imbalances within their deployed applications. Meanwhile, cloud service providers may cut costs by offering a mix of qualified and unqualified services, potentially deceiving users. To enhance the understanding of microservice application organization, we propose a non-intrusive measurement framework, termed NMPI. NMPI facilitates rapid identification of microservice application defects, offering insights into cloud services and detecting fraudulent behavior in microservice-based applications. We model microservice applications using a queue analysis-based approach and filter the dominant frequency components of average response time signals by employing k-means on the fast fourier transform (FFT). Our model constructs a library of performance portraits for various software, with these portraits resembling human fingerprints that carry and mark the software's internal information. Utilizing a two-tier microservices-based application incorporating a database as a case study allows us to demonstrate the effectiveness of NMPI. Our experimental results show that NMPI can produce differentiable profiles of data service performance portraits across a diverse and extensive range of workloads, enabling the identification of software types and the analysis of performance conditions. © 2024 Elsevier B.V., All rights reserved.","Performance measurement framework, no load test generation."
"A., Vasilevskii, Aleksei; O., Kachur, Oleksandr","Self-Service Performance Testing Platform for Autonomous Development Teams","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193933068&partnerID=40&md5=198b91dc3a71a66321491df783768066","In the modern fast paced and highly autonomous software development teams, it's crucial to maintain a sustainable approach to all performance engineering activites, including performance testing. The high degree of autonomy often results in teams building their own frameworks that are not used consistently and may be abandoned due to lack of support or integration with existing infrastructure, processes and tools. To address these challenges, we present a self-service performance testing platform based on open-source software, that supports distributed load generation, historical results storage and a notification system to trigger alerts in Slack messenger. In addition, it integrates with GitHub Actions to enable developers running load tests as part of their CI/CD pipelines. We'd like to share some of the technical solutions and the details of the decision-making process behind the performance testing platform in a scale-up environment, our experience in building this platform and, most importantly, rolling it out to autonomous development teams and onboarding them into the continuous performance improvement process. © 2024 Elsevier B.V., All rights reserved.","Performance testing platform. Does not focus on microservices, does not consider autoscaling, does not provide automatic test generation"
"A.V., Papadopoulos, Alessandro V.; A., Ali-Eldin, Ahmed; K.E., Årzén, Karl Erik; J., Tordsson, Johan; E., Elmroth, Erik","PEAS: A performance evaluation framework for auto-scaling strategies in cloud applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674305&partnerID=40&md5=07102f7edc77372048f434c22af21adf","Numerous auto-scaling strategies have been proposed in the past few years for improving various Quality of Service (QoS) indicators of cloud applications, for example, response time and throughput, by adapting the amount of resources assigned to the application to meet the workload demand. However, the evaluation of a proposed auto-scaler is usually achieved through experiments under specific conditions and seldom includes extensive testing to account for uncertainties in the workloads and unexpected behaviors of the system. These tests by no means can provide guarantees about the behavior of the system in general conditions. In this article, we present a Performance Evaluation framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The evaluation is formulated as a chance constrained optimization problem, which is solved using scenario theory. The adoption of such a technique allows one to give probabilistic guarantees of the obtainable performance. Six different auto-scaling strategies have been selected from the literature for extensive test evaluation and compared using the proposed framework. We build a discrete event simulator and parameterize it based on real experiments. Using the simulator, each auto-scaler's performance is evaluated using 796 distinct real workload traces from projects hosted on the Wikimedia foundations' servers, and their performance is compared using PEAS. The evaluation is carried out using different performance metrics, highlighting the flexibility of the framework, while providing probabilistic bounds on the evaluation and the performance of the algorithms. Our results highlight the problem of generalizing the conclusions of the original published studies and show that based on the evaluation criteria, a controller can be shown to be better than other controllers. © 2019 Elsevier B.V., All rights reserved.",Policy evaluation given workloads. No focus on load test generation. Used statistically extracted workloads for evaluation.
"Authors","Title","Link","Abstract",Reason for Exclusion
"Y., Guo, Yonghe; M., Yang, Meng; X., Li, Xiaolong; C., Jiang, Congfeng; J., Liu, Junming; L., Lao, Lingjia; S., Chen, Shijie; W., Tang, Wei","Advances in Full-Link Stress Testing Technology for Cloud-Native Information System","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219109504&partnerID=40&md5=8e3ce3560704b842630fe91c8f9cfc08","Amidst the ongoing digital transformation, enterprises are progressively adopting microservice architectures to achieve greater flexibility and resilience. Nevertheless, this shift introduces considerable complexity to system testing. This paper presents a comprehensive review of full-link stress testing, a methodology designed to evaluate system performance boundaries within production environments, utilizing production-grade business data and realistic traffic simulations. We begin by discussing the major limitations of traditional stress testing methods, particularly regarding environmental discrepancies, cost efficiency, and risk management. Following this, we examine a typical implementation framework for full-link stress testing, detailing phases from initial planning and monitoring to post-test analysis, ensuring rigorous and accurate evaluation. Key technical challenges, such as test environment adaptation, automated link management, and data preprocessing, are analyzed, with tailored solutions summarized for each. Finally, we explore the growing role of full-link stress testing in enhancing enterprise performance optimization and business continuity, underscoring its significance in supporting scalable and resilient architectures amid digital transformation. © 2025 Elsevier B.V., All rights reserved.",Review focusing on stress-link testing.
"F.A., Silva, Francisco Airton; F.A.M., Trinta, Fernando Antonio Mota; M.S., Bonfim, Michel S.; J.A.F.D.A.F.D., de Macêdo, José António Fernandes D. Antônio Fernandes D.; P.A.L., Rego, Paulo Antonio Leal; V., Lagrota, Vinícius","Performance Evaluation of Cloud Native Applications: A Systematic Mapping Study","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009973373&partnerID=40&md5=ecf3483a53d1c2d2b6cc6c6ef0b8d8d2","Cloud-native applications leverages cloud environments for enhanced scalability, resilience, and agility. Using containerization technologies like Docker enables modular deployments. Despite their advantages, effectively evaluating the performance of these dynamic and distributed applications presents challenges. Cloud-native application performance is complex because diverse metrics–including response times, throughput, and resource utilization–are used across varied workloads. Organizations face hurdles in selecting appropriate evaluation tools, defining specific objectives, and gaining insights into microservices and orchestration. A more structured approach is essential to optimizing deployments and meeting performance expectations in cloud-native environments. This paper presents a systematic mapping study that examined 1158 initial papers and refined them to 32 primary studies. It meticulously analyzes key metrics, software tools, objectives, and contributions related to performance evaluation in cloud-native applications. Visual representations with bubble plots were used as cross-correlation analysis, revealing patterns and research gaps. This study provides insights that guide future research directions, advocating for methodological frameworks to enhance the assessment and optimization of cloud-native application performance. © 2025 Elsevier B.V., All rights reserved.",Review of performance evaluation for cloud.
"B., Çiftçi, Berkcan; B., Ciloglugil, Birol","A Review of Comparative Studies on Performance Evaluation of Communication Mechanisms for Microservices","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010183245&partnerID=40&md5=2f23139d9c6cf4fcfe4190b3e852c2be","As organizations increasingly adopt microservices to improve agility and scalability, selecting the right communication mechanisms becomes critical for system efficiency and fault tolerance. Therefore, a variety of communication technologies, including but not limited to HTTP REST, HTTP/2, gRPC, GraphQL, MQTT, AMQP, Apache Kafka, RabbitMQ, NATS Streaming, RocketMQ, Apache Pulsar, Redis, ActiveMQ, ActiveMQ Artemis, and WebSocket, are employed in a wide range of applications. Benchmarking frameworks such as Apache JMeter and the OpenMessaging Benchmark Framework are utilized to evaluate performances of different communication technologies in a variety of environments. For this purpose, performance evaluation metrics such as throughput, latency and response time are assessed to compare the performances of inter-process communication (IPC) technologies. Hence, this paper presents a review of studies that provide a comparative analysis of communication mechanisms employed in microservice architectures, focusing on the communication technologies, benchmarking frameworks, and performance evaluation metrics utilized for IPC. The findings indicate that HTTP REST and gRPC are the most frequently evaluated synchronous communication technologies because of their simplicity and compatibility. Besides, Kafka and RabbitMQ, which enhance scalability and fault tolerance, are identified as the most widely assessed asynchronous communication technologies. In addition, Apache JMeter is determined as the most popular benchmarking framework to conduct comparative performance evaluations. Finally, the most commonly employed performance evaluation metrics are detected as throughput and latency. © 2025 Elsevier B.V., All rights reserved.",Review of studies on evaluation of protocols performance.
"S., Lakshan, Sivagnanasothy; S., Hussain, Sahdiya","A Review of AI-Driven Techniques for Cost Optimization in Kubernetes Environments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009455938&partnerID=40&md5=c9bd0a4689503681c9f9de5f2fa53806","Kubernetes plays a pivotal role in managing cloud-native applications by automating deployment, scaling, and resource allocation. Despite its robust capabilities, it faces significant challenges in cost optimization due to over-provisioning, under-utilization, and the inability to effectively predict workload variations. Native Kubernetes autoscaling mechanisms such as Horizontal Pod Autoscaler (HPA), Vertical Pod Autoscaler (VPA) and Cluster Autoscaler (CA) often fail to strike an optimal balance between resource utilization and application performance. To address these limitations, recent researches has focused on leveraging AI and machine learning (ML) techniques to enhance Kubernetes' resource allocation efficiency. This review comprehensively examines the state-of-the-art advancements in AI-driven cost optimization strategies for Kubernetes environments. It explores predictive scaling algorithms, reinforcement learning-based orchestration, time-series forecasting models and clustering methods aimed at improving resource utilization and reducing operational expenses. The integration of ML technologies has demonstrated remarkable success in enabling proactive and dynamic scaling decisions, enhancing cost-effectiveness while maintaining high application reliability. Key studies highlight the potential of hybrid ML models, multi-objective optimization approaches and advanced metrics-driven strategies to address the complexities of resource allocation. These methodologies not only minimize inefficiencies but also provide adaptive solutions to evolving workload demands. Furthermore, the paper identifies gaps in existing approaches, such as the limited incorporation of multi-dimensional metrics and the need for continuous model adaptability. This review underscores the transformative potential of AI-driven techniques in Kubernetes environments, presenting a roadmap for future research to refine and expand these strategies. The findings aim to guide organizations in adopting intelligent autoscaling mechanisms to optimize costs, improve resource utilization and drive innovation in cloud-native application management. © 2025 Elsevier B.V., All rights reserved.",Review of techniques for cost reduction in Kubernetes.
"A., Gambi, Alessio; W., Hummer, Waldemar; S., Dustdar, Schahram","Testing elastic systems with surrogate models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883682196&partnerID=40&md5=603da35799da82d23efe79c62aa1ccf0","We combine search-based test case generation and surrogate models for black-box system testing of elastic systems. We aim to efficiently generate tests that expose functional errors and performance problems related to system elasticity. © 2013 IEEE. © 2014 Elsevier B.V., All rights reserved.","Similar concepts. No focus on microservices, no model encodings, no implementations or experiments."
"M., Camilli, Matteo; A.A., Janes, Andrea A.; B., Russo, Barbara","Automated test-based learning and verification of performance models for microservices systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124274286&partnerID=40&md5=ac72f8f21db38c63a2269e714ab2826d","Effective and automated verification techniques able to provide assurances of performance and scalability are highly demanded in the context of microservices systems. In this paper, we introduce a methodology that applies specification-driven load testing to learn the behavior of the target microservices system under multiple deployment configurations. Testing is driven by realistic workload conditions sampled in production. The sampling produces a formal description of the users’ behavior through a Discrete Time Markov Chain. This model drives multiple load testing sessions that query the system under test and feed a Bayesian inference process which incrementally refines the initial model to obtain a complete specification from run-time evidence as a Continuous Time Markov Chain. The complete specification is then used to conduct automated verification by using probabilistic model checking and to compute a configuration score that evaluates alternative deployment options. This paper introduces the methodology, its theoretical foundation, and the toolchain we developed to automate it. Our empirical evaluation shows its applicability, benefits, and costs on a representative microservices system benchmark. We show that the methodology detects performance issues, traces them back to system-level requirements, and, thanks to the configuration score, provides engineers with insights on deployment options. The comparison between our approach and a selected state-of-the-art baseline shows that we are able to reduce the cost up to 73% in terms of number of tests. The verification stage requires negligible execution time and memory consumption. We observed that the verification of 360 system-level requirements took ∼1 minute by consuming at most 34 KB. The computation of the score involved the verification of ∼7k (automatically generated) properties verified in ∼72 seconds using at most ∼50 KB. © 2022 Elsevier B.V., All rights reserved.",Statistically extracted performance tests. Mostly concentrates on benchmarking of deployment configurations. Statically defined load tests.
"R., Aghili, Roozbeh; Q., Qin, Qiaolin; H., Li, Heng; F., Khomh, Foutse","Understanding Web Application Workloads and Their Applications: Systematic Literature Review and Characterization","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215517324&partnerID=40&md5=605259d9c813a4e81045328acdcf98ca","Web applications, accessible via web browsers over the Internet, facilitate complex functionalities without local software installation. In the context of web applications, a workload refers to the number of user requests sent by users or applications to the underlying system. Existing studies have leveraged web application workloads to achieve various objectives, such as workload prediction and auto-scaling. However, these studies are conducted in an ad hoc manner, lacking a systematic understanding of the characteristics of web application workloads. In this study, we first conduct a systematic literature review to identify and analyze existing studies leveraging web application workloads. Our analysis sheds light on their workload utilization, analysis techniques, and high-level objectives. We further systematically analyze the characteristics of the web application workloads identified in the literature review. Our analysis centers on characterizing these workloads at two distinct temporal granularities: daily and weekly. We successfully identify and categorize three daily and three weekly patterns within the workloads. By providing a statistical characterization of these workload patterns, our study highlights the uniqueness of each pattern, paving the way for the development of realistic workload generation and resource provisioning techniques that can benefit a range of applications and research areas. © 2025 Elsevier B.V., All rights reserved.",Study about workloads. Systematic review.
"M., Ghorbian, Mohsen; M., Ghobaei-Arani, Mostafa; L., Esmaeili, Leila","A survey on the scheduling mechanisms in serverless computing: a taxonomy, challenges, and trends","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185925529&partnerID=40&md5=d3546cfba5afdb17fba44c9aab02d80c","In recent years, serverless computing has received significant attention due to its innovative approach to cloud computing. In this novel approach, a new payment model is presented, and a microservice architecture is implemented to convert applications into functions. These characteristics make it an appropriate choice for topics related to the Internet of Things (IoT) devices at the network’s edge because they constantly suffer from a lack of resources, and the topic of optimal use of resources is significant for them. Scheduling algorithms are used in serverless computing to allocate resources, which is a mechanism for optimizing resource utilization. This process can be challenging due to a number of factors, including dynamic behavior, heterogeneous resources, workloads that vary in volume, and variations in number of requests. Therefore, these factors have caused the presentation of algorithms with different scheduling approaches in the literature. Despite many related serverless computing studies in the literature, to the best of the author’s knowledge, no systematic, comprehensive, and detailed survey has been published that focuses on scheduling algorithms in serverless computing. In this paper, we propose a survey on scheduling approaches in serverless computing across different computing environments, including cloud computing, edge computing, and fog computing, that are presented in a classical taxonomy. The proposed taxonomy is classified into six main approaches: Energy-aware, Data-aware, Deadline-aware, Package-aware, Resource-aware, and Hybrid. After that, open issues and inadequately investigated or new research challenges are discussed, and the survey is concluded. © 2024 Elsevier B.V., All rights reserved.",Survey on scheduling mechanisms.
"S., Bello-Trejo, Sebastian; X., Limón, Xavier; J.O., Ocharán-Hernández, Jorge Octavio; L.A., Hernández-González, Lizbeth Alejandra","System-Oriented Testing on the Microservices Architecture: A Systematic Literature Review","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216761869&partnerID=40&md5=fb12f0d193381df576c829ed73b54978","The Microservice Architecture (MSA) is gaining popularity in the industry and literature as a raising research topic. This architectural style brings advantages such as independent development, testing, and deployment. However, it is also plagued with many challenges due to its distributed nature. In the case of testing, complex dependencies, heterogeneous execution environments, and independent development teams make it difficult to test microservices using traditional methods and techniques. Our goal is to provide a collection of tests, tools, state-of-the-art solution proposals, as well as relevant challenges. To achieve this, we conducted a Systematic Literature Review following the guidelines presented by Kitchenham. We searched through search engines including IEEE Xplore, ACM Digital Library, ScienceDirect, SpringerLink, and Wiley Online Library. In total, we retrieved 13,556 articles and thoroughly analyzed them. Through this process, we identified 44 primary studies that were suitable for data extraction and synthesis. From our analysis, we identified 12 testing strategies that are applicable to the MSA. Additionally, we found proposed solutions for automated testing along with microservices benchmarks. Furthermore, we discovered a set of testing tools and challenges specifically related to MSA-based system testing. The main challenges we encountered include automated testing, performance testing, and finding realistic MSA-based systems. Our study aims to benefit software engineering practitioners and researchers interested in microservices testing by providing a comprehensive understanding of the current landscape of testing for MSA-based systems. © 2025 Elsevier B.V., All rights reserved.",Systematic review on microservice testing. 
"A., Alvarado, Anderson; A., Castro, Angie; K., López, Kevin; M., Rivera, Mauren; M.J.H., Curiel, Mariela J.H.","MiMoQ: A System for Experimentation of Microservice-Based Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214014994&partnerID=40&md5=ac080a0aa29e39f37f5cbd561fcdd227","Microservices have emerged as an architectural style, where software architectures are broken up into loosely coupled components or services that can be developed, tested, deployed, scaled, operated, and upgraded independently. Thus many companies are re-architecting monolithic systems with microservices-based architecture to improve quality attributes (QA) such as scalability and performance. Once the refactoring is concluded, designers have to evaluate whether the expectations raised regarding quality attributes are met. This evaluation must be done in controlled environments and involves knowing the ecosystem used for deployment, as well as the available tools for monitoring and workload generation. In this paper, we present the MiMoQ tool for experimentation of microservice-based applications, which are deployed in Kubernetes. MiMoQ (Microservices Metrics of Quality) aims to facilitate the development of the experimental process, ranging from the generation of workloads to the collection and analysis of metrics, thus freeing experimenters from tasks such as the selection, installation, configuration, and integration of different types of tools for monitoring and workload generation. In this way, performance analysts can focus on the design of experiments, analysis of results, and continuous improvement of applications. © 2025 Elsevier B.V., All rights reserved.",Tool for microservices experimentation. No load test generation.
"L., Giamattei, Luca; A., Guerriero, Antonio; I., Malavolta, Ivano; C., Mascia, Cristian; R., Pietrantuono, Roberto; S., Russo, Stefano","Identifying Performance Issues in Microservice Architectures through Causal Reasoning","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196362887&partnerID=40&md5=8e8953b181e926bc2310200161eaf665","Evaluating the performance of Microservices Architectures (MSA) is essential to ensure their proper functioning and meet end-user satisfaction. For MSA performance analysts, one of the most challenging tasks is to determine the cause of any deviation of relevant metrics from the specified range.We introduce CAR-PT (CAusal-Reasoning-driven Performance Testing), a model-based technique for workload generation designed for the performance testing of MSA. CAR-PT leverages causal reasoning to effectively explore the space of operational conditions, with the goal of identifying those that lead to performance issues. Preliminary results show that CAR-PT is effective in generating configurations for discovering performance issues of an MSA. © 2024 Elsevier B.V., All rights reserved.","Uses causal reasoning for load test generation of microservices. Does not consider autoscaler, or varying workloads, only generates a static load."
"R., Matar, Raghad; J., Jahić, Jasmin","MDEPT: Microservices Design Evaluator and Performance Tester","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203589301&partnerID=40&md5=82f01dcd77c4dbf0ab4498076431d32f","In microservices-based systems, architects find it hard to reason about the impact of their design decisions on performance before implementing them. While definitions of anti-patterns help to avoid inadequate design decisions, they are context-dependent. Static analysis of software design can identify constructs that conform to anti-patterns. However, this is not suitable for quantifying the extent to which these anti-patterns would affect system performance. Ideally, we should be able to predict the dynamic behavior of a system before it is implemented. However, existing approaches either cannot achieve this because they analyze the design statically or require complex and laborious modeling and simulation approaches. To address this challenge, we previously introduced a conceptual solution idea that facilitates rapid evaluation of high-level architectural models by combining both static and dynamic analysis. In this paper, we build upon our previous work and introduce the Microservices Design Evaluator and Performance Tester (MDEPT) approach. Mainly, we formalize modeling specifications for microservices systems, introduce a fully functional toolchain for our approach, and present the evaluation results. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"I., Tarhri, Ilyass; D., Allaki, Driss; H., Kamal Idrissi, Hamza","Towards Generating a Dataset for Failure Prediction in Microservices Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202613636&partnerID=40&md5=69dd50dbf264f3c6df3c4fb303054a68","Microservices architecture has gained significant traction in modern software development due to its agility and scalability. However, using artificial intelligence models to effectively predict the failure of microservices workloads poses challenges, as there is a lack of comprehensive datasets that provide dedicated data to train these models. This paper presents a methodology for generating a dataset specifically designed for failure prediction tasks in microservices applications. We detail our approach for capturing relevant metrics, including the chosen workload generation method and data collection techniques. The paper then describes the characteristics of the generated dataset, including its size, granularity, and captured workload failure scenarios and patterns. Evaluation results demonstrate the utility of the obtained dataset in gaining insights into microservices performance. The aim of this work is to offer a new dataset with useful workload metrics, serving as a valuable resource for researchers and practitioners in the field. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"T.F., da Silva Pinheiro, Thiago Felipe; M.A.T., Mialaret, Marco Aurélio Tomaz; P., Pereira, Paulo; L.C.S., Lins, Luan Carlos Soares; D., da Silva, Daliton; J.R., Dantas, Jamilson Ramalho; P.R.M., MacIel, Paulo Romero Martins","Performance Modeling of Microservices with Circuit Breakers using Stochastic Petri Nets","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197342863&partnerID=40&md5=f4fb903456f8f700b7b8427a14a20d44","Circuit Breakers (CBs) are critical for containerized microservices infrastructures when managing a surge in workload, as they can serve as a protection mechanism to prevent system overload and meet service level agreements (SLAs). In the event of a sudden increase in workload, microservices (MSs) can face challenges such as resource exhaustion and many discarded requests. CBs can help mitigate these issues by monitoring the services and, if necessary, redirecting the flow of requests to another infrastructure. However, it is difficult to evaluate the behavior of these infrastructures and CB mechanisms in a production environment. To address the above issues, this paper proposes a model using stochastic Petri nets (SPNs) to represent these infrastructures and their auto-scaling mechanisms, the MSs, the CBs, the incoming external arrival rate, and the workload generated between the MSs. Service providers can estimate metrics including circuit breaker activation (CBA), overload forwarding rate (OFR), containers utilization (Uc), unallocated containers (NUc), throughput (TP), discard probability (DP), and discard rate (DR). The model enables the performance evaluation of individual MSs and the entire microservice platform (MP). The work investigates how the microservices adapt to changing conditions and the trade-offs associated with different CB configurations. Using a real testbed, our solution was validated with a confidence interval (CI) of 95%. A case study was used to investigate the feasibility of the solution by evaluating its application in a real-world scenario. We found that the CBs reduced DR by 71.4% on average. This corresponds to an average number of 75,454 requests over 1 hour that were not discarded but forwarded to another infrastructure. © 2024 Elsevier B.V., All rights reserved.",Focus on simulation.
"F.B., Fava, Felipe Bedinotto; L.F., Laviola Leite, Luiz Felipe; L.F.A., da Silva, Luis Fernando Alves; P.R., da Silva Amalfi Costa, Pedro Ramires; A.G., Diniz Nogueira, Angelo Gaspar; A.F., Gobus Lopes, Amanda Fagundes; C., Schepke, Claudio; D.L., Kreutz, Diego Luis; R.B., Mansilha, Rodrigo Brandão","Assessing the Performance of Docker in Docker Containers for Microservice-Based Architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191754548&partnerID=40&md5=c5aee2184a583f6050158144efffb031","We provide a comprehensive and updated assessment of Docker versus Docker in Docker (DinD), evaluating its impact on CPU, memory, disk, and network. Using different workloads, we evaluate DinD's performance across distinct hardware platforms and GNU/Linux distributions on cloud Infrastructure as a Service (laaS) platforms like Google Compute Engine (GCE) and traditional server-based environments. We developed an automated tools suite to achieve our goal. We execute four well-known benchmarks on Docker and its nested-container variant. Our findings indicate that nested-containers require up to 7 seconds for startup, while the Docker standard containers require less than 0.5 seconds for Debian and Alpine operating systems. Our results suggest that Docker containers based on Debian consistently outperform their Alpine counter-parts, showing lower CPU latency. A key distinction among these Docker images lies in the varying number of installed libraries (e.g., stretching from 13 to 119) across different Linux distributions for the same system (e.g., MySQL). Furthermore, the number of events and CPU latency indicates that the influence of DinD over Docker proves that it is insignificant for both operating systems. In terms of memory, running containers of Debian-based images consume 20% more size of memory than those based on Alpine. No significant differences are between nested-containers and Dockers for disk and network IO. It is worth emphasizing that some of the disparities, such as a bigger memory footprint, appear to be a direct result of the software stack in use, including different kernel versions. libraries. and other essential packages. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"A.A., Avritzer, Alberto A.; M., Camilli, Matteo; A.A., Janes, Andrea A.; B., Russo, Barbara; C., Trubiani, Catia; A.V., Van Hoorn, André V.","Continuous Dependability Assessment of Microservice Systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186764267&partnerID=40&md5=9b253422f23378a63b52766433b38bec","In this paper, we overview the tutorial presented at the 16th European Conference on Software Architecture. The tutorial’s goal was to summarize the challenges and approaches for verification and validation in microservices systems. We introduced the PPTAM approach for dependability assessment. PPTAM employs a variety of architectural artifacts and steps, including the use of operational data obtained from production-level application performance management (APM) tools, the automated assessment of load tests based on defined scalability requirements, and the development of computationally efficient algorithms for software performance anti-pattern (SPA) detection that could be implemented in CI/CD pipelines. © 2024 Elsevier B.V., All rights reserved.",No load test generation. Focus on testing process evaluation.
"R., Shrestha, Raju; B., Nisha, Beebu","Performance Evaluation and Comparison of Microservices and Serverless Deployments in Cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183317577&partnerID=40&md5=4b052d2146f7821485361203dfc1df71","Microservices and serverless are arguably the two most widely used architectures today for deploying applications in the cloud. With both these technologies, applications can take advantage of faster delivery, lightweight, scalable, and lower development and maintenance costs. However, there are ongoing debates concerning which of these two architectures to use for deploying a given application. This paper evaluates and compares them quantitatively in terms of their performance and cost, based on a use case study for an image processing application. The study was conducted by deploying the application using the two technologies in the two major cloud platforms, Amazon AWS and Google Cloud. Results showed that serverless perform better in terms of performance and cost, whereas microservices show superiority in terms of memory use. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"N., Bartelucci, Nicolo; P., Bellavista, Paolo","A Practical Guide to Autoscaling Solutions for Next Generation Internet Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174572500&partnerID=40&md5=ce8c013400b3f805e3fba3ec63e9a08b","With the increasing opportunities offered by the cloud continuum (with the availability of distributed edge cloud nodes in addition to traditional datacenter nodes) and the associated growing complexity of distributed virtualized deployment environments, it is often not trivial to statically select the best autoscaling solution to optimally manage resource elasticity for a specific application and for its requirements. In addition, for example because the traffic to a next generation Internet application in the cloud continuum may be highly variable, there is the need of a high degree of adaptability to dynamically determine the best tradeoff between the usage of virtualized resources and its cost. The paper aims at providing a practical guide to the state-of-the-art of autoscaling technologies by describing, comparing, and evaluating them. In particular, we propose an original taxonomy to classify and label the most promising and emerging autoscaling technologies. This taxonomy allows us also to define criteria and practical guidelines on which autoscaling solution to select and integrate in next generation cloud continuum applications, and sheds some light on the most promising directions expected for future research work to fill the current technology gaps. © 2023 Elsevier B.V., All rights reserved.",No load test generation. Guidelines on autoscaler selection.
"G., Siachamis, George; J., Kanis, Job; W., Koper, Wybe; K., Psarakis, Kyriakos; M., Fragkoulis, Marios; A.V., Van Deursen, Arie Van; A., Katsifodimos, Asterios","Towards Evaluating Stream Processing Autoscalers","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163872494&partnerID=40&md5=baaa53f84d2f7316484ef81fe5112f88","In this work, we evaluate autoscaling solutions for stream processing engines. Although autoscaling has become a mainstream subject of research in the last decade, the database research community has yet to evaluate different autoscaling techniques under a proper benchmarking setting and evaluation framework. As a result, every newly proposed autoscaling solution only performs a shallow performance evaluation and comparison against existing solutions. In this paper, we evaluate autoscaling solutions by employing two streaming queries and a dynamic workload that follows a cosinus pattern. Our experiments reveal that current autoscaling techniques fail to account for generated lag due to rescaling or underprovisioning and cannot efficiently handle practical scenarios of intensely dynamic workloads. © 2025 Elsevier B.V., All rights reserved.",No load test generation. Satically defined load test.
"R., Matar, Raghad; J., Jahić, Jasmin","An Approach for Evaluating the Potential Impact of Anti-Patterns on Microservices Performance","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159050848&partnerID=40&md5=3f440086c99132e6faacd6a2466a1dd9","The microservices architectural style has been increasingly adopted in recent years due to its advantageous characteristics. While there exist design patterns for microservices that are theoretically beneficial for ensuring performance (and anti-patterns that should be avoided), they do not always lead to the expected impact. Some designs are solutions for achieving quality properties other than performance, and while they might be categorized as anti-patterns in theory, their influence on system performance can be minimal, depending on the context. Architects find it hard to reason about these trade-offs and the impact that design decisions will have on performance before implementing them. To solve this problem, in this paper, we propose an approach that enables rapid evaluation of high-level architectural models by combining both static and dynamic analysis. The static analysis identifies the design anti-patterns that are known to hinder system performance and guides the architect in reasoning about these design decisions. The approach then generates source code for the system under study based on the architectural design model for a subsequent dynamic analysis to assess whether the statically detected anti-patterns do indeed have a negative effect on the performance of the analyzed system. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"Q.L., Trieu, Quoc Lap; B., Javadi, Bahman; J., Basilakis, Jim; A.N., Toosi, Adel N.","Performance Evaluation of Serverless Edge Computing for Machine Learning Applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150675868&partnerID=40&md5=4fb352dd6fa2f96f606aee14dcdabe73","Next generation technologies such as smart health-care, self-driving cars, and smart cities require new approaches to deal with the network traffic generated by the Internet of Things (IoT) devices, as well as efficient programming models to deploy machine learning techniques. Serverless edge computing is an emerging computing paradigm from the integration of two recent technologies, edge computing and serverless computing, that can possibly address these challenges. However, there is little work to explore the capability and performance of such a technology. In this paper, a comprehensive performance analysis of a serverless edge computing system using popular open-source frameworks, namely, Kubeless, OpenFaaS, Fission, and funcX is presented. The experiments considered different programming languages, workloads, and the number of concurrent users. The machine learning workloads have been used to evaluate the performance of the system under different working conditions to provide insights into the best practices. The evaluation results revealed some of the current challenges in serverless edge computing and open research opportunities in this emerging technology for machine learning applications. © 2023 Elsevier B.V., All rights reserved.",No load test generation.
"C.G., Ramonell, Carlos G.; R.A., Chacón, Rolando A.","Towards Automated Pipelines for Processing Load Test Data on a HS Railway Bridge in Spain using a Digital Twin","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137109993&partnerID=40&md5=1c4ede63e282754a00c37bfb475526c3","This document presents an automated pipeline to process sensor-based data produced during load tests on digitally twinned HS railway bridges. The research is developed within the frame of the H2020 European project ASHVIN, related to Assistants for Healthy, Safe, and Productive Virtual Construction, Design, Operation & Maintenance using Digital Twins. The pipeline is developed within a digital twin application based on event-driven microservices, which integrates the ASHVIN IoT platform, the IFC building information model and an array of services dedicated to automating processes performed during the operation stage of structural assets. A load test carried out on a bridge located on a HS railway in Spain is used as a demonstrator. © 2024 Elsevier B.V., All rights reserved.",No load test generation.
"R.R., Karn, Rupesh Raj; R., Das, Rammi; D.R., Pant, Dibakar Raj; J., Heikkonen, Jukka; R.K., Kanth, Rajeev Kumar","Automated Testing and Resilience of Microservice's Network-link using Istio Service Mesh","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130365842&partnerID=40&md5=d365dd784e9d7dea17261444db4739fc","Microservices technology has gained considerable popularity in software design to deploy complex applications in the form of micro-modular microservice components. Each service is implemented as an autonomous system, and its internal constituent data can be accessed via a network interface. Such architecture increases the complexity of the network because each module is a separate entity for development and operations. A fault in any service affects the operation of another service and could completely break the application. It is, therefore, necessary to create a framework for the systematic testing and resilience of the network link in microservices, independent of the programming language and business logic. It helps the network administrator track the cause of the fault. In this paper, we have shown the use of the service mesh Istio to monitor communication between microservices and to develop automated testing and resilience. Istio provides various types of fault injectors for communication links between services. A Locust load testing tool is used to exert a microservice load. The faulty link is located via the Jaeger and Grafana dashboard within the Istio frame. For resilience or correction of the fault, a new connection is temporarily established between the affected microservice by deploying redundant services. In addition, microservices scaling and the implementation of the circuit breaker have been shown to remedy network congestion. The setup is demonstrated in the Kubernetes cluster with the Hipster shop e-commerce application. © 2022 Elsevier B.V., All rights reserved.",No load test generation. Testing framework.
"K., Somashekhar, Karoor; B.E., Eswara Reddy, B. Eswar","Performance Evaluation of Multi-Tier Application by using the Comprehensive Workload Modelling in the Cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106029962&partnerID=40&md5=d8e1d09426a6bf3d4b64f44b65ce0fb1","Cloud-computing is the on-request accessibility of computer system-assets, especially data-storage and computing power, without client direct dynamic administration. Proponents similarly cloud computing ensures that ventures allow them to get more of their applications faster, more reasonably, with more rationality, and with less support, which empowers asset groups to quickly change assets to meet fluctuations and exceptional needs. The current framework centers around how to diminish the dynamic remaining task at hand and increment the reaction time prerequisites in the multi-application by utilizing the auto-scaling virtual machine cloud assets. The proposed framework is to improve the exhaustive remaining task at hand model which considers the space of the specific information document in the cloud for multi-level applications. The total framework uses the RDF Classifier model through which the Request Time and Average Response Time are determined. The proposed strategy plays out the auto-scaling utilization of the remaining task at hand conducted approach which assists with foreseeing application reaction time. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"D., Ernst, Dominik; S., Tai, Stefan","Offline Trace Generation for Microservice Observability","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122991034&partnerID=40&md5=8a239c5ecf16adebaead9f044114f466","Microservices are independently deployable and scalable architectural units owned by different teams, supporting continuous software engineering with increased team autonomy. Their distributed nature, heterogeneity and the shift towards decentralized responsibility, however, introduce difficulties for system-wide observability, making it harder to keep track of broader, strategic goals from an enterprise perspective. Further, different systems supporting runtime observability exist, including language-specific libraries, middleware for transportation, processing and storage, which tend to be complex and typically incur significant implementation and configuration overheads.To address system-scoped challenges of observability in microservice architectures, we propose an offline approach to distributed tracing. First, intra- and inter-service execution paths are described as a behavioral model of the microservices under observation. This model serves as input to an application workload generator, which produces realistic trace data comparable to trace data won through actual runtime observations from distributed tracing systems. Our approach thus allows to evaluate observability of microservices both in an offline and implementation-agnostic manner, which is less costly and serves as a complement and ex-ante perspective, facilitating a system-wide perspective. We present the idea, a workload generator design and a proof-of-concept prototype along with an initial evaluation. © 2022 Elsevier B.V., All rights reserved.","Focus on statistical workload generation, not adversarial. No focus on autoscaling."
"M., Catillo, Marta; L., Ocone, Luciano; U., Villano, Umberto; M., Rak, Massimiliano","Black-box load testing to support auto-scaling web applications in the cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105757250&partnerID=40&md5=96ba12055d4edcf57a91a32120888804","One of the most interesting features of cloud environments is the possibility to deploy scalable applications, which can automatically modulate the amount of leased resources so as to adapt to load variations and to guarantee the desired level of quality of service. As auto-scaling has severe implications on execution costs, making optimal choices is of paramount importance. This paper presents a method based on off-line black-box load testing that allows to obtain performance indexes of a web application in multiple configurations under realistic load. These indexes, along with available resource cost information, can be exploited by auto-scaler tools to implement the desired scaling policy, making a trade-off between cost and user-perceived performance. © 2021 Elsevier B.V., All rights reserved.",Focus on statistically extracted workloads.
"K., Leochico, Kester; E.B., John, Eugene B.","Evaluating cloud auto-scaler resource allocation planning under high-performance computing workloads","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108027413&partnerID=40&md5=d8b565952ebd20521960ccdbcfb69c42","In this paper, we analyze the effects of changing the workload mix and adjusting the average job runtime (as represented by the service rate) between one representative of high-performance computing workloads and one representative of web serving workloads on the performance of three cloud auto-scalers from the literature in an effort to better model the behavior of cloud auto-scalers under high-performance computing scenarios involving long-running jobs rather than short-lived jobs as in previous studies. The simulation and analysis was carried out using the Performance Evaluation framework for Auto-Scaling (PEAS), a cloud simulator framework for modeling the performance of cloud auto-scaling algorithms using scenario theory. © 2021 Elsevier B.V., All rights reserved.",No load test generation.
"R.D.S., Matos, Rubens De S.; J.R., Dantas, Jamilson Ramalho; E., Araujo, Eltton; P.R.M., MacIel, Paulo Romero Martins","Bottleneck Detection in Cloud Computing Performance and Dependability: Sensitivity Rankings for Hierarchical Models","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089571996&partnerID=40&md5=eaee9db1d57e4590ac8d826477d15f34","Cloud computing became widespread on IT industry, saving costs of acquisition and maintenance for companies of all sizes, and enabling fair management of resources according to the demand. Stochastic models can enable performance and dependability evaluation of cloud computing systems efficiently, what is needed for proper capacity planning. Distinct models may be combined in a hierarchy to address the huge number of components and levels of interaction among the system parts. Identification of bottlenecks in such composite models might be hard yet, due to the huge amount of input factors and variables which may interfere with the results. This paper proposes a method for bottleneck detection of computational systems represented with hierarchical models, that is remarkably applied in cloud computing systems. This is achieved through the composition of indices computed from lower level models in equations and solution methods of the top level model, for computing the sensitivity indices of all parameters with respect to a global system measure. A unified sensitivity ranking, comprising the composite indices, indicates the parameters with highest impact on output metrics. A case study supports the demonstration of accuracy and utility of our methodology. The study addresses a web service running on a private cloud with auto scaling mechanisms. The methods and algorithms presented here are helpful for decision-making when designing and managing cloud computing infrastructures, regarding incremental and architectural improvements. © 2020 Elsevier B.V., All rights reserved.",Focus on capacity planning. Does not consider autoscaling.
"A.A., Avritzer, Alberto A.; V., Ferme, Vincenzo; A.A., Janes, Andrea A.; B., Russo, Barbara; A.V., Van Hoorn, André V.; H., Schulz, Henning; D.S., Menasché, Daniel Sadoc; V.Q., Rufino, Vilc Queupe","Scalability Assessment of Microservice Architecture Deployment Configurations: A Domain-based Approach Leveraging Operational Profiles and Load Tests","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080916968&partnerID=40&md5=e43b246bf5eaa46b90ca4f546d32a6f9","Microservices have emerged as an architectural style for developing distributed applications. Assessing the performance of architecture deployment configurations — e.g., with respect to deployment alternatives — is challenging and must be aligned with the system usage in the production environment. In this paper, we introduce an approach for using operational profiles to generate load tests to automatically assess scalability pass/fail criteria of microservice configuration alternatives. The approach provides a Domain-based metric for each alternative that can, for instance, be applied to make informed decisions about the selection of alternatives and to conduct production monitoring regarding performance-related system properties, e.g., anomaly detection. We have evaluated our approach using extensive experiments in a large bare metal host environment and a virtualized environment. First, the data presented in this paper supports the need to carefully evaluate the impact of increasing the level of computing resources on performance. Specifically, for the experiments presented in this paper, we observed that the evaluated Domain-based metric is a non-increasing function of the number of CPU resources for one of the environments under study. In a subsequent series of experiments, we investigate the application of the approach to assess the impact of security attacks on the performance of architecture deployment configurations. © 2020 Elsevier B.V., All rights reserved.",No load test generation. Focus on benchmark of deployment configurations. Uses static load tests.
"A.A., Avritzer, Alberto A.","Challenges and Approaches for the Assessment of Micro-Service Architecture Deployment Alternatives in DevOps : A tutorial presented at ICSA 2020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085758948&partnerID=40&md5=007015f380c80cc1636038a5500a2a9e","The goal of this tutorial is to provide an overview of challenges and approaches for architecture/dependability assessment in the context of DevOps and microservices. Specifically, we present approaches that employ operational data obtained from production-level application performance management (APM) tools, giving access to operational workload profiles, architectural information, failure models, and security intrusions. We use this data to automatically create and conFigure architecture assessments based on models, load tests, and resilience benchmarks. The focus of this tutorial is on approaches that employ production usage, because these approaches provide more accurate recommendations for microservice architecture dependability assessment than approaches that do not consider production usage.We present an overview of (1) the state-of-the-art approaches for obtaining operational data from production systems using APM tools, (2) the challenges of dependability for DevOps and microservices, (3) selected approaches based on operational data to assess dependability. The architecture assessment focus of this tutorial is on scalability, resilience, survivability, and security. Particularly, we present a demo of the automated approach for the evaluation of a domain-based scalability and security metric assessment that is based on the microservice architecture ability to satisfy the performance requirement under load and/or intrusions. We illustrate the approach by presenting experimental results using a benchmark microservice architecture. © 2021 Elsevier B.V., All rights reserved.",Focus on statistically extracted load tests.
"R.A., Lima, Rodrigo Alves; J., Kimball, Joshua; J.E., Ferreira, João Eduardo; C., Pu, Calton","Wise Toolkit: Enabling Microservice-Based System Performance Experiments","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092095035&partnerID=40&md5=62ec023b694e4466d5a7c93c88a3ded0","In this paper, we present the Wise toolkit for microservice-based system performance experiments. Wise comprises a microservice-based application benchmark with controllable workload generation; milliScope, a set of system resource and event monitoring tools; and WED-Make, a workflow language and code generation tool for the construction and execution of system experiments with automatic provenance collection. We also show a running example reproducing the experimental verification of the millibottleneck theory of performance bugs to illustrate how we have used Wise for the performance study of microservice-based benchmark applications in the cloud. © 2020 Elsevier B.V., All rights reserved.",Testing framework. No load test generation.
"H., Schulz, Henning; T., Angerstein, Tobias; D., Okanović, Dušan; A.V., Van Hoorn, André V.","Microservice-tailored generation of session-based workload models for representative load testing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077799730&partnerID=40&md5=bdba5855dcc5e37eee9e30501adbbdf8","Load tests are commonly used to assess the performance of an application system. A representative load test uses workload characteristics according to the user behavior in production. Session-based systems have special workload characteristics as the system is used as sequences of inter-related requests. Approaches exist to automatically extract session-based workload models from production request logs. However, they focus on system-level testing, which is in stark contrast with modern development practices, where one development team is in charge of developing, testing, and deploying a single microservice. Hence, representative session-based workload models for testing single microservices and their integration are desirable. To deal with these issues, we propose a concept for tailoring a representative load test workload to target only certain services, instead of targeting the whole system. Our goal is to transform the workload for one or more specified service(s) from the system-level workload collected in production. Using this approach, only a subset of the application's microservices is deployed for a load test, specifically the targeted services and the services they depend on. We propose two algorithms. The log-based algorithm deals with extracting the workload for a specific service from collected production traces. The model-based algorithm performs the workload tailoring on the level of the workload model. In an experiment series with a representative microservice application, we compare both algorithms with system-level and request-based workoad models. The results show that when load testing a set of services, the tailored workload models outperform untailored workload models in terms of test duration and the capacity of the test infrastructure, and outperform request-based workload models in terms of representativeness. © 2020 Elsevier B.V., All rights reserved.",Focus on the extraction of tests to stress single services instead of full system from statistically extracted traces.
"N., Astyrakakis, Nikolaos; Y., Nikoloudakis, Yannis; I., Kefaloukos, Ioannis; C.A., Skianis, Charalampos A.; E.M., Pallis, Evangelos M.; E.K., Markakis, Evangelos K.","Cloud-native application validation stress testing through a framework for auto-cluster deployment","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073788039&partnerID=40&md5=d658de0192434c635e75ac73d1469f0e","The proliferation of cloud-native applications (applications built specifically for the cloud) has taken cloud computing to another level, but it also revealed several issues such as the lack of automation and complexity of deployment. Extensive research on the subject, revealed many endeavors towards that end. Nonetheless, there are still issues to be solved, such as the cluster monitoring and the cluster deployment automation. Most of the existing solutions, are semi-automated or completely manual approaches, with some of them targeting specific cloud providers/vendors. Furthermore, there is also very limited research that tackles the validation of such applications. Untrusted developers, are able to develop and upload applications to marketplaces, without being audited and verified. Towards addressing the above-mentioned issues, this paper presents a novel, completely automated tool for deploying and monitoring a Kubernetes cluster over OpenStack. Additionally, we propose a tool that provides automatic validation of cloud-native applications. The evaluation of the proposed toolbox resulted in the deployment of Kubernetes clusters with remarkably low overall times, compared to other, manual approaches. The validation process lasted approximately 11 minutes for a containerized application with the Kubernetes Horizontal Pod Autoscaler (HPA) enabled and approximately 3 minutes for a containerized application with the Kubernetes HPA disabled. These overall times are relatively shorter than several other non-automated approaches. The afore-mentioned overall times, are analogous to the underlying hardware and network resources of our test-bed. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"Q., Lei, Qing; W., Liao, Weidong; Y., Jiang, Yingtao; M., Yang, Mei; H., Li, Haifeng","Performance and scalability testing strategy based on Kubemark","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067495142&partnerID=40&md5=1e579db1ceb9bcf43a6e156844f3f86a","The technology of container orchestration dramatically speeds up the extension of applications architected on microservices. As the complexity of those applications continues to increase, the orchestration system needs to resolve performance challenge to deploy thousands of coexisting applications to work cooperatively, and to reach the requirements of efficiency and scalability of microservices architectures. The Kubernetes is an open source project to implement container orchestration, and more popular than the others. For performance testing, the Kubernetes provides Kubemark as a deployment tool, which can simulate a large-scale Kubenetes clusters. Kubemark supports the performance evaluation of cluster scale much larger than the real cluster scale. This paper addresses performance issues of microservices structure, describes the architecture of Kubernetes to implement schedule of resource, and finally proposes a method of performance testing with Kubemark. © 2020 Elsevier B.V., All rights reserved.",No load test generation.
"M., Santana, Matheus; A.R., Sampaio, Adalberto R.; M.V.P., Andrade, Marcos Vinicius P.; N.S., Rosa, Nelson Souto","Transparent tracing of microservice-based applications","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065675356&partnerID=40&md5=e2a198fd4d1b7c6f31536cef43bb257a","Tracing has been applied to study and understand the behavior and performance of distributed systems. Despite the attention this topic has received, two important aspects are still challenges and especially harmful in the context of microservice-based applications: source code instrumentation and performance overhead. Existing attempts resort on working around overhead (e.g., sampling techniques) and do not address microservices architecture's high technological heterogeneity. Our main contribution is a novel approach for tracing microservices which joins proxies' usage (for handling tracing concerns) and operating system syscalls monitoring (for diagnosing causality between multiple requests). It makes advances on the field by completely separating instrumentation and application code while minimizing performance overhead. We carry out a performance evaluation to show the impact of our solution on the execution of microservice-based applications. Our proposal fosters developers' productivity by allowing them to focus on business logic instead of instrumentation and copes with the intrinsic heterogeneity of microservices by relying on deployment modifications and operating systems mechanisms solely. © 2019 Elsevier B.V., All rights reserved.",No load test generation.
"W., Delnat, Wito; E., Truyen, Eddy; A., Rafique, Ansar; D., Van Landuyt, Dimitri; W., Joosen, Wouter","K8-scalar: A workbench to compare autoscalers for container-orchestrated database clusters","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051570766&partnerID=40&md5=47a6f5c1dea8e77f02708533b494bf81","Although a considerable amount of research exists on auto-scaling of database clusters, the design of an effective auto-scaling strategy requires fine-grained tailoring towards the specific application scenario. This paper presents an easy-to-use and extensible workbench exemplar, named K8-Scalar (Kube-Scalar), which allows researchers to implement and evaluate different self-adaptive approaches to auto-scaling container-orchestrated services. The workbench is based on Docker, a popular technology for easing the deployment of containerized software that also has been positioned as an enabler for reproducible research. The workbench also relies on a container orchestration framework: Kubernetes (K8s), the de-facto industry standard for orchestration and monitoring of elastically scalable container-based services. Finally, it integrates and extends Scalar, a generic testbed for evaluating the scalability of large-scale systems with support for evaluating the performance of autoscalers for database clusters. The paper discusses (i) the architecture and implementation of K8-Scalar and how a particular autoscaler can be plugged in, (ii) sketches the design of a Riemann-based autoscaler for database clusters, (iii) illustrates how to design, setup and analyze a series of experiments to configure and evaluate the performance of this autoscaler for a particular database (i.e., Cassandra) and a particular workload type, and (iv) validates the effectiveness of K8-Scalar as a workbench for accurately comparing the performance of different auto-scaling strategies. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"G., Marco, Gribaudo; M., Iacono, Mauro; D., Manini, Daniele","Performance Evaluation of Replication Policies in Microservice Based Architectures","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047010813&partnerID=40&md5=b622b181e18832f1fb0b58c9aff36dbe","Nowadays applications tend to be executed on distributed environments provisioned using on-demand infrastructures. The use of techniques such as application containers simplifies the orchestration of complex systems. In this context, microservices based architectures offer a promising solution for what concerns software development and scalability. In this paper, we propose an approach to study the automatic scalability of microservices architectures deployed in public and private clouds. A Fluid Petri Net model describes the characterise of the platform, and a real trace drives the approach to consider a realistic scenario. Our focus is on evaluating the performances, costs and energy consumptions from both the service provider and infrastructure provider point of view. © 2018 Elsevier B.V., All rights reserved.",No load test generation.
"A.A., Avritzer, Alberto A.; V., Ferme, Vincenzo; A.A., Janes, Andrea A.; B., Russo, Barbara; H., Schulz, Henning; A.V., Van Hoorn, André V.","A quantitative approach for the assessment of microservice architecture deployment alternatives by automated performance testing","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057288257&partnerID=40&md5=afa053782dbc18df7cf0c8d0f5be32af","Microservices have emerged as an architectural style for developing distributed applications. Assessing the performance of architectural deployment alternatives is challenging and must be aligned with the system usage in the production environment. In this paper, we introduce an approach for using operational profiles to generate load tests to automatically assess scalability pass/fail criteria of several microservices deployment alternatives. We have evaluated our approach with different architecture deployment alternatives using extensive lab studies in a large bare metal host environment and a virtualized environment. The data presented in this paper supports the need to carefully evaluate the impact of increasing the level of computing resources on performance. Specifically, for the case study presented in this paper, we observed that the evaluated performance metric is a non-increasing function of the number of CPU resources for one of the environments under study. © 2019 Elsevier B.V., All rights reserved.",No load test generation. Focus on deployment configuration benchmarking. Used statistically extracted traces.
"I.D.S., Fe, Iure De Sousa; R.D.S., Matos, Rubens De S.; J.R., Dantas, Jamilson Ramalho; C., Melo, Carlos; P.R.M., MacIel, Paulo Romero Martins","Stochastic model of performance and cost for auto-scaling planning in public cloud","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044355714&partnerID=40&md5=e160bb004af29fddc9877f3894a77963","Cloud computing has the potential to reduce the cost of systems on the Internet. Elasticity mechanisms, such as autoscaling, enable avoiding wastes, delivering only the necessary resources. Defining and implementing an efficient auto-scaling policy is a complex task that depends on the parameter setting, types of VM contracts and the expected workload. All those variables must be taken into account when establishing the tradeoffs between performance and cost to fulfill a given service-level agreement (SLA). We propose a stochastic model to assist in cloud planning. The model was validated for a set of significant scenarios by comparing the respective model's results with those obtained from real system measurements. This model takes as input the auto-scaling configuration parameters and the time between user requests. The proposed model is employed to compute throughput, mean response time, and cost of the cloud computing infrastructure setup. A sensitivity analysis was also conducted for identifying the parameters impact on the system performance. © 2018 Elsevier B.V., All rights reserved.",No load test generation. Focus on modelling.
"A., Ilyushkin, Alexey; A., Ali-Eldin, Ahmed; N.R., Herbst, Nikolas Roman; A.V., Papadopoulos, Alessandro V.; B., Ghiţ, Bogdan; D.H.J., Epema, Dick H.J.; A., Iosup, Alexandru","An experimental performance evaluation of autoscaling policies for complex workflows","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019018662&partnerID=40&md5=b592ce58c914ef9f3f9b4c9892daabea","Simplifying the task of resource management and scheduling for customers, while still delivering complex Quality-of-Service (QoS), is key to cloud computing. Many autoscaling policies have been proposed in the past decade to decide on behalf of cloud customers when and how to provision resources to a cloud application utilizing cloud elasticity features. However, in prior work, when a new policy is proposed, it is seldom compared to the state-of-the-art, and is often compared only to static provisioning using a predefined QoS target. This reduces the ability of cloud customers and of cloud operators to choose and deploy an autoscaling policy. In our work, we conduct an experimental performance evaluation of autoscaling policies, using as application model workflows, a commonly used formalism for automating resource management for applications with well-defined yet complex structure. We present a detailed comparative study of general state-of-the-art autoscaling policies, along with two new workflow-specific policies. To understand the performance differences between the 7 policies, we conduct various forms of pairwise and group comparisons. We report both individual and aggregated metrics. Our results highlight the trade-offs between the suggested policies, and thus enable a better understanding of the current state-of-the-art. © 2017 Elsevier B.V., All rights reserved.",No load test generation. Benchmarking through modeling.
"M., Galceran-Oms, Marc; J., Cortadella, Jordi; M.A., Kishinevsky, Michael A.","Symbolic performance analysis of elastic systems","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650897551&partnerID=40&md5=6fcf1d59c96048bdcaacb56113db1860","Elastic systems, either synchronous or asynchronous, can be optimized for the average-case performance when they have units with early evaluation or variable latency. The performance evaluation of such systems using analytical methods is a complex problem and may become a bottleneck when an extensive exploration of different architectural configurations must be done. This paper proposes an analytical method for performance evaluation using symbolic expressions. Two version of the method are presented: an exact method that has high run time complexity and an efficient approximate method that computes the lower bound of the system throughput. ©2010 IEEE. © 2021 Elsevier B.V., All rights reserved.", No load test generation. Focus on modelling.
